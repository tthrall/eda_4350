{
  "hash": "72d1a1f947ce3b4918a356682db71bf4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Dirichlet Distribution\"\nsubtitle: \"Selected topics from Part 1 of Data Mining Intro\"\nauthor: \n  - name: \"Send comments to: Tony T (adthral)\"\ndate: \"2025-01-26 12:53 GMT\"\noutput: \n  html_document:\n    toc: true\n    df_print: paged\n    mathjax: default\n  word_document:\n    toc: true\n    df_print: tibble\n  pdf_document:\n    toc: true\n    df_print: tibble\nabstract: \n  \"The Dirichlet and Multinomial distributions are introduced in preparation for a discussion of Latent Dirichlet Allocation (LDA).\"\n---\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n------------------------------------------------------------------------\n\n## Background\n\nLatent Dirichlet Allocation (LDA) was proposed as a method of topic modeling in 2003 in a paper by Blei, Ng, and Jordan.  The method is briefly mentioned in Part 1 of the course.  Several course participants requested a more detailed description.  This note prepares for the requested response by introducing the Dirichlet and Multinomial distributions.\n\n## The Multinomial Distribution\n\nConsider a categorical (qualitative) random variable $X$ that randomly selects one of $K$ distinct categories $\\{c_1, \\ldots, c_K \\}$.\n\n$$\n\\begin{align}\n  X &\\in (c_1, \\ldots, c_K) \\\\ \n  \\\\ \n  p_k &= P(X = c_k) > 0 \\\\ \n  \\\\ \n  \\sum_{k = 1}^K p_k &= 1 \\\\ \n\\end{align}\n$$\n\nProbability vector $p_{\\bullet} = (p_1, \\ldots, p_K)$ is thus the probability distribution of $X$ on $\\{c_1, \\ldots, c_K \\}$.\n\nIf the categories are ordered, we might represent $X$ numerically as a random index $k \\in \\{1, \\ldots, K \\}$.  But for present purposes we suppose the categories are not ordered, and we represent $X$ as a vector of random indicator variables[^one-hot].\n\n[^one-hot]: In machine learning this mapping of a categorical variable to an indicator vector is called \"one-hot encoding\".\n\n$$\n\\begin{align}\n  I_{\\bullet} &= (I_1, \\ldots, I_K) \\\\ \n  \\\\ \n  I_k &= 1 \\text{, if } X = c_k \\\\ \n  I_k &= 0 \\text{, if } X \\ne c_k  \\\\ \n  \\\\ \n  I_{\\bullet} &= \\mathbb{e}_1 = (1, 0, \\ldots, 0)  \\text{, with probability } p_1\\\\ \n  I_{\\bullet} &= \\mathbb{e}_2 = (0, 1, \\ldots, 0)  \\text{, with probability } p_2\\\\ \n  \\vdots \\\\ \n  I_{\\bullet} &= \\mathbb{e}_K = (0, 0, \\ldots, 1)  \\text{, with probability } p_K \\\\ \n\\end{align}\n$$\n\nThat is, the random indicator vector $I_{\\bullet}$ selects one of the Euclidean basis vectors $\\mathbb{e}_k \\in \\mathbb{R}^K$ with probability $p_k$. Therefore the expected value of $I_{\\bullet}$ equals the vector of category probabilities $p_{\\bullet} = (p_1, \\ldots, p_K)$.\n\n$$\n\\begin{align}\n  E(I_k) &= P(I_k = 1) \\\\ \n  & = P(X = c_k) \\\\ \n  & = p_k \\\\\n  \\\\ \n  E(I_{\\bullet}) &= (E(I_1), \\ldots, E(I_K)) \\\\ \n  &= (p_1, \\ldots, p_K) \\\\ \n  &= p_{\\bullet} \\\\ \n\\end{align}\n$$\n\nNow suppose that $(X_1, \\ldots, X_n)$ are independent random variables all having the same distribution as $X$.  Corresponding to $X_{\\nu}$ we have an indicator vector that we'll denote as $I_{\\bullet}^{(\\nu)}$.  Let $S_{\\bullet}$ denote the sum over the $n$ indicator vectors $\\{ I_{\\bullet}^{(\\nu)} \\}_{\\nu}$.\n\n$$\n\\begin{align}\n  S_{\\bullet} &= \\sum_{\\nu = 1}^n I_{\\bullet}^{(\\nu)} \\text{, so that } \\\\ \n  \\\\ \n  S_k &= \\sum_{\\nu = 1}^n I_k^{(\\nu)} \\\\ \n  &= \\text{ number of } \\nu \\text{ such that } X_{\\nu} = c_k\n\\end{align}\n$$\n\nFor any given set of possible counts $s_{\\bullet} = (s_1, \\ldots, s_K)$, that is, of non-negative integers summing to $n$, and for a given probability vector $p_{\\bullet}$, the probability that $S_{\\bullet} = s_{\\bullet}$ is as follows.\n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet} \\; | \\; p_{\\bullet}) &= {n \\choose s_{\\bullet}} \\prod_{k = 1}^K p_k^{s_k}\n\\end{align}\n$$\n\nwhere \n\n$$\n\\begin{align}\n  {n \\choose s_{\\bullet}} &= \\frac{n!}{s_1! s_2! \\cdots s_K!}\n\\end{align}\n$$\n\ngives the number of assignments of $n$ objects to the $K$ categories such that each category $c_k$ receives the prescribed number $s_k$ of objects.\n\nThe probability distribution of $S_{\\bullet}$ is called the _multinomial distribution_.\n\nFor $K = 2$, this simplifies to the binomial distribution, with parameters $(n, p)$, where $(p_1, p_2) = (p, \\; 1 - p)$.  If $\\nu$ denotes the observed number of succeses in $n$ Bernoulli trials, then $(s_1, s_2) = (\\nu, \\; n - \\nu)$.\n\n## Bayesian Inference\n\nThe Bayesian approach to statistical estimation provides a framework for representing the state of knowledge, or degree of uncertainty, about a model parameter before and after collecting relevant data.\n\nAs an example consider the binomial distribution mentioned above, where we are estimating the probability $p$ of success based on a sequence of $n$ independent Bernoulli trials.  Following the notation above, we use two dependent indicator random variables $(I_1, I_2)$ to represent success and failure respectively, with $I_2 = 1- I_1$, and with $(p_1, p_2) = (p, \\; 1-p)$.\n\nPrior to observing the sequence of Bernoulli trials, we might represent the state of information about $p$ as a uniform distribution, so that each possible value of $p \\in [0, 1]$ is deemed equally likely.  Alternatively, if the Bernoulli sequence represents the outcomes of tossing a coin that is presumed fair, or nearly fair, we might represent that information as a probability distribution having a mode at the value $p = 1/2$.\n\nMore specifically, it turns out to be mathematically convenient to represent prior information about probability $p$ as a member of the beta family of probability distributions over the unit interval.  The uniform distribution is the special case of setting beta shape parameters to the values $(1, 1)$.  Alternatively setting the shape parameters to $(2, 2)$ gives a density function symmetric about the mode at $p = 1/2$.\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](s_6a_Dirichlet_dstn_files/figure-html/g_beta_xmpl-1.png){width=672}\n:::\n:::\n\n\n\nIn general, a beta distribution having shape parameters $(\\alpha, \\beta)$ has the following density function.\n\n$$\n\\begin{align}\n  P(p) &= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  \\\\ \n  & \\text{with }  \\alpha > 0 \\text{, and } \\beta > 0\n\\end{align}\n$$\n\nSuppose now that we adopt the distribution just mentioned, $\\mathcal{Beta}(\\alpha, \\beta)$, as the prior distribution of success probability $p$, for some specified positive parameter values $(\\alpha, \\beta)$.  We then observe $s_1$ successes and $s_2$ failures from $n = s_1 + s_2$ independent Bernoulli trials.  Based on these observations we update the prior distribution to form the posterior distribution of $p$ as follows.\n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n\\end{align}\n$$\n\nNote that \n\n$$\n\\begin{align}\n  P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) &= {n \\choose s_1} p^{s_1} (1 - p)^{n - s_1} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  &= {n \\choose s_1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{s_1 + \\alpha - 1} (1 - p)^{n - s_1 + \\beta - 1} \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\\\ \n\\end{align}\n$$\n\nso that \n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet}) &= \\int_0^1 P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^1 p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s_1)\\Gamma(\\beta + s_2)}{\\Gamma(\\alpha + \\beta + s_1 + s_2)} \\\\ \n\\end{align}\n$$\n\nConsequently, the ratio of the last two expressions gives \n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{\\Gamma(\\alpha + \\beta + s_1 + s_2)}{\\Gamma(\\alpha + s_1) \\Gamma(\\beta + s_2)} p^{\\alpha + s_1 - 1} (1-p)^{\\beta + s_2 -1}\n\\end{align}\n$$\n\nThat is, the posterior distribution is $\\mathcal{Beta}(\\alpha + s_1, \\beta + s_2)$.\n\nThis is the \"mathematical convenience\" previously alluded to: the prior and posterior probability distributions of $p$ belong to the same family of parametric probability distributions, namely the $\\mathcal{Beta}$ family.  For this reason the $\\mathcal{Beta}$ family is said to be _conjugate_ to the binomial family.\n\n## The Dirichlet Probability Distribution\n\n### Definition\n\nThe binomial distribution is a special case of the multinomial distribution in which the number of categories $K$ is equal to 2.  (In the discussion above we referred to the categories as success and failue, respectively.)  More generally, for a fixed integer $K \\ge 2$, the family of distributions conjugate to the multinomial family of distributions over $K$ categories is the following _Dirichlet_ family.\n\n$$\n\\begin{align}\n  P(p_{\\bullet}) &= \\frac{\\Gamma(\\alpha_1+ \\cdots + \\alpha_K)}{\\Gamma(\\alpha_1) \\times \\cdots \\times \\Gamma(\\alpha_K)} \\prod_{k = 1}^K p_k^{\\alpha_k - 1} \\\\ \n  \\\\ \n  & \\text{with }  \\alpha_k > 0 \\text{ for } k \\in \\{1, \\ldots, K \\}\n\\end{align}\n$$\n\nThis is the $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$ distribution over $p_{\\bullet}$, where probability vector $p_{\\bullet}$ ranges over the $K - 1$ dimesional simplex such that each component $p_k$ is non-negative and all the components $(p_1, \\cdots, p_K)$ together sum to unity.\n\n### Special Case: $\\alpha_k = \\frac{\\alpha_{+}}{K}$\n\nWe will denote the sum of the components of $\\alpha_{\\bullet} = (\\alpha_1, \\ldots, \\alpha_K)$ as $\\alpha_{+}$.[^alpha_plus]\n\n[^alpha_plus]: An alternative notation for the sum of $(\\alpha_1, \\ldots, \\alpha_K)$ is $\\alpha_0$.\n\n$$\n\\begin{align}\n  \\alpha_{+} &= \\sum_{k = 1}^K \\alpha_k \\\\\n\\end{align}\n$$\n\nConsider the special case in which all the components of $\\alpha_{\\bullet}$ have the same value \n\n$$\n\\begin{align}\n  \\alpha_k &= \\frac{\\alpha_{+}}{K} \\\\\n  & \\text{for } k \\in \\{1, \\dots, K \\}\n\\end{align}\n$$\n\nThe probability distribution is then symmetric in the components of $p_{\\bullet}$, and $\\alpha_{+}$ is referred to as the _concentration parameter_.  The uniform distribution over the domain of $p_{\\bullet}$ (that is, over the $K - 1$ dimensional simplex) is obtained by setting $\\alpha_{+} = K$.  Setting $\\alpha_{+} > K$ concentrates the distribution around the centroid \n\n$$\n\\begin{align}\n  p_{\\textbf{ctr}} &= (\\frac{1}{K}, \\ldots, \\frac{1}{K}) \\\\\n\\end{align}\n$$\n\nFor example the previous figure shows two alternative concentrations of the symmetric beta density function of scalar parameter $p$, where $(p_1, p_2) = (p, \\; 1 - p)$, and $p_{\\textbf{ctr}} = (\\frac{1}{2}, \\frac{1}{2})$.\n\nAlternatively, setting $\\alpha_{+} < K$ concentrates the distribution away from the centroid toward the corners of the simplex.\n\n### Posterior Distribution\n\nRecall that $S_{\\bullet}$ constructed above has a multinomial distribution if it can be represented as the sum of $n$ independent trials, each trial yielding one of the Euclidean basis vectors $\\mathbb{e}_k$ with probability $p_k$.  Then $S_k$, the $k^{th}$ component of $S_{\\bullet}$, counts the number of trials yielding $\\mathbb{e}_k$.\n\nNow suppose that $p_{\\bullet}$ has prior distribution $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$, for some specification of $\\alpha_{\\bullet}$, and that the observed outcome of the $n$ trials is a given vector $s_{\\bullet}$ of counts.  What is the posterior distribution of $p_{\\bullet}$ given the observation $S_{\\bullet} = s_{\\bullet}$?  This turns out to be \n\n$$\n\\begin{align}\n  \\{ p_{\\bullet} \\; | \\; s_{\\bullet} \\} &\\sim \\mathcal{Dirichlet}(\\alpha_{\\bullet} + s_{\\bullet})\n\\end{align}\n$$\n\nThat is, the Dirichlet family is conjugate to the multinomial family, just as the beta family is conjugate to the binomial family.\n\n## Resources\n\n[Dirichlet distribution - Wikipedia](https://en.wikipedia.org/wiki/Dirichlet_distribution)\n\n",
    "supporting": [
      "s_6a_Dirichlet_dstn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}