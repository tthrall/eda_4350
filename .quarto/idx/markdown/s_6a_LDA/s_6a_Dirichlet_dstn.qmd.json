{"title":"The Dirichlet Distribution","markdown":{"yaml":{"title":"The Dirichlet Distribution","subtitle":"Selected topics from Part 1 of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"The Dirichlet and Multinomial distributions are introduced in preparation for a discussion of Latent Dirichlet Allocation (LDA)."},"headingText":"library(gutenbergr)","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(here)\n# library(ISLR2)\n# library(janeaustenr)\nlibrary(LaplacesDemon)\nlibrary(latex2exp)\n# library(learnr)\n# library(quanteda)\n# library(SAPP)\n# library(tidytext)\nlibrary(tidyverse)\n# library(timeSeriesDataSets)\nlibrary(tinytex)\n# library(tm)\n# library(tokenizers)\nlibrary(topicmodels)\n# library(tsibble)\nlibrary(tufte)\n\n```\n\n```{r local_source}\n# source(here(\"code\", \"rmse_per_grp.R\"))\n# source(here(\"code\", \"xtabs_to_jaccard.R\"))\n# source(here(\"code\", \"gen_circuit.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Background\n\nLatent Dirichlet Allocation (LDA) was proposed as a method of topic modeling in 2003 in a paper by Blei, Ng, and Jordan.  The method is briefly mentioned in Part 1 of the course.  Several course participants requested a more detailed description.  This note prepares for the requested response by introducing the Dirichlet and Multinomial distributions.\n\n## The Multinomial Distribution\n\nConsider a categorical (qualitative) random variable $X$ that randomly selects one of $K$ distinct categories $\\{c_1, \\ldots, c_K \\}$.\n\n$$\n\\begin{align}\n  X &\\in (c_1, \\ldots, c_K) \\\\ \n  \\\\ \n  p_k &= P(X = c_k) > 0 \\\\ \n  \\\\ \n  \\sum_{k = 1}^K p_k &= 1 \\\\ \n\\end{align}\n$$\n\nProbability vector $p_{\\bullet} = (p_1, \\ldots, p_K)$ is thus the probability distribution of $X$ on $\\{c_1, \\ldots, c_K \\}$.\n\nIf the categories are ordered, we might represent $X$ numerically as a random index $k \\in \\{1, \\ldots, K \\}$.  But for present purposes we suppose the categories are not ordered, and we represent $X$ as a vector of random indicator variables[^one-hot].\n\n[^one-hot]: In machine learning this mapping of a categorical variable to an indicator vector is called \"one-hot encoding\".\n\n$$\n\\begin{align}\n  I_{\\bullet} &= (I_1, \\ldots, I_K) \\\\ \n  \\\\ \n  I_k &= 1 \\text{, if } X = c_k \\\\ \n  I_k &= 0 \\text{, if } X \\ne c_k  \\\\ \n  \\\\ \n  I_{\\bullet} &= \\mathbb{e}_1 = (1, 0, \\ldots, 0)  \\text{, with probability } p_1\\\\ \n  I_{\\bullet} &= \\mathbb{e}_2 = (0, 1, \\ldots, 0)  \\text{, with probability } p_2\\\\ \n  \\vdots \\\\ \n  I_{\\bullet} &= \\mathbb{e}_K = (0, 0, \\ldots, 1)  \\text{, with probability } p_K \\\\ \n\\end{align}\n$$\n\nThat is, the random indicator vector $I_{\\bullet}$ selects one of the Euclidean basis vectors $\\mathbb{e}_k \\in \\mathbb{R}^K$ with probability $p_k$. Therefore the expected value of $I_{\\bullet}$ equals the vector of category probabilities $p_{\\bullet} = (p_1, \\ldots, p_K)$.\n\n$$\n\\begin{align}\n  E(I_k) &= P(I_k = 1) \\\\ \n  & = P(X = c_k) \\\\ \n  & = p_k \\\\\n  \\\\ \n  E(I_{\\bullet}) &= (E(I_1), \\ldots, E(I_K)) \\\\ \n  &= (p_1, \\ldots, p_K) \\\\ \n  &= p_{\\bullet} \\\\ \n\\end{align}\n$$\n\nNow suppose that $(X_1, \\ldots, X_n)$ are independent random variables all having the same distribution as $X$.  Corresponding to $X_{\\nu}$ we have an indicator vector that we'll denote as $I_{\\bullet}^{(\\nu)}$.  Let $S_{\\bullet}$ denote the sum over the $n$ indicator vectors $\\{ I_{\\bullet}^{(\\nu)} \\}_{\\nu}$.\n\n$$\n\\begin{align}\n  S_{\\bullet} &= \\sum_{\\nu = 1}^n I_{\\bullet}^{(\\nu)} \\text{, so that } \\\\ \n  \\\\ \n  S_k &= \\sum_{\\nu = 1}^n I_k^{(\\nu)} \\\\ \n  &= \\text{ number of } \\nu \\text{ such that } X_{\\nu} = c_k\n\\end{align}\n$$\n\nFor any given set of possible counts $s_{\\bullet} = (s_1, \\ldots, s_K)$, that is, of non-negative integers summing to $n$, and for a given probability vector $p_{\\bullet}$, the probability that $S_{\\bullet} = s_{\\bullet}$ is as follows.\n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet} \\; | \\; p_{\\bullet}) &= {n \\choose s_{\\bullet}} \\prod_{k = 1}^K p_k^{s_k}\n\\end{align}\n$$\n\nwhere \n\n$$\n\\begin{align}\n  {n \\choose s_{\\bullet}} &= \\frac{n!}{s_1! s_2! \\cdots s_K!}\n\\end{align}\n$$\n\ngives the number of assignments of $n$ objects to the $K$ categories such that each category $c_k$ receives the prescribed number $s_k$ of objects.\n\nThe probability distribution of $S_{\\bullet}$ is called the _multinomial distribution_.\n\nFor $K = 2$, this simplifies to the binomial distribution, with parameters $(n, p)$, where $(p_1, p_2) = (p, \\; 1 - p)$.  If $\\nu$ denotes the observed number of succeses in $n$ Bernoulli trials, then $(s_1, s_2) = (\\nu, \\; n - \\nu)$.\n\n## Bayesian Inference\n\nThe Bayesian approach to statistical estimation provides a framework for representing the state of knowledge, or degree of uncertainty, about a model parameter before and after collecting relevant data.\n\nAs an example consider the binomial distribution mentioned above, where we are estimating the probability $p$ of success based on a sequence of $n$ independent Bernoulli trials.  Following the notation above, we use two dependent indicator random variables $(I_1, I_2)$ to represent success and failure respectively, with $I_2 = 1- I_1$, and with $(p_1, p_2) = (p, \\; 1-p)$.\n\nPrior to observing the sequence of Bernoulli trials, we might represent the state of information about $p$ as a uniform distribution, so that each possible value of $p \\in [0, 1]$ is deemed equally likely.  Alternatively, if the Bernoulli sequence represents the outcomes of tossing a coin that is presumed fair, or nearly fair, we might represent that information as a probability distribution having a mode at the value $p = 1/2$.\n\nMore specifically, it turns out to be mathematically convenient to represent prior information about probability $p$ as a member of the beta family of probability distributions over the unit interval.  The uniform distribution is the special case of setting beta shape parameters to the values $(1, 1)$.  Alternatively setting the shape parameters to $(2, 2)$ gives a density function symmetric about the mode at $p = 1/2$.\n\n```{r beta_xmpl}\n# choices of beta shape parameters (a, b)\nab_tbl   <- tibble::tibble(a = 1:2, b = 1:2)\nn_shapes <- nrow(ab_tbl)\n\n# number of plotting points for beta density function\nn_pts <- 200L\n\n# grid points along x axis\nx_grid <- ((1:n_pts) - 0.5)/n_pts\n\n# long tibble to represent different shapes of beta distribution\nbeta_xmpl <- tibble::tibble(\n  p     = rep(x_grid, n_shapes), \n  shape = rep(1:n_shapes, each = n_pts), \n  alpha = rep(ab_tbl$ a[1:n_shapes], each = n_pts), \n  beta  = rep(ab_tbl$ b[1:n_shapes], each = n_pts), \n  f     = dbeta(p, alpha, beta)\n)\n```\n\n```{r g_beta_xmpl}\ng_beta_xmpl <- beta_xmpl |> \n  dplyr::mutate(shape = as_factor(shape)) |> \n  ggplot(mapping = aes(\n    x = p, y = f, color = shape\n  )) + \n  geom_line(show.legend = FALSE) + \n  facet_grid(row = vars(shape)) + \n  labs(\n    title = \"Alternative prior densities of parameter p\", \n    subtitle = \"Beta shape parameters (1, 1) and (2, 2)\"\n  )\ng_beta_xmpl\n```\n\nIn general, a beta distribution having shape parameters $(\\alpha, \\beta)$ has the following density function.\n\n$$\n\\begin{align}\n  P(p) &= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  \\\\ \n  & \\text{with }  \\alpha > 0 \\text{, and } \\beta > 0\n\\end{align}\n$$\n\nSuppose now that we adopt the distribution just mentioned, $\\mathcal{Beta}(\\alpha, \\beta)$, as the prior distribution of success probability $p$, for some specified positive parameter values $(\\alpha, \\beta)$.  We then observe $s_1$ successes and $s_2$ failures from $n = s_1 + s_2$ independent Bernoulli trials.  Based on these observations we update the prior distribution to form the posterior distribution of $p$ as follows.\n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n\\end{align}\n$$\n\nNote that \n\n$$\n\\begin{align}\n  P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) &= {n \\choose s_1} p^{s_1} (1 - p)^{n - s_1} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  &= {n \\choose s_1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{s_1 + \\alpha - 1} (1 - p)^{n - s_1 + \\beta - 1} \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\\\ \n\\end{align}\n$$\n\nso that \n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet}) &= \\int_0^1 P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^1 p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s_1)\\Gamma(\\beta + s_2)}{\\Gamma(\\alpha + \\beta + s_1 + s_2)} \\\\ \n\\end{align}\n$$\n\nConsequently, the ratio of the last two expressions gives \n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{\\Gamma(\\alpha + \\beta + s_1 + s_2)}{\\Gamma(\\alpha + s_1) \\Gamma(\\beta + s_2)} p^{\\alpha + s_1 - 1} (1-p)^{\\beta + s_2 -1}\n\\end{align}\n$$\n\nThat is, the posterior distribution is $\\mathcal{Beta}(\\alpha + s_1, \\beta + s_2)$.\n\nThis is the \"mathematical convenience\" previously alluded to: the prior and posterior probability distributions of $p$ belong to the same family of parametric probability distributions, namely the $\\mathcal{Beta}$ family.  For this reason the $\\mathcal{Beta}$ family is said to be _conjugate_ to the binomial family.\n\n## The Dirichlet Probability Distribution\n\n### Definition\n\nThe binomial distribution is a special case of the multinomial distribution in which the number of categories $K$ is equal to 2.  (In the discussion above we referred to the categories as success and failue, respectively.)  More generally, for a fixed integer $K \\ge 2$, the family of distributions conjugate to the multinomial family of distributions over $K$ categories is the following _Dirichlet_ family.\n\n$$\n\\begin{align}\n  P(p_{\\bullet}) &= \\frac{\\Gamma(\\alpha_1+ \\cdots + \\alpha_K)}{\\Gamma(\\alpha_1) \\times \\cdots \\times \\Gamma(\\alpha_K)} \\prod_{k = 1}^K p_k^{\\alpha_k - 1} \\\\ \n  \\\\ \n  & \\text{with }  \\alpha_k > 0 \\text{ for } k \\in \\{1, \\ldots, K \\}\n\\end{align}\n$$\n\nThis is the $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$ distribution over $p_{\\bullet}$, where probability vector $p_{\\bullet}$ ranges over the $K - 1$ dimesional simplex such that each component $p_k$ is non-negative and all the components $(p_1, \\cdots, p_K)$ together sum to unity.\n\n### Special Case: $\\alpha_k = \\frac{\\alpha_{+}}{K}$\n\nWe will denote the sum of the components of $\\alpha_{\\bullet} = (\\alpha_1, \\ldots, \\alpha_K)$ as $\\alpha_{+}$.[^alpha_plus]\n\n[^alpha_plus]: An alternative notation for the sum of $(\\alpha_1, \\ldots, \\alpha_K)$ is $\\alpha_0$.\n\n$$\n\\begin{align}\n  \\alpha_{+} &= \\sum_{k = 1}^K \\alpha_k \\\\\n\\end{align}\n$$\n\nConsider the special case in which all the components of $\\alpha_{\\bullet}$ have the same value \n\n$$\n\\begin{align}\n  \\alpha_k &= \\frac{\\alpha_{+}}{K} \\\\\n  & \\text{for } k \\in \\{1, \\dots, K \\}\n\\end{align}\n$$\n\nThe probability distribution is then symmetric in the components of $p_{\\bullet}$, and $\\alpha_{+}$ is referred to as the _concentration parameter_.  The uniform distribution over the domain of $p_{\\bullet}$ (that is, over the $K - 1$ dimensional simplex) is obtained by setting $\\alpha_{+} = K$.  Setting $\\alpha_{+} > K$ concentrates the distribution around the centroid \n\n$$\n\\begin{align}\n  p_{\\textbf{ctr}} &= (\\frac{1}{K}, \\ldots, \\frac{1}{K}) \\\\\n\\end{align}\n$$\n\nFor example the previous figure shows two alternative concentrations of the symmetric beta density function of scalar parameter $p$, where $(p_1, p_2) = (p, \\; 1 - p)$, and $p_{\\textbf{ctr}} = (\\frac{1}{2}, \\frac{1}{2})$.\n\nAlternatively, setting $\\alpha_{+} < K$ concentrates the distribution away from the centroid toward the corners of the simplex.\n\n### Posterior Distribution\n\nRecall that $S_{\\bullet}$ constructed above has a multinomial distribution if it can be represented as the sum of $n$ independent trials, each trial yielding one of the Euclidean basis vectors $\\mathbb{e}_k$ with probability $p_k$.  Then $S_k$, the $k^{th}$ component of $S_{\\bullet}$, counts the number of trials yielding $\\mathbb{e}_k$.\n\nNow suppose that $p_{\\bullet}$ has prior distribution $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$, for some specification of $\\alpha_{\\bullet}$, and that the observed outcome of the $n$ trials is a given vector $s_{\\bullet}$ of counts.  What is the posterior distribution of $p_{\\bullet}$ given the observation $S_{\\bullet} = s_{\\bullet}$?  This turns out to be \n\n$$\n\\begin{align}\n  \\{ p_{\\bullet} \\; | \\; s_{\\bullet} \\} &\\sim \\mathcal{Dirichlet}(\\alpha_{\\bullet} + s_{\\bullet})\n\\end{align}\n$$\n\nThat is, the Dirichlet family is conjugate to the multinomial family, just as the beta family is conjugate to the binomial family.\n\n## Resources\n\n[Dirichlet distribution - Wikipedia](https://en.wikipedia.org/wiki/Dirichlet_distribution)\n\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\n# library(gutenbergr)\nlibrary(here)\n# library(ISLR2)\n# library(janeaustenr)\nlibrary(LaplacesDemon)\nlibrary(latex2exp)\n# library(learnr)\n# library(quanteda)\n# library(SAPP)\n# library(tidytext)\nlibrary(tidyverse)\n# library(timeSeriesDataSets)\nlibrary(tinytex)\n# library(tm)\n# library(tokenizers)\nlibrary(topicmodels)\n# library(tsibble)\nlibrary(tufte)\n\n```\n\n```{r local_source}\n# source(here(\"code\", \"rmse_per_grp.R\"))\n# source(here(\"code\", \"xtabs_to_jaccard.R\"))\n# source(here(\"code\", \"gen_circuit.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Background\n\nLatent Dirichlet Allocation (LDA) was proposed as a method of topic modeling in 2003 in a paper by Blei, Ng, and Jordan.  The method is briefly mentioned in Part 1 of the course.  Several course participants requested a more detailed description.  This note prepares for the requested response by introducing the Dirichlet and Multinomial distributions.\n\n## The Multinomial Distribution\n\nConsider a categorical (qualitative) random variable $X$ that randomly selects one of $K$ distinct categories $\\{c_1, \\ldots, c_K \\}$.\n\n$$\n\\begin{align}\n  X &\\in (c_1, \\ldots, c_K) \\\\ \n  \\\\ \n  p_k &= P(X = c_k) > 0 \\\\ \n  \\\\ \n  \\sum_{k = 1}^K p_k &= 1 \\\\ \n\\end{align}\n$$\n\nProbability vector $p_{\\bullet} = (p_1, \\ldots, p_K)$ is thus the probability distribution of $X$ on $\\{c_1, \\ldots, c_K \\}$.\n\nIf the categories are ordered, we might represent $X$ numerically as a random index $k \\in \\{1, \\ldots, K \\}$.  But for present purposes we suppose the categories are not ordered, and we represent $X$ as a vector of random indicator variables[^one-hot].\n\n[^one-hot]: In machine learning this mapping of a categorical variable to an indicator vector is called \"one-hot encoding\".\n\n$$\n\\begin{align}\n  I_{\\bullet} &= (I_1, \\ldots, I_K) \\\\ \n  \\\\ \n  I_k &= 1 \\text{, if } X = c_k \\\\ \n  I_k &= 0 \\text{, if } X \\ne c_k  \\\\ \n  \\\\ \n  I_{\\bullet} &= \\mathbb{e}_1 = (1, 0, \\ldots, 0)  \\text{, with probability } p_1\\\\ \n  I_{\\bullet} &= \\mathbb{e}_2 = (0, 1, \\ldots, 0)  \\text{, with probability } p_2\\\\ \n  \\vdots \\\\ \n  I_{\\bullet} &= \\mathbb{e}_K = (0, 0, \\ldots, 1)  \\text{, with probability } p_K \\\\ \n\\end{align}\n$$\n\nThat is, the random indicator vector $I_{\\bullet}$ selects one of the Euclidean basis vectors $\\mathbb{e}_k \\in \\mathbb{R}^K$ with probability $p_k$. Therefore the expected value of $I_{\\bullet}$ equals the vector of category probabilities $p_{\\bullet} = (p_1, \\ldots, p_K)$.\n\n$$\n\\begin{align}\n  E(I_k) &= P(I_k = 1) \\\\ \n  & = P(X = c_k) \\\\ \n  & = p_k \\\\\n  \\\\ \n  E(I_{\\bullet}) &= (E(I_1), \\ldots, E(I_K)) \\\\ \n  &= (p_1, \\ldots, p_K) \\\\ \n  &= p_{\\bullet} \\\\ \n\\end{align}\n$$\n\nNow suppose that $(X_1, \\ldots, X_n)$ are independent random variables all having the same distribution as $X$.  Corresponding to $X_{\\nu}$ we have an indicator vector that we'll denote as $I_{\\bullet}^{(\\nu)}$.  Let $S_{\\bullet}$ denote the sum over the $n$ indicator vectors $\\{ I_{\\bullet}^{(\\nu)} \\}_{\\nu}$.\n\n$$\n\\begin{align}\n  S_{\\bullet} &= \\sum_{\\nu = 1}^n I_{\\bullet}^{(\\nu)} \\text{, so that } \\\\ \n  \\\\ \n  S_k &= \\sum_{\\nu = 1}^n I_k^{(\\nu)} \\\\ \n  &= \\text{ number of } \\nu \\text{ such that } X_{\\nu} = c_k\n\\end{align}\n$$\n\nFor any given set of possible counts $s_{\\bullet} = (s_1, \\ldots, s_K)$, that is, of non-negative integers summing to $n$, and for a given probability vector $p_{\\bullet}$, the probability that $S_{\\bullet} = s_{\\bullet}$ is as follows.\n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet} \\; | \\; p_{\\bullet}) &= {n \\choose s_{\\bullet}} \\prod_{k = 1}^K p_k^{s_k}\n\\end{align}\n$$\n\nwhere \n\n$$\n\\begin{align}\n  {n \\choose s_{\\bullet}} &= \\frac{n!}{s_1! s_2! \\cdots s_K!}\n\\end{align}\n$$\n\ngives the number of assignments of $n$ objects to the $K$ categories such that each category $c_k$ receives the prescribed number $s_k$ of objects.\n\nThe probability distribution of $S_{\\bullet}$ is called the _multinomial distribution_.\n\nFor $K = 2$, this simplifies to the binomial distribution, with parameters $(n, p)$, where $(p_1, p_2) = (p, \\; 1 - p)$.  If $\\nu$ denotes the observed number of succeses in $n$ Bernoulli trials, then $(s_1, s_2) = (\\nu, \\; n - \\nu)$.\n\n## Bayesian Inference\n\nThe Bayesian approach to statistical estimation provides a framework for representing the state of knowledge, or degree of uncertainty, about a model parameter before and after collecting relevant data.\n\nAs an example consider the binomial distribution mentioned above, where we are estimating the probability $p$ of success based on a sequence of $n$ independent Bernoulli trials.  Following the notation above, we use two dependent indicator random variables $(I_1, I_2)$ to represent success and failure respectively, with $I_2 = 1- I_1$, and with $(p_1, p_2) = (p, \\; 1-p)$.\n\nPrior to observing the sequence of Bernoulli trials, we might represent the state of information about $p$ as a uniform distribution, so that each possible value of $p \\in [0, 1]$ is deemed equally likely.  Alternatively, if the Bernoulli sequence represents the outcomes of tossing a coin that is presumed fair, or nearly fair, we might represent that information as a probability distribution having a mode at the value $p = 1/2$.\n\nMore specifically, it turns out to be mathematically convenient to represent prior information about probability $p$ as a member of the beta family of probability distributions over the unit interval.  The uniform distribution is the special case of setting beta shape parameters to the values $(1, 1)$.  Alternatively setting the shape parameters to $(2, 2)$ gives a density function symmetric about the mode at $p = 1/2$.\n\n```{r beta_xmpl}\n# choices of beta shape parameters (a, b)\nab_tbl   <- tibble::tibble(a = 1:2, b = 1:2)\nn_shapes <- nrow(ab_tbl)\n\n# number of plotting points for beta density function\nn_pts <- 200L\n\n# grid points along x axis\nx_grid <- ((1:n_pts) - 0.5)/n_pts\n\n# long tibble to represent different shapes of beta distribution\nbeta_xmpl <- tibble::tibble(\n  p     = rep(x_grid, n_shapes), \n  shape = rep(1:n_shapes, each = n_pts), \n  alpha = rep(ab_tbl$ a[1:n_shapes], each = n_pts), \n  beta  = rep(ab_tbl$ b[1:n_shapes], each = n_pts), \n  f     = dbeta(p, alpha, beta)\n)\n```\n\n```{r g_beta_xmpl}\ng_beta_xmpl <- beta_xmpl |> \n  dplyr::mutate(shape = as_factor(shape)) |> \n  ggplot(mapping = aes(\n    x = p, y = f, color = shape\n  )) + \n  geom_line(show.legend = FALSE) + \n  facet_grid(row = vars(shape)) + \n  labs(\n    title = \"Alternative prior densities of parameter p\", \n    subtitle = \"Beta shape parameters (1, 1) and (2, 2)\"\n  )\ng_beta_xmpl\n```\n\nIn general, a beta distribution having shape parameters $(\\alpha, \\beta)$ has the following density function.\n\n$$\n\\begin{align}\n  P(p) &= \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  \\\\ \n  & \\text{with }  \\alpha > 0 \\text{, and } \\beta > 0\n\\end{align}\n$$\n\nSuppose now that we adopt the distribution just mentioned, $\\mathcal{Beta}(\\alpha, \\beta)$, as the prior distribution of success probability $p$, for some specified positive parameter values $(\\alpha, \\beta)$.  We then observe $s_1$ successes and $s_2$ failures from $n = s_1 + s_2$ independent Bernoulli trials.  Based on these observations we update the prior distribution to form the posterior distribution of $p$ as follows.\n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n\\end{align}\n$$\n\nNote that \n\n$$\n\\begin{align}\n  P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) &= {n \\choose s_1} p^{s_1} (1 - p)^{n - s_1} \\times \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1} (1-p)^{\\beta -1 } \\\\ \n  &= {n \\choose s_1} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{s_1 + \\alpha - 1} (1 - p)^{n - s_1 + \\beta - 1} \\\\\n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\\\ \n\\end{align}\n$$\n\nso that \n\n$$\n\\begin{align}\n  P(S_{\\bullet} = s_{\\bullet}) &= \\int_0^1 P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p) \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\int_0^1 p^{\\alpha + s_1 - 1} (1 - p)^{\\beta + s_2 - 1} \\,dp \\\\ \n  &= \\frac{(s_1 + s_2)!}{s_1! s_2!} \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s_1)\\Gamma(\\beta + s_2)}{\\Gamma(\\alpha + \\beta + s_1 + s_2)} \\\\ \n\\end{align}\n$$\n\nConsequently, the ratio of the last two expressions gives \n\n$$\n\\begin{align}\n  P(p \\; | \\; S_{\\bullet} = s_{\\bullet}) &= \\frac{P(p, \\;  S_{\\bullet}=s_{\\bullet})}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{P(S_{\\bullet}=s_{\\bullet} \\; | \\; p) \\; P(p)}{P(S_{\\bullet}=s_{\\bullet})} \\\\ \n  &= \\frac{\\Gamma(\\alpha + \\beta + s_1 + s_2)}{\\Gamma(\\alpha + s_1) \\Gamma(\\beta + s_2)} p^{\\alpha + s_1 - 1} (1-p)^{\\beta + s_2 -1}\n\\end{align}\n$$\n\nThat is, the posterior distribution is $\\mathcal{Beta}(\\alpha + s_1, \\beta + s_2)$.\n\nThis is the \"mathematical convenience\" previously alluded to: the prior and posterior probability distributions of $p$ belong to the same family of parametric probability distributions, namely the $\\mathcal{Beta}$ family.  For this reason the $\\mathcal{Beta}$ family is said to be _conjugate_ to the binomial family.\n\n## The Dirichlet Probability Distribution\n\n### Definition\n\nThe binomial distribution is a special case of the multinomial distribution in which the number of categories $K$ is equal to 2.  (In the discussion above we referred to the categories as success and failue, respectively.)  More generally, for a fixed integer $K \\ge 2$, the family of distributions conjugate to the multinomial family of distributions over $K$ categories is the following _Dirichlet_ family.\n\n$$\n\\begin{align}\n  P(p_{\\bullet}) &= \\frac{\\Gamma(\\alpha_1+ \\cdots + \\alpha_K)}{\\Gamma(\\alpha_1) \\times \\cdots \\times \\Gamma(\\alpha_K)} \\prod_{k = 1}^K p_k^{\\alpha_k - 1} \\\\ \n  \\\\ \n  & \\text{with }  \\alpha_k > 0 \\text{ for } k \\in \\{1, \\ldots, K \\}\n\\end{align}\n$$\n\nThis is the $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$ distribution over $p_{\\bullet}$, where probability vector $p_{\\bullet}$ ranges over the $K - 1$ dimesional simplex such that each component $p_k$ is non-negative and all the components $(p_1, \\cdots, p_K)$ together sum to unity.\n\n### Special Case: $\\alpha_k = \\frac{\\alpha_{+}}{K}$\n\nWe will denote the sum of the components of $\\alpha_{\\bullet} = (\\alpha_1, \\ldots, \\alpha_K)$ as $\\alpha_{+}$.[^alpha_plus]\n\n[^alpha_plus]: An alternative notation for the sum of $(\\alpha_1, \\ldots, \\alpha_K)$ is $\\alpha_0$.\n\n$$\n\\begin{align}\n  \\alpha_{+} &= \\sum_{k = 1}^K \\alpha_k \\\\\n\\end{align}\n$$\n\nConsider the special case in which all the components of $\\alpha_{\\bullet}$ have the same value \n\n$$\n\\begin{align}\n  \\alpha_k &= \\frac{\\alpha_{+}}{K} \\\\\n  & \\text{for } k \\in \\{1, \\dots, K \\}\n\\end{align}\n$$\n\nThe probability distribution is then symmetric in the components of $p_{\\bullet}$, and $\\alpha_{+}$ is referred to as the _concentration parameter_.  The uniform distribution over the domain of $p_{\\bullet}$ (that is, over the $K - 1$ dimensional simplex) is obtained by setting $\\alpha_{+} = K$.  Setting $\\alpha_{+} > K$ concentrates the distribution around the centroid \n\n$$\n\\begin{align}\n  p_{\\textbf{ctr}} &= (\\frac{1}{K}, \\ldots, \\frac{1}{K}) \\\\\n\\end{align}\n$$\n\nFor example the previous figure shows two alternative concentrations of the symmetric beta density function of scalar parameter $p$, where $(p_1, p_2) = (p, \\; 1 - p)$, and $p_{\\textbf{ctr}} = (\\frac{1}{2}, \\frac{1}{2})$.\n\nAlternatively, setting $\\alpha_{+} < K$ concentrates the distribution away from the centroid toward the corners of the simplex.\n\n### Posterior Distribution\n\nRecall that $S_{\\bullet}$ constructed above has a multinomial distribution if it can be represented as the sum of $n$ independent trials, each trial yielding one of the Euclidean basis vectors $\\mathbb{e}_k$ with probability $p_k$.  Then $S_k$, the $k^{th}$ component of $S_{\\bullet}$, counts the number of trials yielding $\\mathbb{e}_k$.\n\nNow suppose that $p_{\\bullet}$ has prior distribution $\\mathcal{Dirichlet}(\\alpha_{\\bullet})$, for some specification of $\\alpha_{\\bullet}$, and that the observed outcome of the $n$ trials is a given vector $s_{\\bullet}$ of counts.  What is the posterior distribution of $p_{\\bullet}$ given the observation $S_{\\bullet} = s_{\\bullet}$?  This turns out to be \n\n$$\n\\begin{align}\n  \\{ p_{\\bullet} \\; | \\; s_{\\bullet} \\} &\\sim \\mathcal{Dirichlet}(\\alpha_{\\bullet} + s_{\\bullet})\n\\end{align}\n$$\n\nThat is, the Dirichlet family is conjugate to the multinomial family, just as the beta family is conjugate to the binomial family.\n\n## Resources\n\n[Dirichlet distribution - Wikipedia](https://en.wikipedia.org/wiki/Dirichlet_distribution)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_6a_Dirichlet_dstn.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"The Dirichlet Distribution","subtitle":"Selected topics from Part 1 of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"The Dirichlet and Multinomial distributions are introduced in preparation for a discussion of Latent Dirichlet Allocation (LDA)."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}