{"title":"Text Analysis","markdown":{"yaml":{"title":"Text Analysis","subtitle":"Part 1, session 2b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Introduce basic ideas and methods of text analysis."},"headingText":"library(topicmodels)","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(gutenbergr)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(janeaustenr)\nlibrary(latex2exp)\nlibrary(quanteda)\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(tm)\nlibrary(tokenizers)\nlibrary(tufte)\n\n# package topic models available?\ntopicmodels_loaded <- require(topicmodels)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"rmse_per_grp.R\"))\nsource(here(\"code\", \"xtabs_to_jaccard.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Introduction\n\nThis session highlights some basic ideas and methods that underpin a rapidly advancing field. We follow the online book by Silge and Robinson cited below and use their R package `tidytext`. The authors emphasize the \"tidy\" formatting of data (i.e., as key-value pairs) along with a set of R packages sharing this approach, collectively called the R `tidyverse`.\n\n## Text Example\n\nToward the end of Shakespeare's play \"Macbeth\", the protagonist proclaims:\n\n```{r sound_fury}\nsound_fury <- c(\"Life's but a walking shadow, a poor player \",\n          \"That struts and frets his hour upon the stage, \",\n          \"And then is heard no more: it is a tale \", \n          \"Told by an idiot, full of sound and fury, \", \n          \"Signifying nothing.\")\n\nsound_fury\n```\n\n(Source: \"Macbeth\", Act V, Scene V, lines 24-28.)\n\nFor purposes of technical analysis we break these flowing lines into a table of words. We begin as follows.\n\n```{r sf_line_tbl, echo=TRUE}\nsf_line_tbl <- tibble::tibble(\n  l_idx = 24:28, \n  line  = sound_fury\n)\nsf_line_tbl\n```\n\nThe table above merely identifies the original line number of each line. The next step is to break each line into a sequence of \"tokens\", where a *token* is a meaningful unit of text (such as a word) to be used as the unit of analysis. (\"Tokenization\" is the process of splitting text into tokens.) Applying `tidytext::unnest_tokens()` to the data table above, we obtain the following table, with just one token (word) per row.\n\n```{r sf_token_tbl, echo=TRUE}\nsf_token_tbl <- sf_line_tbl |> \n  tidytext::unnest_tokens(\n    input  = \"line\", \n    output = \"word\"\n  )\nsf_token_tbl\n```\n\nThe next step is to remove so-called stop-words, that is, articles (\"a\", \"the\", ...), connectors (\"and\", \"or\", ...) and other words that provide structure to a sentence but otherwise carry little information. The `tidytext` package contains a data frame, `stop_words`, of such words, which enables us to remove them from the above table of tokens.\n\n```{r sf_tokens_xsw, echo=TRUE}\nsf_tokens_xsw <- sf_token_tbl |> \n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\"\n  )\nsf_tokens_xsw\n```\n\n## Larger Text Examples\n\nWe'll use larger bodies of text via the following R packages.\n\n### Jane Austen's novels\n\n-   Package `janeaustenr`: Jane Austen (1775-1817) completed 6 novels, which the function `austen_books()` returns as a data frame with 2 columns: the `text` of the novels divided into strings (each approximating a line of printed text), and `book`, which gives the titles of the novels (in order of publication) as a factor.\n\nHere are the number of strings per book.\n\n```{r ja_strings_per_book}\nja_strings_per_book <- austen_books() |> \n  summarise(\n    .by = book, \n    n_strings = n()\n  )\nja_strings_per_book\n```\n\nHere are the ten words used most frequently across these novels (excluding stop-words).\n\n```{r ja_plus_chapters}\n# new columns per book: line number, chapter\nja_plus_chapters <- austen_books() |> \n  mutate(\n    .by        = book, \n    linenumber = row_number(),\n    chapter    = cumsum(\n      str_detect(\n        text, \n        regex(\n          \"^chapter [\\\\divxlc]\",\n          ignore_case = TRUE)\n      ))\n  )\n```\n\n```{r ja_words_raw}\n# break each line of text into several rows of words\nja_words_raw <- ja_plus_chapters |> \n  unnest_tokens(\n    input  = text, \n    output = word\n  )\n```\n\n```{r ja_words_xsw}\n# remove stop-words\nja_words_xsw <- ja_words_raw |> \n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\")\n```\n\n```{r ja_word_ct}\n# count the number of occurrences of each word\n# list largest counts first\nja_word_ct <- ja_words_xsw |> \n  dplyr::count(word, sort = TRUE)\n```\n\n```{r ja_word_ct__kable}\nja_word_ct |> \n  dplyr::slice_head(n = 10) |> \n  knitr::kable(\n  caption = \"Jane Austen: 10 most common (non-stop) words\", \n  col.names = c(\"word\", \"count\")\n)\n```\n\n### The Gutenberg Project\n\n-   Package `gutenbergr`: Enables the user to download and process public domain works in the [Project Gutenberg](https://www.gutenberg.org/) collection.\n\nThe collection boasts over 75000 free electronic books. The data frame `gutenberg_subjects` uses the Library of Congress Classifications (`lcc`) and Library of Congress Subject Headings (`lcsh`) to categorize topics included in the collection. The package offers this and other such metadata to facilitate searching for desired works.\n\nAs a contrast with Jane Austen, here are some well-known science fiction novels of H.G. Wells (1866-1946).\n\n```{r hgwells_books}\nhgwells_books <- tibble::tribble(\n  ~id, ~title,\n    35L, \"The Time Machine\",\n    36L, \"The War of the Worlds\",\n  5230L, \"The Invisible Man\",\n   159L, \"The Island of Doctor Moreau\"\n)\nhgwells_books\n```\n\nAmong these novels, here are the most frequently used words (again excluding stop-words).\n\n```{r hgwells_lines}\n# load file as tibble by one of these methods\nuncl_download <- FALSE\nload_rda      <- TRUE\n\nif (uncl_download) {\n  hgwells_lines <- gutenberg_download(\n    gutenberg_id = hgwells_books$ id)\n} else {\n  if (load_rda) {\n    load(here(\"data\", \"rda\", \"hgwells.rda\"))\n    hgwells_lines <- hgwells\n    rm(hgwells)\n  }\n}\n```\n\n```{r hgwells_words}\n# break each line into rows, one word per row\nhgwells_words <- hgwells_lines |> \n  unnest_tokens(\n    input  = text, \n    output = word\n  ) |> \n  # remove stop-words\n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\")\n```\n\n```{r hgwells_words__write}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  hgwells_words |> write_tsv(\n    here(\"data\", \"retain\", \"hgwells_words.txt\")\n  )\n}\n```\n\n```{r hgwells_word_ct}\nhgwells_word_ct <- hgwells_words |> \n  dplyr::count(word, sort = TRUE)\n```\n\n```{r hgwells_word_ct__kable}\nhgwells_word_ct |> \n  dplyr::slice_head(n = 10) |> \n  knitr::kable(\n  caption = \"H.G. Wells: 10 most common (non-stop) words\", \n  col.names = c(\"word\", \"count\")\n)\n```\n\n## Class Exercise\n\nTeam up with a classmate and devise a way to compare word frequencies in the novels of Jane Austen and H.G. Wells, respectively. Share with the class your comparison of just the top 10 words used by each author. Propose a method for comparing all the words used by each author. Take 20 minutes to prepare to report to the class.\n\n## TF-IDF: Term Frequency - Inverse Doc Frequency\n\nCan the number of times each word appears in a document be used to indicate what the document is about? On the one hand, the number of occurrences of a given word in a given document might indicate the importance of the word within the document. On the other hand, words that commonly occur in most documents are unlikely to distinguish the key ideas in a given document.\n\nWe've already introduced one way to separate the wheat from the chaff: remove stop-words. Another approach, called *tf-idf*, is to multiply a term's relative frequency (tf) in a selected document by its *inverse document frequency* (idf) with respect to a collection or *corpus* of documents. That is, the relative frequency of a term $t_0$ in a given document $d_0$ is the number of occurrences $\\mathcal{n}(t_0, d_0)$ of the given term divided by the number of occurrences of all terms.\n\n$$\n\\begin{align}\n  tf(t_0, d_0) &= \\frac{\\mathcal{n}(t_0, d_0)}{\\sum_{t \\in d_0}\\mathcal{n}(t, d_0)}\n\\end{align}\n$$\n\nAs for inverse document frequency (idf), there are several alternative definitions. Here's the definition we'll use.\n\n$$\n\\begin{align}\n  idf(t, \\mathcal{D}) &= \\log_e \\left( \\frac{| \\mathcal{D} |}{| \\mathcal{D}_t |} \\right) \\\\ \n  \\\\ \n  & t = \\text{term} \\\\ \n  & \\mathcal{D} = \\text{corpus of documents } \\\\\n  & \\mathcal{D}_t = \\{ d \\in \\mathcal{D} : t \\in d  \\}\n\\end{align}\n$$\n\nExample: let $\\mathcal{D}$ denote the set of Jane Austen's 6 novels, and let each novel take its turn as the document $d_0$ of interest. For each book, the most distinctive word (that maximizes tf-idf) is as follows.\n\n```{r ja_book_wd_ct}\n# count (book, word) occurrences\n# including stop-words\nja_book_wd_ct <- ja_words_raw |> \n  # n = number of (book, word) occurrences\n  dplyr::count(book, word)\n```\n\n```{r ja_tf_idf}\n# calculate (tf, idf, tf-idf)\nja_tf_idf <- ja_book_wd_ct |> \n  tidytext::bind_tf_idf(word, book, n) |> \n  arrange(book, desc(tf_idf))\n```\n\n```{r ja_max_wd_per_book}\n# use max tf-idf to find the most distinctive word per book\nja_max_wd_per_book <- ja_tf_idf |> \n  group_by(book) |> \n  filter(tf_idf == max(tf_idf, na.rm = TRUE))\n```\n\n```{r ja_max_wd_per_book__kable}\nja_max_wd_per_book |> \n  knitr::kable(\n    caption = \"Max tf-idf word per book\", \n    digits = 3\n  )\n```\n\n## Document-Term Matrix (DTM)\n\nSo far, we've been analyzing text arranged in the tidy text format: a table in which each row pertains to a unique (document, token) pair. The `tidytext::unnest_tokens()` function counts the number of occurrences of each such pair. Tables in this format can be explored and visualized using the suite of tidy tools, including packages `dplyr`, `tidyr`, and `ggplot2`.\n\nAside from the `tidytext` package, most R tools for natural language processing aren't compatible with this format. The [CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) lists packages that take other structures of input and provide non-tidy outputs. These packages are very useful in text mining applications, and many existing text datasets are structured according to these non-tidy formats.\n\nOne of the most common structures that text mining packages work with is the [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (or DTM). This is a matrix where:\n\n-   each row represents one document (such as a book or article),\n-   each column represents one term, and\n-   each value (typically) contains the number of appearances of that term in that document.\n\nSince most (document, term) pairings have zero occurrences, DTMs are usually implemented as sparse matrices. These objects can be treated as matrices (enabling one to access particular rows and columns), but are stored in a more efficient format.\n\nDTM objects cannot be used directly with tidy tools, and tidy data frames cannot be used as input for most text mining packages. Therefore, the `tidytext` package provides two functions that convert between the two formats.\n\n-   `tidy()` turns a document-term matrix into a tidy data frame. This function comes from the `broom` package, which provides similar tidying functions for many statistical models and objects.\n-   `cast()` turns a tidy one-term-per-row data frame into a matrix. Package `tidytext` provides three variations of this function, each converting to a different type of matrix:\n    -   `cast_sparse()` (converting to a sparse matrix from the `Matrix` package);\n    -   `cast_dtm()` (converting to a `DocumentTermMatrix` object from package `tm`); and\n    -   `cast_dfm()` (converting to a `dfm` object from quanteda).\n\nA widely used implementation of DTMs in R is the `DocumentTermMatrix` class in the `tm` package. Many available text mining datasets are provided in this format.\n\n### Example: Associated Press articles\n\nAs an example, here's a description of Associated Press newspaper articles included as a DTM in the `topicmodels` package.\n\n```{r AssociatedPress, echo=TRUE}\nif (topicmodels_loaded) {\n  data(\"AssociatedPress\", package = \"topicmodels\")\n}\n\n# AssociatedPress\n# <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\n# Non-/sparse entries: 302031/23220327\n# Sparsity           : 99%\n# Maximal term length: 18\n# Weighting          : term frequency (tf)\n```\n\nThis Associated Press DTM consists of 2246 documents (rows) and 10473 terms (columns), with 99% of the potential (document, term) pairings having zero instances (and thus excluded from the sparse matrix).\n\n### Example: inaugural addresses of US presidents\n\nThe inaugural addresses of US presidents, provided by package `quanteda`, is an interesting example of a document-features matrix (DFM), a variant of a DTM. Here are the identifying variables for the first 6 presidential addresses.\n\n```{r inaug_dfm, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0), echo=TRUE}\ndata(\"data_corpus_inaugural\", package = \"quanteda\")\nhead(docvars(data_corpus_inaugural), 6)\n```\n\nHere is a list of tokens from the addresses given in 1861, 1933, and 1961.\n\n```{r some_presidential_tokens, echo=TRUE}\nsome_presidential_tokens <- data_corpus_inaugural |> \n  corpus_subset(Year %in% c(1861L, 1933L, 1961L)) |> \n  tokens()\nsome_presidential_tokens\n```\n\nHere is a rendering of this list of tokens as a document-feature matrix (DFM).\n\n```{r presidential_dfm, echo=TRUE}\npresidential_dfm <- some_presidential_tokens |> \n  quanteda::dfm()\npresidential_dfm\n```\n\nWe now reconfigure the DFM as a tidy data frame (tibble).\n\n```{r presidential_tbl, echo=TRUE}\npresidential_tbl <- presidential_dfm |> \n  tidytext::tidy()\npresidential_tbl\n```\n\n```{r presidential_tbl__save}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  presidential_tbl |> write_tsv(\n    here(\"data\", \"retain\", \"presidential_tbl.txt\")\n  )\n}\n```\n\nWe can now use `tidytext::bind_tf_idf()` to determine the words that most distinguish the three presidential addresses.\n\n## Topic Models\n\nText analysis methods can be applied to a variety of document types, including books, speeches, blog posts, news articles, and so on. Sometimes we can divide a collection of documents into natural groups to be analyzed separately. We can also use topic modeling to construct such groups. Topic modeling is the unsupervised categorization of documents, similar to clustering of numeric data.\n\n### Latent Dirichlet allocation (LDA)\n\nLatent Dirichlet allocation (LDA) is a popular method for fitting topic models, and is guided by two principles.\n\n-   **Every document is a mixture of topics.** For example, in a two-topic model we could say \"Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.\"\n-   **Every topic is a mixture of words.** For example, consider a two-topic model of American news, with one topic for \"politics\" and one for \"entertainment.\" The most common words in the politics topic might be \"President\", \"Congress\", and \"government\", while the entertainment topic may be made up of words such as \"movies\", \"television\", and \"actor\". Importantly, words can be shared between topics; a word like \"budget\" might appear in both equally.\n\nThis approach allows the constructed groups to overlap, similar to the soft clustering of numeric data.\n\n### Example: Associated Press articles\n\nTo illustrate, we'll apply function `LDA()` to the data set (DTM) `AssociatedPress`, both provided by the `topicmodels` package. The DTM is a collection of 2246 news articles from an American news agency, mostly published around 1988. For purposes of illustration we'll specify a two-topic model, as follows.[^1]\n\n[^1]: Function `LDA()` in package `topicmodels` returns a topic model of class \"LDA_VEM\". LDA denotes latent Dirichlet allocation. VEM denotes the Variational Expectation Maximization (EM) algorithm.\n\n```{r ap_lda, echo=TRUE}\nif (topicmodels_loaded) {\n  ap_lda <- AssociatedPress |> \n    LDA(\n      k = 2, \n      # set a seed so that the output of the model is predictable\n      control = list(seed = 1234)\n    )\n  ap_lda\n}\n```\n\nWe now construct (topic, term) probabilities (called $\\beta$ in the LDA literature).\n\n```{r ap_topics}\n# TODO: overcome error generated by tidy(ap_lda)\n# in the meantime, patch as follows\n\nif (topicmodels_loaded) {\n  beta_mat           <- ap_lda@beta\n  colnames(beta_mat) <- ap_lda@terms\n  rownames(beta_mat) <- paste0(\"topic_\", 1:2)\n  \n  beta_tbl           <- beta_mat |> \n    as_tibble(rownames = \"topic\") |> \n    dplyr::mutate(topic = 1:2)\n  \n  beta_long          <- beta_tbl |> \n    tidyr::pivot_longer(\n      cols      = - topic, \n      names_to  = \"term\", \n      values_to = \"ln_prob\"\n    ) |> \n    dplyr::arrange(term)\n  \n  ap_topics <- beta_long |> \n    dplyr::mutate(\n      beta = exp(ln_prob)\n    ) |> \n    dplyr::select(- ln_prob)\n  \n} else {\n  # read in previously saved file\n  ap_topics <- read_tsv(here(\n    \"data\", \"retain\", \"ap_topics.txt\"\n  ))\n}\n\nap_topics |> print(digits = 2)\n```\n\n```{r ap_topics__save}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  ap_topics |> write_tsv(\n    here(\"data\", \"retain\", \"ap_topics.txt\")\n  )\n}\n```\n\nHere are the most probable terms for each of the two constructed topics, along with their probabilities (beta), shown as a bar chart.\n\n```{r ap_top_terms}\nap_top_terms <- ap_topics |> \n  slice_max(beta, n = 10, by = \"topic\")\n```\n\n```{r ap_top_terms_wide}\n# afterthought: display bar chart, not this table\nap_top_terms_1 <- ap_top_terms |> \n  dplyr::filter(topic == 1L)\n\nap_top_terms_2 <- ap_top_terms |> \n  dplyr::filter(topic == 2L)\n\nap_top_terms_wide <- tibble::tibble(\n  rank    = 1:nrow(ap_top_terms_1), \n  topic_1 = ap_top_terms_1$term, \n  topic_2 = ap_top_terms_2$term\n)\n```\n\n```{r g_ap_top_terms}\ng_ap_top_terms <- ap_top_terms |> \n  mutate(term = reorder_within(\n    x = term, \n    by = beta, \n    within = topic)) |> \n  ggplot(mapping = aes(\n    x = beta, y = term, fill = factor(topic)\n  )) + \n  geom_col(show.legend = FALSE) + \n  facet_wrap(~ as_factor(topic), scales = \"free\") + \n  scale_y_reordered() + \n  labs(title = \"AP articles: most probable terms by topic\")\ng_ap_top_terms\n```\n\nTopics 1 and 2 seem to pertain to business and politics, respectively, although \"new\" and \"people\" are prominent terms for both topics.\n\nAnother way to compare topics 1 and 2 is to examine the terms shared by the two topics and then find the terms having the biggest disparity in (topic, term) probability (beta). Here's a bar chart showing the more prominent differences, expressed as\n\n$$\n\\begin{align}\n  \\log_2 \\left( \\frac{\\beta_2}{\\beta_1} \\right)\n\\end{align}\n$$\n\nrestricting the set of terms to those assigned to both topics $(\\min(\\beta_1, \\beta_2) > 0)$ with at least one of them exceeding a probability threshhold, say $(\\max(\\beta_1, \\beta_2) > 0.001)$.\n\n```{r beta_ratio_wide}\nbeta_ratio_wide <- ap_topics |> \n  mutate(topic = paste0(\"beta_\", topic)) |> \n  pivot_wider(names_from = topic, values_from = beta) |> \n  filter(\n    pmin(beta_1, beta_2) > 0, \n    pmax(beta_1, beta_2) > 0.001\n  ) |> \n  mutate(log_ratio = log2(beta_2 / beta_1))\n```\n\n```{r g_beta_ratio_wide}\ng_beta_ratio_wide <- beta_ratio_wide |> \n  group_by(direction = log_ratio > 0) |> \n  slice_max(abs(log_ratio), n = 10) |> \n  ungroup() |> \n  mutate(term = reorder(term, log_ratio)) |> \n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\ng_beta_ratio_wide\n```\n\nThis figure further supports the earlier conjecture that topics 1 and 2 pertain to business and politics, respectively.\n\n## Additional Aspects of Text Analysis\n\nWe've only touched on a few basic ideas and methods underpinning text analysis. Additional topics (on both the analysis and generation of text) include the following (which vary in complexity).\n\n-   n-grams: phrases of $n$ consecutive words\n-   word networks (as graphs)\n-   sentiment analysis\n-   clustering, categorization, and prediction\n-   word embedding (as real-valued vectors)\n-   specialized tokenizers, stemming\n-   non-English human languages (including machine translation)\n-   large language models (LLMs)\n\n## Team Exercises\n\n1.  As a follow-up to the class exercise, propose a way to compare the vocabularies of Austen and Wells. What words are shared most? Least? Should stop-words be excluded? Express your proposal as pseudo-code.\n\n2.  We presented a table showing the word whose tf-idf is maximum for each of Jane Austen's novels. Extend this comparison to show the words having the topmost tf-idf values. How would you present this comparison as a table? As a figure?\n\n3.  Following the example of a document-feature matrix (DFM), extract the inaugural addresses of 1861, 1933, and 1961 from `quanteda::data_corpus_inaugural`. For each token in each address, calculate its tf-idf to determine the tokens that most distinguish the three addresses.\n\n4.  In the preceding exercise, the meta-data give us the name of the speaker and the year of the address. How would you use that knowledge to evaluate a topic-modeling algorithm applied to the inaugural addresses? Time permitting, conduct such an evaluation of a topic-modeling method based on the `topicmodels` R package.\n\n## Resources\n\n[Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/) by Silge and Robinson\n\n[tf–idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n\n[CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)\n\n[Introduction to the tm Package: Text Mining in R](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)\n\n[Quantitative Analysis of Textual Data • quanteda](https://quanteda.io/)\n\n[Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) by David Blei, Andrew Ng, and Michael Jordan. JMLR (2003)\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(gutenbergr)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(janeaustenr)\nlibrary(latex2exp)\nlibrary(quanteda)\nlibrary(tidytext)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(tm)\nlibrary(tokenizers)\n# library(topicmodels)\nlibrary(tufte)\n\n# package topic models available?\ntopicmodels_loaded <- require(topicmodels)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"rmse_per_grp.R\"))\nsource(here(\"code\", \"xtabs_to_jaccard.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Introduction\n\nThis session highlights some basic ideas and methods that underpin a rapidly advancing field. We follow the online book by Silge and Robinson cited below and use their R package `tidytext`. The authors emphasize the \"tidy\" formatting of data (i.e., as key-value pairs) along with a set of R packages sharing this approach, collectively called the R `tidyverse`.\n\n## Text Example\n\nToward the end of Shakespeare's play \"Macbeth\", the protagonist proclaims:\n\n```{r sound_fury}\nsound_fury <- c(\"Life's but a walking shadow, a poor player \",\n          \"That struts and frets his hour upon the stage, \",\n          \"And then is heard no more: it is a tale \", \n          \"Told by an idiot, full of sound and fury, \", \n          \"Signifying nothing.\")\n\nsound_fury\n```\n\n(Source: \"Macbeth\", Act V, Scene V, lines 24-28.)\n\nFor purposes of technical analysis we break these flowing lines into a table of words. We begin as follows.\n\n```{r sf_line_tbl, echo=TRUE}\nsf_line_tbl <- tibble::tibble(\n  l_idx = 24:28, \n  line  = sound_fury\n)\nsf_line_tbl\n```\n\nThe table above merely identifies the original line number of each line. The next step is to break each line into a sequence of \"tokens\", where a *token* is a meaningful unit of text (such as a word) to be used as the unit of analysis. (\"Tokenization\" is the process of splitting text into tokens.) Applying `tidytext::unnest_tokens()` to the data table above, we obtain the following table, with just one token (word) per row.\n\n```{r sf_token_tbl, echo=TRUE}\nsf_token_tbl <- sf_line_tbl |> \n  tidytext::unnest_tokens(\n    input  = \"line\", \n    output = \"word\"\n  )\nsf_token_tbl\n```\n\nThe next step is to remove so-called stop-words, that is, articles (\"a\", \"the\", ...), connectors (\"and\", \"or\", ...) and other words that provide structure to a sentence but otherwise carry little information. The `tidytext` package contains a data frame, `stop_words`, of such words, which enables us to remove them from the above table of tokens.\n\n```{r sf_tokens_xsw, echo=TRUE}\nsf_tokens_xsw <- sf_token_tbl |> \n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\"\n  )\nsf_tokens_xsw\n```\n\n## Larger Text Examples\n\nWe'll use larger bodies of text via the following R packages.\n\n### Jane Austen's novels\n\n-   Package `janeaustenr`: Jane Austen (1775-1817) completed 6 novels, which the function `austen_books()` returns as a data frame with 2 columns: the `text` of the novels divided into strings (each approximating a line of printed text), and `book`, which gives the titles of the novels (in order of publication) as a factor.\n\nHere are the number of strings per book.\n\n```{r ja_strings_per_book}\nja_strings_per_book <- austen_books() |> \n  summarise(\n    .by = book, \n    n_strings = n()\n  )\nja_strings_per_book\n```\n\nHere are the ten words used most frequently across these novels (excluding stop-words).\n\n```{r ja_plus_chapters}\n# new columns per book: line number, chapter\nja_plus_chapters <- austen_books() |> \n  mutate(\n    .by        = book, \n    linenumber = row_number(),\n    chapter    = cumsum(\n      str_detect(\n        text, \n        regex(\n          \"^chapter [\\\\divxlc]\",\n          ignore_case = TRUE)\n      ))\n  )\n```\n\n```{r ja_words_raw}\n# break each line of text into several rows of words\nja_words_raw <- ja_plus_chapters |> \n  unnest_tokens(\n    input  = text, \n    output = word\n  )\n```\n\n```{r ja_words_xsw}\n# remove stop-words\nja_words_xsw <- ja_words_raw |> \n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\")\n```\n\n```{r ja_word_ct}\n# count the number of occurrences of each word\n# list largest counts first\nja_word_ct <- ja_words_xsw |> \n  dplyr::count(word, sort = TRUE)\n```\n\n```{r ja_word_ct__kable}\nja_word_ct |> \n  dplyr::slice_head(n = 10) |> \n  knitr::kable(\n  caption = \"Jane Austen: 10 most common (non-stop) words\", \n  col.names = c(\"word\", \"count\")\n)\n```\n\n### The Gutenberg Project\n\n-   Package `gutenbergr`: Enables the user to download and process public domain works in the [Project Gutenberg](https://www.gutenberg.org/) collection.\n\nThe collection boasts over 75000 free electronic books. The data frame `gutenberg_subjects` uses the Library of Congress Classifications (`lcc`) and Library of Congress Subject Headings (`lcsh`) to categorize topics included in the collection. The package offers this and other such metadata to facilitate searching for desired works.\n\nAs a contrast with Jane Austen, here are some well-known science fiction novels of H.G. Wells (1866-1946).\n\n```{r hgwells_books}\nhgwells_books <- tibble::tribble(\n  ~id, ~title,\n    35L, \"The Time Machine\",\n    36L, \"The War of the Worlds\",\n  5230L, \"The Invisible Man\",\n   159L, \"The Island of Doctor Moreau\"\n)\nhgwells_books\n```\n\nAmong these novels, here are the most frequently used words (again excluding stop-words).\n\n```{r hgwells_lines}\n# load file as tibble by one of these methods\nuncl_download <- FALSE\nload_rda      <- TRUE\n\nif (uncl_download) {\n  hgwells_lines <- gutenberg_download(\n    gutenberg_id = hgwells_books$ id)\n} else {\n  if (load_rda) {\n    load(here(\"data\", \"rda\", \"hgwells.rda\"))\n    hgwells_lines <- hgwells\n    rm(hgwells)\n  }\n}\n```\n\n```{r hgwells_words}\n# break each line into rows, one word per row\nhgwells_words <- hgwells_lines |> \n  unnest_tokens(\n    input  = text, \n    output = word\n  ) |> \n  # remove stop-words\n  anti_join(\n    y  = tidytext::stop_words, \n    by = \"word\")\n```\n\n```{r hgwells_words__write}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  hgwells_words |> write_tsv(\n    here(\"data\", \"retain\", \"hgwells_words.txt\")\n  )\n}\n```\n\n```{r hgwells_word_ct}\nhgwells_word_ct <- hgwells_words |> \n  dplyr::count(word, sort = TRUE)\n```\n\n```{r hgwells_word_ct__kable}\nhgwells_word_ct |> \n  dplyr::slice_head(n = 10) |> \n  knitr::kable(\n  caption = \"H.G. Wells: 10 most common (non-stop) words\", \n  col.names = c(\"word\", \"count\")\n)\n```\n\n## Class Exercise\n\nTeam up with a classmate and devise a way to compare word frequencies in the novels of Jane Austen and H.G. Wells, respectively. Share with the class your comparison of just the top 10 words used by each author. Propose a method for comparing all the words used by each author. Take 20 minutes to prepare to report to the class.\n\n## TF-IDF: Term Frequency - Inverse Doc Frequency\n\nCan the number of times each word appears in a document be used to indicate what the document is about? On the one hand, the number of occurrences of a given word in a given document might indicate the importance of the word within the document. On the other hand, words that commonly occur in most documents are unlikely to distinguish the key ideas in a given document.\n\nWe've already introduced one way to separate the wheat from the chaff: remove stop-words. Another approach, called *tf-idf*, is to multiply a term's relative frequency (tf) in a selected document by its *inverse document frequency* (idf) with respect to a collection or *corpus* of documents. That is, the relative frequency of a term $t_0$ in a given document $d_0$ is the number of occurrences $\\mathcal{n}(t_0, d_0)$ of the given term divided by the number of occurrences of all terms.\n\n$$\n\\begin{align}\n  tf(t_0, d_0) &= \\frac{\\mathcal{n}(t_0, d_0)}{\\sum_{t \\in d_0}\\mathcal{n}(t, d_0)}\n\\end{align}\n$$\n\nAs for inverse document frequency (idf), there are several alternative definitions. Here's the definition we'll use.\n\n$$\n\\begin{align}\n  idf(t, \\mathcal{D}) &= \\log_e \\left( \\frac{| \\mathcal{D} |}{| \\mathcal{D}_t |} \\right) \\\\ \n  \\\\ \n  & t = \\text{term} \\\\ \n  & \\mathcal{D} = \\text{corpus of documents } \\\\\n  & \\mathcal{D}_t = \\{ d \\in \\mathcal{D} : t \\in d  \\}\n\\end{align}\n$$\n\nExample: let $\\mathcal{D}$ denote the set of Jane Austen's 6 novels, and let each novel take its turn as the document $d_0$ of interest. For each book, the most distinctive word (that maximizes tf-idf) is as follows.\n\n```{r ja_book_wd_ct}\n# count (book, word) occurrences\n# including stop-words\nja_book_wd_ct <- ja_words_raw |> \n  # n = number of (book, word) occurrences\n  dplyr::count(book, word)\n```\n\n```{r ja_tf_idf}\n# calculate (tf, idf, tf-idf)\nja_tf_idf <- ja_book_wd_ct |> \n  tidytext::bind_tf_idf(word, book, n) |> \n  arrange(book, desc(tf_idf))\n```\n\n```{r ja_max_wd_per_book}\n# use max tf-idf to find the most distinctive word per book\nja_max_wd_per_book <- ja_tf_idf |> \n  group_by(book) |> \n  filter(tf_idf == max(tf_idf, na.rm = TRUE))\n```\n\n```{r ja_max_wd_per_book__kable}\nja_max_wd_per_book |> \n  knitr::kable(\n    caption = \"Max tf-idf word per book\", \n    digits = 3\n  )\n```\n\n## Document-Term Matrix (DTM)\n\nSo far, we've been analyzing text arranged in the tidy text format: a table in which each row pertains to a unique (document, token) pair. The `tidytext::unnest_tokens()` function counts the number of occurrences of each such pair. Tables in this format can be explored and visualized using the suite of tidy tools, including packages `dplyr`, `tidyr`, and `ggplot2`.\n\nAside from the `tidytext` package, most R tools for natural language processing aren't compatible with this format. The [CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html) lists packages that take other structures of input and provide non-tidy outputs. These packages are very useful in text mining applications, and many existing text datasets are structured according to these non-tidy formats.\n\nOne of the most common structures that text mining packages work with is the [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (or DTM). This is a matrix where:\n\n-   each row represents one document (such as a book or article),\n-   each column represents one term, and\n-   each value (typically) contains the number of appearances of that term in that document.\n\nSince most (document, term) pairings have zero occurrences, DTMs are usually implemented as sparse matrices. These objects can be treated as matrices (enabling one to access particular rows and columns), but are stored in a more efficient format.\n\nDTM objects cannot be used directly with tidy tools, and tidy data frames cannot be used as input for most text mining packages. Therefore, the `tidytext` package provides two functions that convert between the two formats.\n\n-   `tidy()` turns a document-term matrix into a tidy data frame. This function comes from the `broom` package, which provides similar tidying functions for many statistical models and objects.\n-   `cast()` turns a tidy one-term-per-row data frame into a matrix. Package `tidytext` provides three variations of this function, each converting to a different type of matrix:\n    -   `cast_sparse()` (converting to a sparse matrix from the `Matrix` package);\n    -   `cast_dtm()` (converting to a `DocumentTermMatrix` object from package `tm`); and\n    -   `cast_dfm()` (converting to a `dfm` object from quanteda).\n\nA widely used implementation of DTMs in R is the `DocumentTermMatrix` class in the `tm` package. Many available text mining datasets are provided in this format.\n\n### Example: Associated Press articles\n\nAs an example, here's a description of Associated Press newspaper articles included as a DTM in the `topicmodels` package.\n\n```{r AssociatedPress, echo=TRUE}\nif (topicmodels_loaded) {\n  data(\"AssociatedPress\", package = \"topicmodels\")\n}\n\n# AssociatedPress\n# <<DocumentTermMatrix (documents: 2246, terms: 10473)>>\n# Non-/sparse entries: 302031/23220327\n# Sparsity           : 99%\n# Maximal term length: 18\n# Weighting          : term frequency (tf)\n```\n\nThis Associated Press DTM consists of 2246 documents (rows) and 10473 terms (columns), with 99% of the potential (document, term) pairings having zero instances (and thus excluded from the sparse matrix).\n\n### Example: inaugural addresses of US presidents\n\nThe inaugural addresses of US presidents, provided by package `quanteda`, is an interesting example of a document-features matrix (DFM), a variant of a DTM. Here are the identifying variables for the first 6 presidential addresses.\n\n```{r inaug_dfm, R.options=list(quanteda_print_dfm_max_ndoc = 0, quanteda_print_dfm_max_nfeat = 0), echo=TRUE}\ndata(\"data_corpus_inaugural\", package = \"quanteda\")\nhead(docvars(data_corpus_inaugural), 6)\n```\n\nHere is a list of tokens from the addresses given in 1861, 1933, and 1961.\n\n```{r some_presidential_tokens, echo=TRUE}\nsome_presidential_tokens <- data_corpus_inaugural |> \n  corpus_subset(Year %in% c(1861L, 1933L, 1961L)) |> \n  tokens()\nsome_presidential_tokens\n```\n\nHere is a rendering of this list of tokens as a document-feature matrix (DFM).\n\n```{r presidential_dfm, echo=TRUE}\npresidential_dfm <- some_presidential_tokens |> \n  quanteda::dfm()\npresidential_dfm\n```\n\nWe now reconfigure the DFM as a tidy data frame (tibble).\n\n```{r presidential_tbl, echo=TRUE}\npresidential_tbl <- presidential_dfm |> \n  tidytext::tidy()\npresidential_tbl\n```\n\n```{r presidential_tbl__save}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  presidential_tbl |> write_tsv(\n    here(\"data\", \"retain\", \"presidential_tbl.txt\")\n  )\n}\n```\n\nWe can now use `tidytext::bind_tf_idf()` to determine the words that most distinguish the three presidential addresses.\n\n## Topic Models\n\nText analysis methods can be applied to a variety of document types, including books, speeches, blog posts, news articles, and so on. Sometimes we can divide a collection of documents into natural groups to be analyzed separately. We can also use topic modeling to construct such groups. Topic modeling is the unsupervised categorization of documents, similar to clustering of numeric data.\n\n### Latent Dirichlet allocation (LDA)\n\nLatent Dirichlet allocation (LDA) is a popular method for fitting topic models, and is guided by two principles.\n\n-   **Every document is a mixture of topics.** For example, in a two-topic model we could say \"Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.\"\n-   **Every topic is a mixture of words.** For example, consider a two-topic model of American news, with one topic for \"politics\" and one for \"entertainment.\" The most common words in the politics topic might be \"President\", \"Congress\", and \"government\", while the entertainment topic may be made up of words such as \"movies\", \"television\", and \"actor\". Importantly, words can be shared between topics; a word like \"budget\" might appear in both equally.\n\nThis approach allows the constructed groups to overlap, similar to the soft clustering of numeric data.\n\n### Example: Associated Press articles\n\nTo illustrate, we'll apply function `LDA()` to the data set (DTM) `AssociatedPress`, both provided by the `topicmodels` package. The DTM is a collection of 2246 news articles from an American news agency, mostly published around 1988. For purposes of illustration we'll specify a two-topic model, as follows.[^1]\n\n[^1]: Function `LDA()` in package `topicmodels` returns a topic model of class \"LDA_VEM\". LDA denotes latent Dirichlet allocation. VEM denotes the Variational Expectation Maximization (EM) algorithm.\n\n```{r ap_lda, echo=TRUE}\nif (topicmodels_loaded) {\n  ap_lda <- AssociatedPress |> \n    LDA(\n      k = 2, \n      # set a seed so that the output of the model is predictable\n      control = list(seed = 1234)\n    )\n  ap_lda\n}\n```\n\nWe now construct (topic, term) probabilities (called $\\beta$ in the LDA literature).\n\n```{r ap_topics}\n# TODO: overcome error generated by tidy(ap_lda)\n# in the meantime, patch as follows\n\nif (topicmodels_loaded) {\n  beta_mat           <- ap_lda@beta\n  colnames(beta_mat) <- ap_lda@terms\n  rownames(beta_mat) <- paste0(\"topic_\", 1:2)\n  \n  beta_tbl           <- beta_mat |> \n    as_tibble(rownames = \"topic\") |> \n    dplyr::mutate(topic = 1:2)\n  \n  beta_long          <- beta_tbl |> \n    tidyr::pivot_longer(\n      cols      = - topic, \n      names_to  = \"term\", \n      values_to = \"ln_prob\"\n    ) |> \n    dplyr::arrange(term)\n  \n  ap_topics <- beta_long |> \n    dplyr::mutate(\n      beta = exp(ln_prob)\n    ) |> \n    dplyr::select(- ln_prob)\n  \n} else {\n  # read in previously saved file\n  ap_topics <- read_tsv(here(\n    \"data\", \"retain\", \"ap_topics.txt\"\n  ))\n}\n\nap_topics |> print(digits = 2)\n```\n\n```{r ap_topics__save}\n# save as text file just once\nsave_file <- FALSE\nif (save_file) {\n  ap_topics |> write_tsv(\n    here(\"data\", \"retain\", \"ap_topics.txt\")\n  )\n}\n```\n\nHere are the most probable terms for each of the two constructed topics, along with their probabilities (beta), shown as a bar chart.\n\n```{r ap_top_terms}\nap_top_terms <- ap_topics |> \n  slice_max(beta, n = 10, by = \"topic\")\n```\n\n```{r ap_top_terms_wide}\n# afterthought: display bar chart, not this table\nap_top_terms_1 <- ap_top_terms |> \n  dplyr::filter(topic == 1L)\n\nap_top_terms_2 <- ap_top_terms |> \n  dplyr::filter(topic == 2L)\n\nap_top_terms_wide <- tibble::tibble(\n  rank    = 1:nrow(ap_top_terms_1), \n  topic_1 = ap_top_terms_1$term, \n  topic_2 = ap_top_terms_2$term\n)\n```\n\n```{r g_ap_top_terms}\ng_ap_top_terms <- ap_top_terms |> \n  mutate(term = reorder_within(\n    x = term, \n    by = beta, \n    within = topic)) |> \n  ggplot(mapping = aes(\n    x = beta, y = term, fill = factor(topic)\n  )) + \n  geom_col(show.legend = FALSE) + \n  facet_wrap(~ as_factor(topic), scales = \"free\") + \n  scale_y_reordered() + \n  labs(title = \"AP articles: most probable terms by topic\")\ng_ap_top_terms\n```\n\nTopics 1 and 2 seem to pertain to business and politics, respectively, although \"new\" and \"people\" are prominent terms for both topics.\n\nAnother way to compare topics 1 and 2 is to examine the terms shared by the two topics and then find the terms having the biggest disparity in (topic, term) probability (beta). Here's a bar chart showing the more prominent differences, expressed as\n\n$$\n\\begin{align}\n  \\log_2 \\left( \\frac{\\beta_2}{\\beta_1} \\right)\n\\end{align}\n$$\n\nrestricting the set of terms to those assigned to both topics $(\\min(\\beta_1, \\beta_2) > 0)$ with at least one of them exceeding a probability threshhold, say $(\\max(\\beta_1, \\beta_2) > 0.001)$.\n\n```{r beta_ratio_wide}\nbeta_ratio_wide <- ap_topics |> \n  mutate(topic = paste0(\"beta_\", topic)) |> \n  pivot_wider(names_from = topic, values_from = beta) |> \n  filter(\n    pmin(beta_1, beta_2) > 0, \n    pmax(beta_1, beta_2) > 0.001\n  ) |> \n  mutate(log_ratio = log2(beta_2 / beta_1))\n```\n\n```{r g_beta_ratio_wide}\ng_beta_ratio_wide <- beta_ratio_wide |> \n  group_by(direction = log_ratio > 0) |> \n  slice_max(abs(log_ratio), n = 10) |> \n  ungroup() |> \n  mutate(term = reorder(term, log_ratio)) |> \n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\ng_beta_ratio_wide\n```\n\nThis figure further supports the earlier conjecture that topics 1 and 2 pertain to business and politics, respectively.\n\n## Additional Aspects of Text Analysis\n\nWe've only touched on a few basic ideas and methods underpinning text analysis. Additional topics (on both the analysis and generation of text) include the following (which vary in complexity).\n\n-   n-grams: phrases of $n$ consecutive words\n-   word networks (as graphs)\n-   sentiment analysis\n-   clustering, categorization, and prediction\n-   word embedding (as real-valued vectors)\n-   specialized tokenizers, stemming\n-   non-English human languages (including machine translation)\n-   large language models (LLMs)\n\n## Team Exercises\n\n1.  As a follow-up to the class exercise, propose a way to compare the vocabularies of Austen and Wells. What words are shared most? Least? Should stop-words be excluded? Express your proposal as pseudo-code.\n\n2.  We presented a table showing the word whose tf-idf is maximum for each of Jane Austen's novels. Extend this comparison to show the words having the topmost tf-idf values. How would you present this comparison as a table? As a figure?\n\n3.  Following the example of a document-feature matrix (DFM), extract the inaugural addresses of 1861, 1933, and 1961 from `quanteda::data_corpus_inaugural`. For each token in each address, calculate its tf-idf to determine the tokens that most distinguish the three addresses.\n\n4.  In the preceding exercise, the meta-data give us the name of the speaker and the year of the address. How would you use that knowledge to evaluate a topic-modeling algorithm applied to the inaugural addresses? Time permitting, conduct such an evaluation of a topic-modeling method based on the `topicmodels` R package.\n\n## Resources\n\n[Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/) by Silge and Robinson\n\n[tf–idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n\n[CRAN Task View for Natural Language Processing](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)\n\n[Introduction to the tm Package: Text Mining in R](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)\n\n[Quantitative Analysis of Textual Data • quanteda](https://quanteda.io/)\n\n[Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) by David Blei, Andrew Ng, and Michael Jordan. JMLR (2003)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_2b_4350.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Text Analysis","subtitle":"Part 1, session 2b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Introduce basic ideas and methods of text analysis."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}