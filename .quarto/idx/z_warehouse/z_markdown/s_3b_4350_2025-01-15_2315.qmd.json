{"title":"Statistical Simulation","markdown":{"yaml":{"title":"Statistical Simulation","subtitle":"Part 1, session 3b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Introduce basic ideas and methods of statistical simulation studies."},"headingText":"library(gutenbergr)","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(here)\nlibrary(ISLR2)\n# library(janeaustenr)\nlibrary(latex2exp)\n# library(quanteda)\n# library(tidytext)\nlibrary(tidyverse)\nlibrary(tinytex)\n# library(tm)\n# library(tokenizers)\n# library(topicmodels)\nlibrary(tufte)\n\n```\n\n```{r local_source}\n# source(here(\"code\", \"rmse_per_grp.R\"))\n# source(here(\"code\", \"xtabs_to_jaccard.R\"))\nsource(here(\"code\", \"gen_circuit.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Introduction\n\n[TODO]\n\n## Generating Random Numbers from a Given Distribution\n\nAs mentioned in an earlier session, the R `stats` package provides random number generators for commonly used probability distributions. For each random number generator the package also provides the probability density function (for continuous distributions) or the probability mass function (for discrete distributions), along with cumulative versions (incomplete integrals or sums), and the inverse of the cumulative versions, the quantile functions. The naming convention is illustrated by the uniform distribution over a finite interval (unit interval by default).\n\n-   `dunif()` density at a given point $x$\n-   `punif()` cumulative probability, that $X \\le x$\n-   `qunif()` quantile, return $x$ having prescribed cumulative probability $p$\n-   `runif()` generate indpendent instances $(X_1, \\ldots, X_n)$ of a uniform random variable\n\n```{r rng_tbl}\nrng_tbl <- tibble::tribble(\n    ~fn, ~value, ~distribution,\n    \"rbeta\", \"dbl\", \"Beta\",\n    \"rcauchy\", \"dbl\", \"Cauchy\",\n    \"rchisq\", \"dbl\", \"(non-central) Chi-Squared\",\n    \"rexp\", \"dbl\", \"Exponential\",\n    \"rf\", \"dbl\", \"F\",\n    \"rgamma\", \"dbl\", \"Gamma\",\n    \"rlnorm\", \"dbl\", \"Log Normal\",\n    \"rlogis\", \"dbl\", \"Logistic\",\n    \"rnorm\", \"dbl\", \"Normal\",\n    \"rt\", \"dbl\", \"Student t\",\n    \"runif\", \"dbl\", \"Uniform\",\n    \"rweibull\", \"dbl\", \"Weibull\",\n    \"rbinom\", \"int\", \"Binomial\",\n    \"rgeom\", \"int\", \"Geometric\",\n    \"rhyper\", \"int\", \"Hypergeometric\",\n    \"rnbinom\", \"int\", \"Negative Binomial\",\n    \"rpois\", \"int\", \"Poisson\",\n  )\n```\n\n```{r dpqr_tbl}\n# separate function prefix (type) from remaining name\ndpqr_tbl <- rng_tbl |> \n  mutate(\n    prefix = \"[d, p, q, r]\", \n    suffix = stringr::str_remove(fn, \"r\")\n  ) |> \n  dplyr::select(\n    prefix, suffix, value, distribution\n  )\n```\n\n### Continuous Distributions\n\nThe `stats` package includes the following continuous distributions.\n\n```{r dpqr_dbl__kable}\ndpqr_tbl |> \n  filter(value == \"dbl\") |> \n  dplyr::select(- value) |> \n  knitr::kable(\n    caption = \"Continuous Distributions from `stats`\"\n  )\n```\n\nThese distributions have interesting histories and relationships. For example a beta variable $X$ having shape parameters $(\\alpha, \\beta)$ can be expressed as the ratio $U/(U + V)$, where $(U, V)$ are independent gamma variables having a common scale parameter, and respective shape parameters $(\\alpha, \\beta)$. See the resources listed below if interested.\n\n### Discrete Distributions\n\nThe `stats` package also includes the following discrete distributions.\n\n```{r dpqr_int__kable}\ndpqr_tbl |> \n  filter(value == \"int\") |> \n  dplyr::select(- value) |> \n  knitr::kable(\n    caption = \"Discrete Distributions from `stats`\"\n  )\n```\n\nAgain, these distributions have interesting histories and relationships. For example the Poisson distribution can described as the limiting case of binomial distributions, $\\text{Binom}(n, p_n)$ such that the respective expected values, $n \\times p_n$, converge to a finite, non-zero value $\\lambda$ for increasingly large $n$. Equivalently we can write $p_n \\approx \\lambda/n$, so that $p_n$, the probability of success on a single Bernoulli trial, becomes vanishingly small as the number $n$ of Bernoulli trials increases. For this reason the Poisson distribution has been called \"the law of rare events\".\n\n## Example Application\n\nStatistical simulation has been used to evaluate proposed statistical methods. Here's an admittedly tame example (where the answer has already been determined mathematically).\n\nLet $\\hat{\\mathcal{m}}$ and $\\hat{\\mu}$ denote the median and mean, respectively, of a random sample from a continuous distribution. For a symmetric distribution (with a defined population mean value), the population median and mean coincide. But the sample mean is highly sensitive to a few outlying values in the sample, and is thus less robust than the sample median for long-tailed distributions.\n\nConsequently, in financial applications and other applications where long-tailed distributions occur, the median is the preferred descriptor of the central value of the sample and of the population.\n\nIn some cases the robustness of the sample median comes at a cost. If the population distribution is normal with mean $\\mu$ and variance $\\sigma^2$ then the sample mean has smaller variance than the sample median (although they estimate the same population value).\n\n$$\n\\begin{align}\n  \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} &= \\frac{\\sigma^2}{n} \\\\ \n  \\\\ \n  \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} &= \\frac{\\pi}{2} \\frac{\\sigma^2}{n + 1} \\\\ \n  \\\\ \n  \\frac{ \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} }{ \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} } &\\approx \\frac{\\pi}{2} \\\\ \n  &\\approx 1.57\n\\end{align}\n$$\n\nThis ratio of variances in the normal case can be verified by simulation.\n\nIn this example, simulation would merely illustrate a property already determined mathematically. But in other situations, statistical simulation may be the best practical way to understand the properties of a proposed statistical procedure, or more generally, a system that entails random events.\n\n## Class Exercise\n\nWith a teammate, generate a sample of $n$ pseudo-random numbers following the normal distribution, for example using `stats::rnorm()`. Calculate $\\hat{\\mathcal{m}}$ and $\\hat{\\mu}$, the sample `median()` and `mean()`. Now repeat that process a total of $R$ times, recording the values of the sample mean and median in each run. This gives a sample of size $R$ of the pair $(\\hat{\\mathcal{m}}, \\hat{\\mu})$. What is the sample variance of these two estimators? Take 20 minutes to prepare to report your progress to the class.\n\n## Monte Carlo Simulation\n\nMonte Carlo simulation encompasses a broad set of algorithms that use random sampling to obtain numerical values.  Mathematician [Stanislav Ulam](https://en.wikipedia.org/wiki/Stanis%C5%82aw_Ulam) led the development of this approach (and coined the name) as part of the Manhattan Project during World War 2.  The approach is used when other types of numerical calculation are not feasible.\n\n### Example 1: estimate $\\pi$\n\nHere is a simple illustration.  Recall that a circle of radius $r$ has an area equal to $\\pi \\; r^2$.  Setting $r = 1$ we see that the unit circle has an area equal to $\\pi$.  Centered at the origin of the plane, the area is divided equally among the four quadrants of the plane.  Thus the area within the first quadrant equals $\\pi/4$.  We can use random sampling to estimate $\\pi$ as follows.\n\n  1.  Generate pairs of independent standard uniform random variables $\\{ (X_j, Y_j) \\}_{j = 1}^n$.  They constitute uniformly random points in the unit square.\n  1.  Define $B_j$ to be the Bernoulli indicator function, equal to 1 if $X_j^2 + Y_j^2 < 1$ and equal to zero otherwise.  Note that $B_j$ has expected value $\\pi/4$.\n  1.  Calculate the sample average $A(B_{\\bullet})$ as an estimate of $\\pi/4$.\n  1.  Multiply the sample average by 4 as an estimate of $\\pi$.\n\nThe precision of the estimate improves as the number $n$ of randomly generated points increases.\n\n### Example 2: estimate area of an arbitrary region\n\nThe figure below shows a simple closed curve within the unit square centered at the origin.\n\n```{r c_tbl}\nc_tbl <- gen_circuit_tbl()\n```\n\n```{r g_c_tbl}\ng_c_tbl <- c_tbl |> \n  ggplot(mapping = aes(x = c, y = s)) + \n  geom_path()\ng_c_tbl\n```\n\nThe closed curve defines an interior region $\\mathcal{R}$ bounded by the square $\\mathcal{S} = [-0.5, 0.5] \\times [-0.5, 0.5]$. We assume that for any point $(x, y) \\in \\mathcal{S}$ we can determine whether the point is inside or outside $\\mathcal{R}$. The Monte Carlo estimate of the area of this region is the proportion of points randomly selected from $\\mathcal{S}$ that fall within $\\mathcal{R}$ multiplied by the area of $\\mathcal{S}$ (which is 1 in this case).\n\nIn the current example, the closed curve shown above is a deformation of the unit circle in which the Euclidean norm at each point, $t$, along the circle has been multiplied by a prescribed positive continuous function, $f(t)$.  Consequently a point $(x, y)$ is in the interior of the closed curve if $x^2 + y^2 < f^2(t)$, where $t = \\arctan(y, x)$.\n\nThis is a common application of Mote Carlo simulation: estimating the volume of given region $\\mathcal{R}$ in Euclidean space, under the assumption that, for any given point, we can determine whether the point is inside or outside the region.\n\n### Importance Sampling\n\n[TODO]\n\n## Markov Chain Monte Carlo (MCMC)\n\n[TODO]\n\n## Team Exercises\n\n1.  As in the class exercise, generate $R$ samples, each a sample of size $n$, but this time use the Cauchy distribution (`rcauchy()`) rather than normal distribution (`rnorm()`). Calculate the sample variance and construct a histogram of the $R$ sample means:\n\n$$\n\\left\\{ \\hat{\\mu}_r \\right\\}_{r = 1}^R\n$$\n\n2.  Suppose $(U_1, \\ldots, U_n)$ are independent and identically distributed random variables following the standard uniform distribution on the interval $(0, 1)$. Now set $X_j = - \\log_e(U_j)$ for $j = 1, \\ldots, n$. What is the distribution of $(X_1, \\ldots, X_n)$?\n\n3.  In the Monte Carlo illustration above, what is the standard error of estimate of $\\pi$?\n\n## Resources\n\n[Distributions in Statistics](https://www.google.co.uk/books/edition/_/NSzEtAEACAAJ?hl=en&sa=X&ved=2ahUKEwiMpLHk3uiKAxUF9bsIHcSpBaIQre8FegQIGhAC) by Johnson and Kotz\n\n[Statistical Distributions](https://www.google.co.uk/books/edition/Statistical_Distributions/OA4AJZsnOpMC?hl=en) by Forbes, Evans, Hastings, and Peacock\n\n[R: Random Number Generation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html)\n\n[Monte Carlo method - Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_method)\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\n# library(gutenbergr)\nlibrary(here)\nlibrary(ISLR2)\n# library(janeaustenr)\nlibrary(latex2exp)\n# library(quanteda)\n# library(tidytext)\nlibrary(tidyverse)\nlibrary(tinytex)\n# library(tm)\n# library(tokenizers)\n# library(topicmodels)\nlibrary(tufte)\n\n```\n\n```{r local_source}\n# source(here(\"code\", \"rmse_per_grp.R\"))\n# source(here(\"code\", \"xtabs_to_jaccard.R\"))\nsource(here(\"code\", \"gen_circuit.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Introduction\n\n[TODO]\n\n## Generating Random Numbers from a Given Distribution\n\nAs mentioned in an earlier session, the R `stats` package provides random number generators for commonly used probability distributions. For each random number generator the package also provides the probability density function (for continuous distributions) or the probability mass function (for discrete distributions), along with cumulative versions (incomplete integrals or sums), and the inverse of the cumulative versions, the quantile functions. The naming convention is illustrated by the uniform distribution over a finite interval (unit interval by default).\n\n-   `dunif()` density at a given point $x$\n-   `punif()` cumulative probability, that $X \\le x$\n-   `qunif()` quantile, return $x$ having prescribed cumulative probability $p$\n-   `runif()` generate indpendent instances $(X_1, \\ldots, X_n)$ of a uniform random variable\n\n```{r rng_tbl}\nrng_tbl <- tibble::tribble(\n    ~fn, ~value, ~distribution,\n    \"rbeta\", \"dbl\", \"Beta\",\n    \"rcauchy\", \"dbl\", \"Cauchy\",\n    \"rchisq\", \"dbl\", \"(non-central) Chi-Squared\",\n    \"rexp\", \"dbl\", \"Exponential\",\n    \"rf\", \"dbl\", \"F\",\n    \"rgamma\", \"dbl\", \"Gamma\",\n    \"rlnorm\", \"dbl\", \"Log Normal\",\n    \"rlogis\", \"dbl\", \"Logistic\",\n    \"rnorm\", \"dbl\", \"Normal\",\n    \"rt\", \"dbl\", \"Student t\",\n    \"runif\", \"dbl\", \"Uniform\",\n    \"rweibull\", \"dbl\", \"Weibull\",\n    \"rbinom\", \"int\", \"Binomial\",\n    \"rgeom\", \"int\", \"Geometric\",\n    \"rhyper\", \"int\", \"Hypergeometric\",\n    \"rnbinom\", \"int\", \"Negative Binomial\",\n    \"rpois\", \"int\", \"Poisson\",\n  )\n```\n\n```{r dpqr_tbl}\n# separate function prefix (type) from remaining name\ndpqr_tbl <- rng_tbl |> \n  mutate(\n    prefix = \"[d, p, q, r]\", \n    suffix = stringr::str_remove(fn, \"r\")\n  ) |> \n  dplyr::select(\n    prefix, suffix, value, distribution\n  )\n```\n\n### Continuous Distributions\n\nThe `stats` package includes the following continuous distributions.\n\n```{r dpqr_dbl__kable}\ndpqr_tbl |> \n  filter(value == \"dbl\") |> \n  dplyr::select(- value) |> \n  knitr::kable(\n    caption = \"Continuous Distributions from `stats`\"\n  )\n```\n\nThese distributions have interesting histories and relationships. For example a beta variable $X$ having shape parameters $(\\alpha, \\beta)$ can be expressed as the ratio $U/(U + V)$, where $(U, V)$ are independent gamma variables having a common scale parameter, and respective shape parameters $(\\alpha, \\beta)$. See the resources listed below if interested.\n\n### Discrete Distributions\n\nThe `stats` package also includes the following discrete distributions.\n\n```{r dpqr_int__kable}\ndpqr_tbl |> \n  filter(value == \"int\") |> \n  dplyr::select(- value) |> \n  knitr::kable(\n    caption = \"Discrete Distributions from `stats`\"\n  )\n```\n\nAgain, these distributions have interesting histories and relationships. For example the Poisson distribution can described as the limiting case of binomial distributions, $\\text{Binom}(n, p_n)$ such that the respective expected values, $n \\times p_n$, converge to a finite, non-zero value $\\lambda$ for increasingly large $n$. Equivalently we can write $p_n \\approx \\lambda/n$, so that $p_n$, the probability of success on a single Bernoulli trial, becomes vanishingly small as the number $n$ of Bernoulli trials increases. For this reason the Poisson distribution has been called \"the law of rare events\".\n\n## Example Application\n\nStatistical simulation has been used to evaluate proposed statistical methods. Here's an admittedly tame example (where the answer has already been determined mathematically).\n\nLet $\\hat{\\mathcal{m}}$ and $\\hat{\\mu}$ denote the median and mean, respectively, of a random sample from a continuous distribution. For a symmetric distribution (with a defined population mean value), the population median and mean coincide. But the sample mean is highly sensitive to a few outlying values in the sample, and is thus less robust than the sample median for long-tailed distributions.\n\nConsequently, in financial applications and other applications where long-tailed distributions occur, the median is the preferred descriptor of the central value of the sample and of the population.\n\nIn some cases the robustness of the sample median comes at a cost. If the population distribution is normal with mean $\\mu$ and variance $\\sigma^2$ then the sample mean has smaller variance than the sample median (although they estimate the same population value).\n\n$$\n\\begin{align}\n  \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} &= \\frac{\\sigma^2}{n} \\\\ \n  \\\\ \n  \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} &= \\frac{\\pi}{2} \\frac{\\sigma^2}{n + 1} \\\\ \n  \\\\ \n  \\frac{ \\text{Var} \\left\\{ \\hat{\\mathcal{m}} \\right\\} }{ \\text{Var} \\left\\{ \\hat{\\mu} \\right\\} } &\\approx \\frac{\\pi}{2} \\\\ \n  &\\approx 1.57\n\\end{align}\n$$\n\nThis ratio of variances in the normal case can be verified by simulation.\n\nIn this example, simulation would merely illustrate a property already determined mathematically. But in other situations, statistical simulation may be the best practical way to understand the properties of a proposed statistical procedure, or more generally, a system that entails random events.\n\n## Class Exercise\n\nWith a teammate, generate a sample of $n$ pseudo-random numbers following the normal distribution, for example using `stats::rnorm()`. Calculate $\\hat{\\mathcal{m}}$ and $\\hat{\\mu}$, the sample `median()` and `mean()`. Now repeat that process a total of $R$ times, recording the values of the sample mean and median in each run. This gives a sample of size $R$ of the pair $(\\hat{\\mathcal{m}}, \\hat{\\mu})$. What is the sample variance of these two estimators? Take 20 minutes to prepare to report your progress to the class.\n\n## Monte Carlo Simulation\n\nMonte Carlo simulation encompasses a broad set of algorithms that use random sampling to obtain numerical values.  Mathematician [Stanislav Ulam](https://en.wikipedia.org/wiki/Stanis%C5%82aw_Ulam) led the development of this approach (and coined the name) as part of the Manhattan Project during World War 2.  The approach is used when other types of numerical calculation are not feasible.\n\n### Example 1: estimate $\\pi$\n\nHere is a simple illustration.  Recall that a circle of radius $r$ has an area equal to $\\pi \\; r^2$.  Setting $r = 1$ we see that the unit circle has an area equal to $\\pi$.  Centered at the origin of the plane, the area is divided equally among the four quadrants of the plane.  Thus the area within the first quadrant equals $\\pi/4$.  We can use random sampling to estimate $\\pi$ as follows.\n\n  1.  Generate pairs of independent standard uniform random variables $\\{ (X_j, Y_j) \\}_{j = 1}^n$.  They constitute uniformly random points in the unit square.\n  1.  Define $B_j$ to be the Bernoulli indicator function, equal to 1 if $X_j^2 + Y_j^2 < 1$ and equal to zero otherwise.  Note that $B_j$ has expected value $\\pi/4$.\n  1.  Calculate the sample average $A(B_{\\bullet})$ as an estimate of $\\pi/4$.\n  1.  Multiply the sample average by 4 as an estimate of $\\pi$.\n\nThe precision of the estimate improves as the number $n$ of randomly generated points increases.\n\n### Example 2: estimate area of an arbitrary region\n\nThe figure below shows a simple closed curve within the unit square centered at the origin.\n\n```{r c_tbl}\nc_tbl <- gen_circuit_tbl()\n```\n\n```{r g_c_tbl}\ng_c_tbl <- c_tbl |> \n  ggplot(mapping = aes(x = c, y = s)) + \n  geom_path()\ng_c_tbl\n```\n\nThe closed curve defines an interior region $\\mathcal{R}$ bounded by the square $\\mathcal{S} = [-0.5, 0.5] \\times [-0.5, 0.5]$. We assume that for any point $(x, y) \\in \\mathcal{S}$ we can determine whether the point is inside or outside $\\mathcal{R}$. The Monte Carlo estimate of the area of this region is the proportion of points randomly selected from $\\mathcal{S}$ that fall within $\\mathcal{R}$ multiplied by the area of $\\mathcal{S}$ (which is 1 in this case).\n\nIn the current example, the closed curve shown above is a deformation of the unit circle in which the Euclidean norm at each point, $t$, along the circle has been multiplied by a prescribed positive continuous function, $f(t)$.  Consequently a point $(x, y)$ is in the interior of the closed curve if $x^2 + y^2 < f^2(t)$, where $t = \\arctan(y, x)$.\n\nThis is a common application of Mote Carlo simulation: estimating the volume of given region $\\mathcal{R}$ in Euclidean space, under the assumption that, for any given point, we can determine whether the point is inside or outside the region.\n\n### Importance Sampling\n\n[TODO]\n\n## Markov Chain Monte Carlo (MCMC)\n\n[TODO]\n\n## Team Exercises\n\n1.  As in the class exercise, generate $R$ samples, each a sample of size $n$, but this time use the Cauchy distribution (`rcauchy()`) rather than normal distribution (`rnorm()`). Calculate the sample variance and construct a histogram of the $R$ sample means:\n\n$$\n\\left\\{ \\hat{\\mu}_r \\right\\}_{r = 1}^R\n$$\n\n2.  Suppose $(U_1, \\ldots, U_n)$ are independent and identically distributed random variables following the standard uniform distribution on the interval $(0, 1)$. Now set $X_j = - \\log_e(U_j)$ for $j = 1, \\ldots, n$. What is the distribution of $(X_1, \\ldots, X_n)$?\n\n3.  In the Monte Carlo illustration above, what is the standard error of estimate of $\\pi$?\n\n## Resources\n\n[Distributions in Statistics](https://www.google.co.uk/books/edition/_/NSzEtAEACAAJ?hl=en&sa=X&ved=2ahUKEwiMpLHk3uiKAxUF9bsIHcSpBaIQre8FegQIGhAC) by Johnson and Kotz\n\n[Statistical Distributions](https://www.google.co.uk/books/edition/Statistical_Distributions/OA4AJZsnOpMC?hl=en) by Forbes, Evans, Hastings, and Peacock\n\n[R: Random Number Generation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Random.html)\n\n[Monte Carlo method - Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_method)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_3b_4350_2025-01-15_2315.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Statistical Simulation","subtitle":"Part 1, session 3b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Introduce basic ideas and methods of statistical simulation studies."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}