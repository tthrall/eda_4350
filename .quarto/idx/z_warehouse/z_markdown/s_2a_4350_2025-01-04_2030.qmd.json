{"title":"Clustering: EDA in Higher Dimensions","markdown":{"yaml":{"title":"Clustering: EDA in Higher Dimensions","subtitle":"Part 1, session 2a of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Introduce statistical clustering methods."},"headingText":"library(mlr3measures)","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(dbscan)\nlibrary(GGally)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(latex2exp)\n# library(pracma)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"rmse_per_grp.R\"))\nsource(here(\"code\", \"xtabs_to_jaccard.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Initial Remarks\n\nIn the context of machine learning, \"unsupervised learning\" encompasses algorithms designed to seek patterns in data, with minimal required guidance from the user, and without the benefit of a response variable (or a set of observation \"labels\") to be approximated by functions of predictor variables.  Most but not all of these algorithms are designed for high-dimensional data, that is, data with many variables (columns).\n\nThis approach is in the spirit of exploratory data analysis (EDA), which is practically forced upon us when we come across a new data set (and can remain essential throughout the life-cycle of a project). On the one hand, EDA has no sharp measures of success that we are driven to optimize, so one might say that we're strolling through the data, taking in the scenery. On the other hand, we are looking for unanticipated patterns in the data, and therefore it is useful to record just what patterns we do anticipate, conjecture, or wonder about.\n\nIn our previous sessions we have reviewed some ways of looking at one or two variables at a time. When we are presented with a new data set having many variables, it is useful to simplify the data in some way to help us form a first impression.\n\nOne way is to look for a few functions of the many variables that somehow carry much of the original information. The prime method for doing so is principal component analysis (PCA), where the functions are linear and capture much of the variation in the data. We'll come back to PCA in subsequent sessions.\n\nIn this session we'll discuss another way to simplify the data, namely, to group observations having similar profiles (patterns in the values of the variables). Such clustering methods are currently of great interest in both science and industry, giving rise to a number of new clustering methods.\n\n## US Colleges\n\n```{r college}\ncollege <- ISLR2::College |> \n  as_tibble(rownames = \"college_name\")\n```\n\n```{r college_vars_tbl}\ncollege_vars_tbl <- tibble::tibble(\n  var_name = names(college),\n) |> \n  mutate(\n    dscr = case_when(\n      var_name == \"college_name\" ~ \n        \"Name of the college or university\", \n      var_name == \"Private\" ~ \n        \"No or Yes indicating private or public\", \n      var_name == \"Apps\" ~ \n        \"Number of applications received\", \n      var_name == \"Accept\" ~ \n        \"Number of applications accepted\", \n      var_name == \"Enroll\" ~ \n        \"Number of new students enrolled\", \n      var_name == \"Top10perc\" ~ \n        \"Pct. new students from top 10% of H.S. class\", \n      var_name == \"Top25perc\" ~ \n        \"Pct. new students from top 25% of H.S. class\", \n      var_name == \"F.Undergrad\" ~ \n        \"Number of fulltime undergraduates\", \n      var_name == \"P.Undergrad\" ~ \n        \"Number of parttime undergraduates\", \n      var_name == \"Outstate\" ~ \n        \"Out-of-state tuition\", \n      var_name == \"Room.Board\" ~ \n        \"Room and board costs\", \n      var_name == \"Books\" ~ \n        \"Estimated book costs\", \n      var_name == \"Personal\" ~ \n        \"Estimated personal spending\", \n      var_name == \"PhD\" ~ \n        \"Pct. of faculty with PhD's\", \n      var_name == \"Terminal\" ~ \n        \"Pct. of faculty with terminal degree\", \n      var_name == \"S.F.Ratio\" ~ \n        \"Student/faculty ratio\", \n      var_name == \"perc.alumni\" ~ \n        \"Pct. alumni who donate\", \n      var_name == \"Expend\" ~ \n        \"Instructional expenditure per student\", \n      var_name == \"Grad.Rate\" ~ \n        \"Graduation rate\"\n    )\n  )\n```\n\nTo illustrate ideas we'll use data about US universities and colleges from 1995, described in the book, *An Introduction to Statistical Learning with applications in R* ([ISLR](https://www.statlearning.com)). The data are recorded within the R package `ISLR2` as `ISLR2::College`.  From the R command `help(\"College\")` we see that the data consist of 777 observations (data rows), and obtain the following description of the variables (data columns).\n\n```{r college_vars_tbl__kable}\ncollege_vars_tbl |> knitr::kable(\n  caption = \"US college variables (1995 issue of USNWR)\", \n  col.names = c(\"variable\", \"description\")\n)\n```\n\n## Class Exercise\n\nTeam up with a classmate and make your own copy of the `ISLR2::College` data.  Record your questions and conjectures about the data.  Which of these could be addressed by the set of data variables?  Take 15 minutes to prepare to report out to the class.\n\n## Grouping Variables\n\nClustering algorithms construct groups of observations based solely on statistical information obtained from a given data set, with no reliance on the meaning or importance of the data variables.\n\nA complementary way to form groups is to use so-called \"grouping variables\" that we select or construct from the data.  Often, the grouping variables are evident from the questions we're trying to answer, e.g., to compare spending patterns among different socio-economic groups.\n\nWhen we want to construct groups (clusters) of observations as a means of exploring the data, we should consider what would make one grouping (clustering) more or less effective than another for this purpose.  We'll return to this point.\n\n### Example: college data grouping variables\n\nTo help us consider the goals and criteria of clustering (grouping), let's return to the US college data and choose a few variables with which to group the data.\n\nConsider the distinction between more expensive versus less expensive schools as indicated by variable `Expend` (instructional expenditure per student).  We might ask, \"Are the more expensive schools able to attract more of the top students?\"  So let's also use the variable `Top10perc` (percent of new students coming from the top 10% of their high school class).  We'll simply split each variable at its median value, creating two binary variables, which we'll use as our grouping variables.\n\nBefore proceeding, let's examine the distribution of these variables via density estimates and a scatter plot.\n\n```{r college_factors}\n# split selected variables at their median value\ncollege_factors <- college |> \n  mutate(\n    top_10_fct = Top10perc  < median(Top10perc),\n    expend_fct = Expend     < median(Expend)\n  ) |> \n  # for binary factors above, re-code (T, F) to (\"lwr\", \"upr\")\n  mutate(across(\n    .cols = c(top_10_fct, expend_fct), \n    .fns  = ~ if_else(.x, \"lwr\", \"upr\")\n  )) |> \n  # label each group with a bitcode\n  mutate(\n    top_10_lbl = if_else(top_10_fct == \"lwr\", 0L, 1L), \n    expend_lbl = if_else(expend_fct == \"lwr\", 0L, 1L), \n    grp_lbl    = paste0(\"g_\", top_10_lbl, expend_lbl)\n  ) |> \n  # remove redundant labeling vars\n  dplyr::select(- top_10_lbl, - expend_lbl)\n\n# record the median values\ntop_10_mid  <- median(college_factors$ Top10perc)\nexpend_mid  <- median(college_factors$ Expend)\n```\n\n```{r g_grp_var_matrix}\n# Form a matrix of figures, showing each \n# distinct pair of variables in a scatter diagram.\ng_grp_var_matrix <- college_factors |> \n  GGally::ggpairs(\n    columns = c(\"Top10perc\", \"Expend\")\n  )\ng_grp_var_matrix\n```\n\nWe see that `Top10perc` and `Expend` are positively correlated, and have relatively few departures from a monotonic relationship.  Also note that `Expend` is measured (in US dollars) on a very different scale from `Top10perc`.\n\nNow let's look at the two variables more closely, one at a time.\n\n```{r g_top_10}\ng_top_10 <- college_factors |> \n  ggplot(mapping = aes(x = Top10perc)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = top_10_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% of new students from top 10% in their HS\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_top_10\n```\n\n```{r g_Expend}\ng_Expend <- college_factors |> \n  ggplot(mapping = aes(x = Expend)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = expend_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"Instructional expenditure per student\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_Expend\n```\n\nThe figures above show decent variation on either side of the median value.  Therefore cutting each variable at its median value seems reasonable, as long as we bear in mind that each distribution has a right tail (a quite long tail for `Expend`).\n\nFor each of the variables shown we create a binary variable having two levels, \"lwr\" and \"upr\", meaning an observation is either below the median value, or else is no less than the median value.  We then form 4 groups, each a combination of levels of the 2 binary variables, which we label $g_{00}, g_{01}, g_{10}, g_{11}$ to denote the binary factors based on `Top10perc`, and `Expend` (in that order), with {\"lwr\", \"upr\"} coded as {0, 1}.\n\nTo check this grouping of the data let's first select the variables from which we defined the 4 groups and examine the scatter diagram, now using the color and shape of each data point to distinguish the different groups.\n\n```{r g_top_10_Expend}\ng_top_10_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:3) +\n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouped by binary versions of the two variables\"\n  )\ng_top_10_Expend\n```\n\nThe figure above confirms that we have constructed the binary grouping variables as we intended.\n\nThe table below shows the mean per group of the underlying grouping variables.\n\nNote: we've calculated the mean per group of _all_ the numeric variables.  We'll return to those calculations in the next section.\n\n```{r means_per_grp}\nmeans_per_grp <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # reduce grouping variables to just grp_lbl\n  dplyr::select(- top_10_fct, - expend_fct) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  ), \n  # grouping variable\n  .by = \"grp_lbl\", \n  # group size: number of original data rows per group\n  count = n()\n  ) |> \n  arrange(grp_lbl)\n```\n\n```{r means_per_grp__kable}\nmeans_per_grp |> \n  dplyr::select(\n    grp_lbl, count, Top10perc, Expend\n  ) |> \n  knitr::kable(\n    caption = \"Mean values per group of underlying grouping variables\", \n    col.names = c(\"group\", \"# obs\", \"Top10perc\", \"Expend\"), \n    digits = 1\n  )\n  \n```\n\n### Measures of clustering effectiveness\n\nIn the context of clustering algorithms, groups are not given but are rather to be calculated from the data.  But under what criteria?  Broadly speaking we want the points (each an observation of several numeric variables) to be close together within each cluster (group), and we want the clusters to be well separated from one another.\n\nSo how well do the groups constructed above perform against these criteria?  We want points within each cluster (group) to be close to one another, but how should we measure distance between points?  Euclidean distance springs to mind, but there is a catch: the numeric variables are recorded on differing scales.\n\n### Standardizing variables\n\nThe following table lists the mean, standard deviation, and coefficient of variation $(cv = \\frac{sd}{mean})$ per variable.  The variables having the largest mean value are listed first.\n\n```{r college_moments}\n# calculate unconditional mean and sd of each numeric variable\n\ncollege_means <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # remove grouping variables\n  dplyr::select(- top_10_fct, - expend_fct, - grp_lbl) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  )) |> \n  # pivot row vector to column vector\n  pivot_longer(\n    cols = everything(), \n    names_to = \"var\", \n    values_to = \"mean\"\n  )\n\ncollege_std_devs <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # remove grouping variables\n  dplyr::select(- top_10_fct, - expend_fct, - grp_lbl) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ sd(.x, na.rm = TRUE)\n  )) |> \n  # pivot row vector to column vector\n  pivot_longer(\n    cols = everything(), \n    names_to = \"var\", \n    values_to = \"sd\"\n  )\n\ncollege_moments <- college_means |> \n  left_join(\n    y  = college_std_devs, \n    by = \"var\"\n  ) |> \n  mutate(cv = sd / mean) |> \n  arrange(desc(mean))\n```\n\n```{r college_moments__kable}\ncollege_moments |> knitr::kable(\n  caption = \"Mean, SD, and CV = SD/Mean\", \n  col.names = c(\"variable\", \"mean\", \"sd\", \"cv\"), \n  digits = 1\n)\n```\n\nThe financial variables are measured in thousands of dollars, whereas the percentages merely range from 0 to 100.  The percentages thus contribute little to the overall Euclidean distance from a point (vector of observed numeric values) to a mean vector.  Consequently the percentages will have little influence on calculations of clustering effectiveness based on Euclidean distances.\n\nThis issue can be addressed in a few different ways.  For now we standardize each numeric variable: subtract the overall mean of the variable (across all observations) and then divide by the corresponding standard deviation of the variable.  This will make the respective scales of the variables more compatible.\n\n### Within-group distances in standard units\n\nHaving standardized the college numeric variables we now calculate Euclidean distances.  That is, within each group we calculate the distance of each point to the group mean vector.  This vector of point-minus-mean is referred to as \"error\" (more precisely, \"mean deviation\") in statistical parlance.  We calculate the squared Euclidean length of this vector, and sum these squared distances across all points in the group.  The result is called the (within-group) sum of squared errors (SSE).  Here are the calculated SSE values (and related statistics) per group.\n\n```{r college_z_scores}\n# convert numeric data into standard units (z-scores)\ncollege_z_scores <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # grouping variables: use only grp_lbl\n  dplyr::select(- top_10_fct, - expend_fct) |> \n  # standardize numeric variables\n  mutate(across(\n    .cols = - grp_lbl, \n    .fns  = ~ (.x - mean(.x))/sd(.x)\n  ))\n\n# count numeric variables (exclude grp_lbl)\nn_z_score_vars <- ncol(college_z_scores) - 1L\n```\n\n```{r coll_z_stats_per_grp}\n# compute z-score means and std-devs per group\n\n# number of observations per group as a data frame\ncoll_counts_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    count = n()\n  ) |> \n  arrange(grp_lbl) |> \n  column_to_rownames(\"grp_lbl\")\n\ncoll_z_means_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    across(\n      .cols = everything(), \n      .fns  = ~ mean(.x, na.rm = TRUE)\n    )\n  ) |> \n  arrange(grp_lbl)\n\ncoll_z_sds_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    across(\n      .cols = everything(), \n      .fns  = ~ sd(.x, na.rm = TRUE)\n    )\n  ) |> \n  arrange(grp_lbl)\n\n# transpose coll_z_sds_per_grp\ngrp_z_sds_per_var <- coll_z_sds_per_grp |> \n  column_to_rownames(\"grp_lbl\") |> \n  t() |> \n  as_tibble(rownames = \"var\")\n\ngrp_z_ss <- grp_z_sds_per_var |> \n  # (RMSE per var) to (MSE per var)\n  mutate(across(\n    .cols = - var, \n    .fns  = ~ .x^2\n  )) |> \n  # (MSE per var) to (SSE per var)\n  mutate(\n    g_00 = g_00 * (-1 + coll_counts_per_grp[[\"g_00\", \"count\"]]), \n    g_01 = g_01 * (-1 + coll_counts_per_grp[[\"g_01\", \"count\"]]), \n    g_10 = g_10 * (-1 + coll_counts_per_grp[[\"g_10\", \"count\"]]), \n    g_11 = g_11 * (-1 + coll_counts_per_grp[[\"g_11\", \"count\"]])\n  ) |> \n  # sum (SSE per var) across vars\n  summarise(across(\n    .cols = - var, \n    .fns  = ~ sum(.x, na.rm = TRUE)\n  )) |> \n  # transpose to facilitate summing across groups\n  pivot_longer(\n    cols = everything(), \n    names_to = \"group\", \n    values_to = \"SSE\"\n  ) |> \n  # calculate (count, df, MSE, RMSE) per group\n  mutate(\n    count = coll_counts_per_grp [, \"count\"], \n    df    = count - n_z_score_vars, \n    MSE   = SSE / df, \n    RMSE  = sqrt(MSE)\n  ) |> \n  dplyr::select(\n    group, count, df, RMSE, MSE, SSE\n  )\n\n# within sum of squares (across groups)\ngrp_z_ss_all <- sum(grp_z_ss$SSE, na.rm = TRUE)\n```\n\n```{r grp_z_ss__kable}\ngrp_z_ss |> knitr::kable(\n  caption = \"Point-center distances per group\", \n  digits = 1\n)\n```\n\nThe columns of the table are as follows.\n\n  - group: the label of each group\n  - count: the number of observations (data rows)\n  - df: count minus the number of numeric values (`r n_z_score_vars`)\n  - RMSE: $\\surd{\\text{MSE}}$\n  - MSE: SSE / df\n  - SSE: sum of squared point-center distances\n\nThe RMSE (root mean squared error) is a measure of the typical length of the point-center distances within each group\n\nThe sum of SSE across groups (the \"total within-group SSE\") turns out to be `r as.integer(round(grp_z_ss_all))`.  This value serves as a reference point for other groupings (clusterings) of observations.\n\n## K-means Clustering\n\nWe'll start our discussion of clustering using a function in the R `stats` package, namely `stats::kmeans()`.  The function requires us to specify either the desired number $K$ of clusters (groups) of observations to be formed, or else to provide an initial set of $K$ cluster centers. If we merely provide the desired number $K$ of clusters, `kmeans()` randomly selects $K$ data rows (observations) as the initial set of cluster centers. \n\nGiven a set of cluster centers the algorithm iteratively searches for a better set, meaning a set having a smaller \"within-cluster sum of squares\".  In broad terms, this is done in two steps.\n\n  1.  Assign: the algorithm assigns each data point to the nearest center (in Euclidean distance) thereby partitioning the data points into $K$ clusters.\n  1.  Update: For each cluster of data points, the algorihtm calculates a new cluster center, namely, the mean vector of the data points within the cluster.  This yields a new set of cluster centers.\n  \nConvergence criteria: For each cluster the algorithm adds up the (point, center) squared distances.  Those $K$ sums of squares per cluster are summed acrosss clusters to form the \"total within-cluster sum of squares (WCSS)\" (previously referred to as the \"total within-group SSE\").  In addition the algorithm records the grand mean vector, the average across the entire data set and forms a weighted sum of the squared (cluster-center, grand-mean) Euclidean distances, which is called the \"between-cluster sum of squares (BCSS)\".[^bcss]  The algorithm concludes the search when cluster-membership no longer changes, or when the decrease in WCSS is sufficiently small.\n\n[^bcss]: The sum of squared (point, grand-mean) distances does not depend on any grouping (clustering) of the observations, and is called the \"total sum of squares\".  The between-cluster sum of squares (BCSS) equals the total sum of squares minus the within-cluster sum of squares.  BCSS is also equal to a weighted sum of (cluster-center, grand-mean) squared distances.\n\n### Results for specified inital cluster centers\n\nWe now provide the `kmeans()` function with cluster centers in the form of the group means calculated above.[^cc_matrix]\n\n[^cc_matrix]: `kmeans()` expects the cluster centers to be expressed as a matrix, in which each _row_ represents a single cluster center and each column  represents a \"coordinate\" of the cluster centers corresponding to one of the numeric variables (columns) of the data matrix.\n\nThe resulting clusters are shown in the scatter diagrams below, where for purposes of comparison we have projected the clusters onto the grouping variables we previously adopted.\n\n```{r m_kmeans_ctrs}\n# to ensure reproducibility, set random seed\nset.seed(42) # nod to Hitchhiker's Guide to the Galaxy\n\n# use college_z_means_per_grp as initial cluster centers\nm_kmeans_ctrs <- college_z_scores |>\n  # remove grp_lbl\n  dplyr::select(- grp_lbl) |>\n  kmeans(\n    centers = coll_z_means_per_grp |>\n      # remove grp_lbl\n      dplyr::select(- grp_lbl)\n  )\n\n# record total within-cluster sum of squares\nkm_wcss <- m_kmeans_ctrs$ tot.withinss\n```\n\n```{r college_km_ctrs}\n# # assign observations to km clusters\ncollege_km_ctrs <- college_z_scores |>\n  mutate(cluster = m_kmeans_ctrs$ cluster)\n```\n\n```{r g_top_10_Expend_kmc}\ng_top_10_Expend_kmc <- college_km_ctrs |>\n  mutate(cluster = as_factor(cluster)) |>\n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend,\n    colour = cluster, shape = cluster\n  )) +\n  scale_shape_manual(values = 0:3) +\n  geom_point() +\n  labs(\n    title = \"(Top10perc, Expend)\",\n    subtitle = \"identified by kmeans cluster\"\n  )\ng_top_10_Expend_kmc\n```\n\n### Jaccard similarity\n\nThe figure above shows that the `kmeans()` clusters are distinct from but similar to the groups formed above whose mean values served as initial cluster centers.  Here are the number of observations belonging to each combination of $g_{x,y}$ group and `kmeans()` cluster.\n\n```{r xtabs_cluster_grp}\nxtabs_cluster_grp <- xtabs(\n  data    = college_km_ctrs, \n  formula = ~ cluster + grp_lbl\n)\nxtabs_cluster_grp\n```\n\nThe table above provides in more detail the similarities and differences between the inital (`grp_lbl`) and final (`cluster`) clusters from `kmeans()`.  The table can be represented and summarized by [Jaccard's measure](https://en.wikipedia.org/wiki/Jaccard_index) of the similarity of two finite sets, $(A, B)$, namely:\n\n$$\n\\begin{align}\n  J(A, B) &= \\frac{\\left| A \\cap B \\right|}{\\left| A \\cup B \\right|}\n\\end{align}\n$$\n\nLetting $(A, B)$ denote the respective (row, column) of each cell in the above table, we obtain the following table of Jaccard similarity measures of each (cluster, group) combination.\n\n```{r jaccard_cluster_grp_lst}\njaccard_cluster_grp_lst <- xtabs_cluster_grp |> \n  xtabs_to_jaccard()\n```\n\n```{r jaccard_cluster_grp_wide}\njaccard_cluster_grp_wide <- \n  jaccard_cluster_grp_lst [[\"xt_sim_long\"]] |> \n  dplyr::select(cluster, grp_lbl, jaccard) |> \n  pivot_wider(\n    names_from = \"grp_lbl\", \n    values_from = \"jaccard\"\n  )\n```\n\n```{r jaccard_cluster_grp_wide__kable}\njaccard_cluster_grp_wide |> \n  knitr::kable(\n    caption = \"Jaccard similarity measures\", \n    digits = 2\n  )\n```\n\n```{r jaccard_cluster_grp_avg}\njaccard_cluster_grp_avg <- jaccard_cluster_grp_lst [[2]]\n```\n\nThe weighted average (weighted by cell count) of these similarity values is `r round(jaccard_cluster_grp_avg, 2)`.\n\n### Distribution of point-center distances\n\nThe two figures below show the range of point-center distances for the original grouping and for the `kmeans()` clustering.  In the latter case, points are closer to the mean of the cluster, as can be seen by comparing the scale of the two figures.  The total within-cluster sum of squares is `r as.integer(round(km_wcss))` for `kmeans()` compared to `r as.integer(round(grp_z_ss_all))` for the original grouping.\n\n```{r group_norms_lst}\ngroup_norms_lst <- college_km_ctrs |> \n  dplyr::select(- cluster) |> \n  pt_ctr_dist(grp_name = \"grp_lbl\")\n```\n\n```{r cluster_norms_lst}\ncluster_norms_lst <- college_km_ctrs |> \n  dplyr::select(- grp_lbl) |> \n  pt_ctr_dist(grp_name = \"cluster\")\n```\n\n```{r g_grp_norms}\ngroup_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(grp_lbl)) + \n  labs(\n    title = \"Initial groups: point-center distances\"\n  )\n```\n\n```{r g_cluster_norms}\ncluster_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(cluster)) + \n  labs(\n    title = \"K-means clusters: point-center distances\", \n    subtitle = \"(initial centers from initial grouping)\"\n  )\n```\n\n### Results for random inital cluster centers\n\nLet's run `kmeans()` again, this time specifying only the desired number $K$ of clusters.  The function will then select $K$ rows of the data at random as the initial set of cluster centers.  In any case, no matter how the initial cluster centers may be selected, that selection influences the subsequent iterative formation of clusters.\n\nTo make the algorithm more robust we specify parameter `nstart`, the number times to run `kmeans()`, each using a distinct random selection of data rows as initial cluster centers.  For each run the algorithm computes WCSS (the total within-cluster sum of squares).  Once `nstart` runs have been completed, the run having the smallest value of WCSS is reported to the user.  A rule of thumb is to set `nstart` to a value between 20 and 50.\n\nWe now run `kmeans()` again, setting $K = 4$ and `nstart = 25`.  For purposes of comparison with previous results, here is a projection of the resulting 4 clusters onto a (Top10perc, Expend) scatter diagram.\n\n```{r m_kmeans_nstart}\n# to ensure reproducibility, set random seed\nset.seed(42)\n\nm_kmeans_nstart <- college_z_scores |>\n  # remove grp_lbl\n  dplyr::select(- grp_lbl) |>\n  kmeans(\n    centers = 4, \n    nstart = 25\n  )\n```\n\n```{r g_top_10_Expend_nstart}\ng_top_10_Expend_nstart <- college_z_scores |>\n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ) |>\n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend,\n    colour = cluster, shape = cluster\n  )) +\n  scale_shape_manual(values = 0:3) +\n  geom_point() +\n  labs(\n    title = \"(Top10perc, Expend)\",\n    subtitle = \"identified by kmeans cluster (nstart = 25)\"\n  )\ng_top_10_Expend_nstart\n```\n\nInterestingly, we again see strong separation of clusters associated with the values of `Top10perc` and `Expend`.\n\nThe table below shows the number of observations in each combination of the original grouping and the new `kmeans()` clusters.\n\n```{r xtabs_nstart_grp}\nxtabs_nstart_grp <- xtabs(\n  data    = college_z_scores |> \n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ), \n  formula = ~ cluster + grp_lbl\n)\nxtabs_nstart_grp\n```\nAs with the previous `kmeans()` clusters, we again see two of the clusters well aligned with groups $g_{00}$ and $g_{11}$, respectively.\n\nThe similarity of the current clusters with the previous clusters prompts us to examine the number of observations in each pair (old, new) clusters, distinguishing the two as (cl_group, cl_nstart).\n\n```{r xtabs_cl_group_nstart}\nxtabs_cl_group_nstart  <- xtabs(\n  data    = college_z_scores |> \n  mutate(\n    cl_group  = m_kmeans_ctrs$ cluster |> as_factor(), \n    cl_nstart = m_kmeans_nstart$ cluster |> as_factor()\n  ), \n  formula = ~ cl_group + cl_nstart\n)\nxtabs_cl_group_nstart\n```\n\nWe see that the new clusters are essentially a relabeling of the previous clusters.  The figure below of new point-center distances is essentially the same as the previous figure with the first two clusters swapped.  In fact the total within-cluster sum of squares is unchanged from the previous clustering.\n\n```{r nstart_norms_lst}\nnstart_norms_lst <- college_z_scores |> \n  dplyr::select(- grp_lbl) |> \n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ) |> \n  pt_ctr_dist(grp_name = \"cluster\")\n```\n\n```{r g_nstart_norms}\ng_nstart_norms <- nstart_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(cluster)) + \n  labs(\n    title    = \"K-means clusters: point-center distances\", \n    subtitle = \"(nstart = 25)\"\n  )\ng_nstart_norms\n```\n\n### Which value of $K$?\n\nSo far in our application of `kmeans()` to the college data we have set $K = 4$.  How would the results change with different choices for the value of $K$?  Breaking the observations into smaller but more numerous clusters should in general reduce the total within-cluster sum of squares (WCSS), but we can also expect to reach a point of diminishing returns.\n\nFor the college data, the figure below shows how WCSS decreases as $K$ increases.\n\n```{r mk_tbl}\n# run kmeans on college data for successive values of k\n\n# to ensure reproducibility, set random seed\nset.seed(42)\n\nk_max <- 12L\nnstart_per_k <- 20L\nmk_tbl <- tibble::tibble()\n\ncoll_z <- college_z_scores |> \n  # use only z-scores of original numeric variables\n  dplyr::select(- grp_lbl)\n\nfor (k in 2:k_max) {\n  m_tmp <- coll_z |> \n    kmeans(centers = k, nstart = nstart_per_k)\n  mk_tbl <- mk_tbl |> \n    dplyr::bind_rows(\n      tibble::tibble(\n        k    = k, \n        wcss = m_tmp$ tot.withinss)\n    )\n}\n```\n\n```{r g_mk_tbl}\ng_mk_tbl <- mk_tbl |> \n  ggplot(mapping = aes(\n    x = k, y = wcss\n  )) + \n  scale_x_continuous(\n    name = NULL, \n    breaks = seq(2, 12, 2)\n  ) + \n  geom_line() + \n  labs(\n    title = \"WCSS versus K for college z-scores\", \n    subtitle = \"(nstart = 20)\"\n  )\ng_mk_tbl\n```\n\nIn general, smaller values of $K$ are more interpretable.  Based on the figure above we might set $K$ to be 4 or 5.  This is a judgement call that depends on the circumstances and goals of one's project.\n\n## Other clustering algorithms\n\nClustering methods are an active area of research.  The K-means algorithm, first introduced in the 1930's, has the longest history.  More recent methods can be categorized along the following lines.\n\n  - Soft versus Hard: \"Hard\" clustering assigns each data point to a single cluster.  \"Soft\" clustering assigns to each point a finite probability distribution of cluster membership.\n  \n  - Agglomerative versus Divisive methods define the initial partition of the data into clusters in opposite ways.  Agglomerative clustering initially defines each data point to be a cluster, and then merges clusters.  Divisive clustering initially defines the entire data set to be a single cluster, and then breaks up this and subsequent clusters into smaller clusters.\n  \n  - Hierarchical or not: Instead of a partition, hierarchical methods create a hierarchy of parent (superset) and child (subset) clusters.\n  \n  - Centroid-based versus Density-based: K-means and other centroid-based methods identify a cluster of points with the mean, median, or other central measure of the points in the cluster.  Density-based methods find areas having a high density of points.\n\nSome methods of note are [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) (Density Based Spatial Clustering of Applications with Noise) and [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) (a hierarchical version of DBSCAN).\n\n## Team Exercises\n\n  1.  Iris data: Submit R command `help(\"iris\")` to learn about the measurements of iris flowers available in R: `datasets::iris`.  Summarise these measurements in a graph or table, for example by applying the `GGally::ggpairs()` function.\n\n  1.  Iris K-means: Apply the K-means algorithm to the iris measurements.  How do the computed clusters compare to the grouping by variable `Species`?  Count the number of observations corresponding to each combination of cluster and `Species`, for example by using R function `stats::xtabs()`.\n\n## Resources\n\n[An Introduction to Statistical Learning](https://www.statlearning.com/) by James, Witten, Hastie, & Tibshirani\n\n[K-means clustering with tidy data principles – tidymodels](https://www.tidymodels.org/learn/statistics/k-means/)\n\n[k-means • tidyclust](https://tidyclust.tidymodels.org/articles/k_means.html)\n\n[Cluster analysis - Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)\n\n[HDBSCAN with the dbscan package](https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html)\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(dbscan)\nlibrary(GGally)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(latex2exp)\n# library(mlr3measures)\n# library(pracma)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"rmse_per_grp.R\"))\nsource(here(\"code\", \"xtabs_to_jaccard.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Initial Remarks\n\nIn the context of machine learning, \"unsupervised learning\" encompasses algorithms designed to seek patterns in data, with minimal required guidance from the user, and without the benefit of a response variable (or a set of observation \"labels\") to be approximated by functions of predictor variables.  Most but not all of these algorithms are designed for high-dimensional data, that is, data with many variables (columns).\n\nThis approach is in the spirit of exploratory data analysis (EDA), which is practically forced upon us when we come across a new data set (and can remain essential throughout the life-cycle of a project). On the one hand, EDA has no sharp measures of success that we are driven to optimize, so one might say that we're strolling through the data, taking in the scenery. On the other hand, we are looking for unanticipated patterns in the data, and therefore it is useful to record just what patterns we do anticipate, conjecture, or wonder about.\n\nIn our previous sessions we have reviewed some ways of looking at one or two variables at a time. When we are presented with a new data set having many variables, it is useful to simplify the data in some way to help us form a first impression.\n\nOne way is to look for a few functions of the many variables that somehow carry much of the original information. The prime method for doing so is principal component analysis (PCA), where the functions are linear and capture much of the variation in the data. We'll come back to PCA in subsequent sessions.\n\nIn this session we'll discuss another way to simplify the data, namely, to group observations having similar profiles (patterns in the values of the variables). Such clustering methods are currently of great interest in both science and industry, giving rise to a number of new clustering methods.\n\n## US Colleges\n\n```{r college}\ncollege <- ISLR2::College |> \n  as_tibble(rownames = \"college_name\")\n```\n\n```{r college_vars_tbl}\ncollege_vars_tbl <- tibble::tibble(\n  var_name = names(college),\n) |> \n  mutate(\n    dscr = case_when(\n      var_name == \"college_name\" ~ \n        \"Name of the college or university\", \n      var_name == \"Private\" ~ \n        \"No or Yes indicating private or public\", \n      var_name == \"Apps\" ~ \n        \"Number of applications received\", \n      var_name == \"Accept\" ~ \n        \"Number of applications accepted\", \n      var_name == \"Enroll\" ~ \n        \"Number of new students enrolled\", \n      var_name == \"Top10perc\" ~ \n        \"Pct. new students from top 10% of H.S. class\", \n      var_name == \"Top25perc\" ~ \n        \"Pct. new students from top 25% of H.S. class\", \n      var_name == \"F.Undergrad\" ~ \n        \"Number of fulltime undergraduates\", \n      var_name == \"P.Undergrad\" ~ \n        \"Number of parttime undergraduates\", \n      var_name == \"Outstate\" ~ \n        \"Out-of-state tuition\", \n      var_name == \"Room.Board\" ~ \n        \"Room and board costs\", \n      var_name == \"Books\" ~ \n        \"Estimated book costs\", \n      var_name == \"Personal\" ~ \n        \"Estimated personal spending\", \n      var_name == \"PhD\" ~ \n        \"Pct. of faculty with PhD's\", \n      var_name == \"Terminal\" ~ \n        \"Pct. of faculty with terminal degree\", \n      var_name == \"S.F.Ratio\" ~ \n        \"Student/faculty ratio\", \n      var_name == \"perc.alumni\" ~ \n        \"Pct. alumni who donate\", \n      var_name == \"Expend\" ~ \n        \"Instructional expenditure per student\", \n      var_name == \"Grad.Rate\" ~ \n        \"Graduation rate\"\n    )\n  )\n```\n\nTo illustrate ideas we'll use data about US universities and colleges from 1995, described in the book, *An Introduction to Statistical Learning with applications in R* ([ISLR](https://www.statlearning.com)). The data are recorded within the R package `ISLR2` as `ISLR2::College`.  From the R command `help(\"College\")` we see that the data consist of 777 observations (data rows), and obtain the following description of the variables (data columns).\n\n```{r college_vars_tbl__kable}\ncollege_vars_tbl |> knitr::kable(\n  caption = \"US college variables (1995 issue of USNWR)\", \n  col.names = c(\"variable\", \"description\")\n)\n```\n\n## Class Exercise\n\nTeam up with a classmate and make your own copy of the `ISLR2::College` data.  Record your questions and conjectures about the data.  Which of these could be addressed by the set of data variables?  Take 15 minutes to prepare to report out to the class.\n\n## Grouping Variables\n\nClustering algorithms construct groups of observations based solely on statistical information obtained from a given data set, with no reliance on the meaning or importance of the data variables.\n\nA complementary way to form groups is to use so-called \"grouping variables\" that we select or construct from the data.  Often, the grouping variables are evident from the questions we're trying to answer, e.g., to compare spending patterns among different socio-economic groups.\n\nWhen we want to construct groups (clusters) of observations as a means of exploring the data, we should consider what would make one grouping (clustering) more or less effective than another for this purpose.  We'll return to this point.\n\n### Example: college data grouping variables\n\nTo help us consider the goals and criteria of clustering (grouping), let's return to the US college data and choose a few variables with which to group the data.\n\nConsider the distinction between more expensive versus less expensive schools as indicated by variable `Expend` (instructional expenditure per student).  We might ask, \"Are the more expensive schools able to attract more of the top students?\"  So let's also use the variable `Top10perc` (percent of new students coming from the top 10% of their high school class).  We'll simply split each variable at its median value, creating two binary variables, which we'll use as our grouping variables.\n\nBefore proceeding, let's examine the distribution of these variables via density estimates and a scatter plot.\n\n```{r college_factors}\n# split selected variables at their median value\ncollege_factors <- college |> \n  mutate(\n    top_10_fct = Top10perc  < median(Top10perc),\n    expend_fct = Expend     < median(Expend)\n  ) |> \n  # for binary factors above, re-code (T, F) to (\"lwr\", \"upr\")\n  mutate(across(\n    .cols = c(top_10_fct, expend_fct), \n    .fns  = ~ if_else(.x, \"lwr\", \"upr\")\n  )) |> \n  # label each group with a bitcode\n  mutate(\n    top_10_lbl = if_else(top_10_fct == \"lwr\", 0L, 1L), \n    expend_lbl = if_else(expend_fct == \"lwr\", 0L, 1L), \n    grp_lbl    = paste0(\"g_\", top_10_lbl, expend_lbl)\n  ) |> \n  # remove redundant labeling vars\n  dplyr::select(- top_10_lbl, - expend_lbl)\n\n# record the median values\ntop_10_mid  <- median(college_factors$ Top10perc)\nexpend_mid  <- median(college_factors$ Expend)\n```\n\n```{r g_grp_var_matrix}\n# Form a matrix of figures, showing each \n# distinct pair of variables in a scatter diagram.\ng_grp_var_matrix <- college_factors |> \n  GGally::ggpairs(\n    columns = c(\"Top10perc\", \"Expend\")\n  )\ng_grp_var_matrix\n```\n\nWe see that `Top10perc` and `Expend` are positively correlated, and have relatively few departures from a monotonic relationship.  Also note that `Expend` is measured (in US dollars) on a very different scale from `Top10perc`.\n\nNow let's look at the two variables more closely, one at a time.\n\n```{r g_top_10}\ng_top_10 <- college_factors |> \n  ggplot(mapping = aes(x = Top10perc)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = top_10_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% of new students from top 10% in their HS\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_top_10\n```\n\n```{r g_Expend}\ng_Expend <- college_factors |> \n  ggplot(mapping = aes(x = Expend)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = expend_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"Instructional expenditure per student\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_Expend\n```\n\nThe figures above show decent variation on either side of the median value.  Therefore cutting each variable at its median value seems reasonable, as long as we bear in mind that each distribution has a right tail (a quite long tail for `Expend`).\n\nFor each of the variables shown we create a binary variable having two levels, \"lwr\" and \"upr\", meaning an observation is either below the median value, or else is no less than the median value.  We then form 4 groups, each a combination of levels of the 2 binary variables, which we label $g_{00}, g_{01}, g_{10}, g_{11}$ to denote the binary factors based on `Top10perc`, and `Expend` (in that order), with {\"lwr\", \"upr\"} coded as {0, 1}.\n\nTo check this grouping of the data let's first select the variables from which we defined the 4 groups and examine the scatter diagram, now using the color and shape of each data point to distinguish the different groups.\n\n```{r g_top_10_Expend}\ng_top_10_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:3) +\n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouped by binary versions of the two variables\"\n  )\ng_top_10_Expend\n```\n\nThe figure above confirms that we have constructed the binary grouping variables as we intended.\n\nThe table below shows the mean per group of the underlying grouping variables.\n\nNote: we've calculated the mean per group of _all_ the numeric variables.  We'll return to those calculations in the next section.\n\n```{r means_per_grp}\nmeans_per_grp <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # reduce grouping variables to just grp_lbl\n  dplyr::select(- top_10_fct, - expend_fct) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  ), \n  # grouping variable\n  .by = \"grp_lbl\", \n  # group size: number of original data rows per group\n  count = n()\n  ) |> \n  arrange(grp_lbl)\n```\n\n```{r means_per_grp__kable}\nmeans_per_grp |> \n  dplyr::select(\n    grp_lbl, count, Top10perc, Expend\n  ) |> \n  knitr::kable(\n    caption = \"Mean values per group of underlying grouping variables\", \n    col.names = c(\"group\", \"# obs\", \"Top10perc\", \"Expend\"), \n    digits = 1\n  )\n  \n```\n\n### Measures of clustering effectiveness\n\nIn the context of clustering algorithms, groups are not given but are rather to be calculated from the data.  But under what criteria?  Broadly speaking we want the points (each an observation of several numeric variables) to be close together within each cluster (group), and we want the clusters to be well separated from one another.\n\nSo how well do the groups constructed above perform against these criteria?  We want points within each cluster (group) to be close to one another, but how should we measure distance between points?  Euclidean distance springs to mind, but there is a catch: the numeric variables are recorded on differing scales.\n\n### Standardizing variables\n\nThe following table lists the mean, standard deviation, and coefficient of variation $(cv = \\frac{sd}{mean})$ per variable.  The variables having the largest mean value are listed first.\n\n```{r college_moments}\n# calculate unconditional mean and sd of each numeric variable\n\ncollege_means <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # remove grouping variables\n  dplyr::select(- top_10_fct, - expend_fct, - grp_lbl) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  )) |> \n  # pivot row vector to column vector\n  pivot_longer(\n    cols = everything(), \n    names_to = \"var\", \n    values_to = \"mean\"\n  )\n\ncollege_std_devs <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # remove grouping variables\n  dplyr::select(- top_10_fct, - expend_fct, - grp_lbl) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ sd(.x, na.rm = TRUE)\n  )) |> \n  # pivot row vector to column vector\n  pivot_longer(\n    cols = everything(), \n    names_to = \"var\", \n    values_to = \"sd\"\n  )\n\ncollege_moments <- college_means |> \n  left_join(\n    y  = college_std_devs, \n    by = \"var\"\n  ) |> \n  mutate(cv = sd / mean) |> \n  arrange(desc(mean))\n```\n\n```{r college_moments__kable}\ncollege_moments |> knitr::kable(\n  caption = \"Mean, SD, and CV = SD/Mean\", \n  col.names = c(\"variable\", \"mean\", \"sd\", \"cv\"), \n  digits = 1\n)\n```\n\nThe financial variables are measured in thousands of dollars, whereas the percentages merely range from 0 to 100.  The percentages thus contribute little to the overall Euclidean distance from a point (vector of observed numeric values) to a mean vector.  Consequently the percentages will have little influence on calculations of clustering effectiveness based on Euclidean distances.\n\nThis issue can be addressed in a few different ways.  For now we standardize each numeric variable: subtract the overall mean of the variable (across all observations) and then divide by the corresponding standard deviation of the variable.  This will make the respective scales of the variables more compatible.\n\n### Within-group distances in standard units\n\nHaving standardized the college numeric variables we now calculate Euclidean distances.  That is, within each group we calculate the distance of each point to the group mean vector.  This vector of point-minus-mean is referred to as \"error\" (more precisely, \"mean deviation\") in statistical parlance.  We calculate the squared Euclidean length of this vector, and sum these squared distances across all points in the group.  The result is called the (within-group) sum of squared errors (SSE).  Here are the calculated SSE values (and related statistics) per group.\n\n```{r college_z_scores}\n# convert numeric data into standard units (z-scores)\ncollege_z_scores <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # grouping variables: use only grp_lbl\n  dplyr::select(- top_10_fct, - expend_fct) |> \n  # standardize numeric variables\n  mutate(across(\n    .cols = - grp_lbl, \n    .fns  = ~ (.x - mean(.x))/sd(.x)\n  ))\n\n# count numeric variables (exclude grp_lbl)\nn_z_score_vars <- ncol(college_z_scores) - 1L\n```\n\n```{r coll_z_stats_per_grp}\n# compute z-score means and std-devs per group\n\n# number of observations per group as a data frame\ncoll_counts_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    count = n()\n  ) |> \n  arrange(grp_lbl) |> \n  column_to_rownames(\"grp_lbl\")\n\ncoll_z_means_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    across(\n      .cols = everything(), \n      .fns  = ~ mean(.x, na.rm = TRUE)\n    )\n  ) |> \n  arrange(grp_lbl)\n\ncoll_z_sds_per_grp <- college_z_scores |> \n  summarise(\n    .by = grp_lbl, \n    across(\n      .cols = everything(), \n      .fns  = ~ sd(.x, na.rm = TRUE)\n    )\n  ) |> \n  arrange(grp_lbl)\n\n# transpose coll_z_sds_per_grp\ngrp_z_sds_per_var <- coll_z_sds_per_grp |> \n  column_to_rownames(\"grp_lbl\") |> \n  t() |> \n  as_tibble(rownames = \"var\")\n\ngrp_z_ss <- grp_z_sds_per_var |> \n  # (RMSE per var) to (MSE per var)\n  mutate(across(\n    .cols = - var, \n    .fns  = ~ .x^2\n  )) |> \n  # (MSE per var) to (SSE per var)\n  mutate(\n    g_00 = g_00 * (-1 + coll_counts_per_grp[[\"g_00\", \"count\"]]), \n    g_01 = g_01 * (-1 + coll_counts_per_grp[[\"g_01\", \"count\"]]), \n    g_10 = g_10 * (-1 + coll_counts_per_grp[[\"g_10\", \"count\"]]), \n    g_11 = g_11 * (-1 + coll_counts_per_grp[[\"g_11\", \"count\"]])\n  ) |> \n  # sum (SSE per var) across vars\n  summarise(across(\n    .cols = - var, \n    .fns  = ~ sum(.x, na.rm = TRUE)\n  )) |> \n  # transpose to facilitate summing across groups\n  pivot_longer(\n    cols = everything(), \n    names_to = \"group\", \n    values_to = \"SSE\"\n  ) |> \n  # calculate (count, df, MSE, RMSE) per group\n  mutate(\n    count = coll_counts_per_grp [, \"count\"], \n    df    = count - n_z_score_vars, \n    MSE   = SSE / df, \n    RMSE  = sqrt(MSE)\n  ) |> \n  dplyr::select(\n    group, count, df, RMSE, MSE, SSE\n  )\n\n# within sum of squares (across groups)\ngrp_z_ss_all <- sum(grp_z_ss$SSE, na.rm = TRUE)\n```\n\n```{r grp_z_ss__kable}\ngrp_z_ss |> knitr::kable(\n  caption = \"Point-center distances per group\", \n  digits = 1\n)\n```\n\nThe columns of the table are as follows.\n\n  - group: the label of each group\n  - count: the number of observations (data rows)\n  - df: count minus the number of numeric values (`r n_z_score_vars`)\n  - RMSE: $\\surd{\\text{MSE}}$\n  - MSE: SSE / df\n  - SSE: sum of squared point-center distances\n\nThe RMSE (root mean squared error) is a measure of the typical length of the point-center distances within each group\n\nThe sum of SSE across groups (the \"total within-group SSE\") turns out to be `r as.integer(round(grp_z_ss_all))`.  This value serves as a reference point for other groupings (clusterings) of observations.\n\n## K-means Clustering\n\nWe'll start our discussion of clustering using a function in the R `stats` package, namely `stats::kmeans()`.  The function requires us to specify either the desired number $K$ of clusters (groups) of observations to be formed, or else to provide an initial set of $K$ cluster centers. If we merely provide the desired number $K$ of clusters, `kmeans()` randomly selects $K$ data rows (observations) as the initial set of cluster centers. \n\nGiven a set of cluster centers the algorithm iteratively searches for a better set, meaning a set having a smaller \"within-cluster sum of squares\".  In broad terms, this is done in two steps.\n\n  1.  Assign: the algorithm assigns each data point to the nearest center (in Euclidean distance) thereby partitioning the data points into $K$ clusters.\n  1.  Update: For each cluster of data points, the algorihtm calculates a new cluster center, namely, the mean vector of the data points within the cluster.  This yields a new set of cluster centers.\n  \nConvergence criteria: For each cluster the algorithm adds up the (point, center) squared distances.  Those $K$ sums of squares per cluster are summed acrosss clusters to form the \"total within-cluster sum of squares (WCSS)\" (previously referred to as the \"total within-group SSE\").  In addition the algorithm records the grand mean vector, the average across the entire data set and forms a weighted sum of the squared (cluster-center, grand-mean) Euclidean distances, which is called the \"between-cluster sum of squares (BCSS)\".[^bcss]  The algorithm concludes the search when cluster-membership no longer changes, or when the decrease in WCSS is sufficiently small.\n\n[^bcss]: The sum of squared (point, grand-mean) distances does not depend on any grouping (clustering) of the observations, and is called the \"total sum of squares\".  The between-cluster sum of squares (BCSS) equals the total sum of squares minus the within-cluster sum of squares.  BCSS is also equal to a weighted sum of (cluster-center, grand-mean) squared distances.\n\n### Results for specified inital cluster centers\n\nWe now provide the `kmeans()` function with cluster centers in the form of the group means calculated above.[^cc_matrix]\n\n[^cc_matrix]: `kmeans()` expects the cluster centers to be expressed as a matrix, in which each _row_ represents a single cluster center and each column  represents a \"coordinate\" of the cluster centers corresponding to one of the numeric variables (columns) of the data matrix.\n\nThe resulting clusters are shown in the scatter diagrams below, where for purposes of comparison we have projected the clusters onto the grouping variables we previously adopted.\n\n```{r m_kmeans_ctrs}\n# to ensure reproducibility, set random seed\nset.seed(42) # nod to Hitchhiker's Guide to the Galaxy\n\n# use college_z_means_per_grp as initial cluster centers\nm_kmeans_ctrs <- college_z_scores |>\n  # remove grp_lbl\n  dplyr::select(- grp_lbl) |>\n  kmeans(\n    centers = coll_z_means_per_grp |>\n      # remove grp_lbl\n      dplyr::select(- grp_lbl)\n  )\n\n# record total within-cluster sum of squares\nkm_wcss <- m_kmeans_ctrs$ tot.withinss\n```\n\n```{r college_km_ctrs}\n# # assign observations to km clusters\ncollege_km_ctrs <- college_z_scores |>\n  mutate(cluster = m_kmeans_ctrs$ cluster)\n```\n\n```{r g_top_10_Expend_kmc}\ng_top_10_Expend_kmc <- college_km_ctrs |>\n  mutate(cluster = as_factor(cluster)) |>\n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend,\n    colour = cluster, shape = cluster\n  )) +\n  scale_shape_manual(values = 0:3) +\n  geom_point() +\n  labs(\n    title = \"(Top10perc, Expend)\",\n    subtitle = \"identified by kmeans cluster\"\n  )\ng_top_10_Expend_kmc\n```\n\n### Jaccard similarity\n\nThe figure above shows that the `kmeans()` clusters are distinct from but similar to the groups formed above whose mean values served as initial cluster centers.  Here are the number of observations belonging to each combination of $g_{x,y}$ group and `kmeans()` cluster.\n\n```{r xtabs_cluster_grp}\nxtabs_cluster_grp <- xtabs(\n  data    = college_km_ctrs, \n  formula = ~ cluster + grp_lbl\n)\nxtabs_cluster_grp\n```\n\nThe table above provides in more detail the similarities and differences between the inital (`grp_lbl`) and final (`cluster`) clusters from `kmeans()`.  The table can be represented and summarized by [Jaccard's measure](https://en.wikipedia.org/wiki/Jaccard_index) of the similarity of two finite sets, $(A, B)$, namely:\n\n$$\n\\begin{align}\n  J(A, B) &= \\frac{\\left| A \\cap B \\right|}{\\left| A \\cup B \\right|}\n\\end{align}\n$$\n\nLetting $(A, B)$ denote the respective (row, column) of each cell in the above table, we obtain the following table of Jaccard similarity measures of each (cluster, group) combination.\n\n```{r jaccard_cluster_grp_lst}\njaccard_cluster_grp_lst <- xtabs_cluster_grp |> \n  xtabs_to_jaccard()\n```\n\n```{r jaccard_cluster_grp_wide}\njaccard_cluster_grp_wide <- \n  jaccard_cluster_grp_lst [[\"xt_sim_long\"]] |> \n  dplyr::select(cluster, grp_lbl, jaccard) |> \n  pivot_wider(\n    names_from = \"grp_lbl\", \n    values_from = \"jaccard\"\n  )\n```\n\n```{r jaccard_cluster_grp_wide__kable}\njaccard_cluster_grp_wide |> \n  knitr::kable(\n    caption = \"Jaccard similarity measures\", \n    digits = 2\n  )\n```\n\n```{r jaccard_cluster_grp_avg}\njaccard_cluster_grp_avg <- jaccard_cluster_grp_lst [[2]]\n```\n\nThe weighted average (weighted by cell count) of these similarity values is `r round(jaccard_cluster_grp_avg, 2)`.\n\n### Distribution of point-center distances\n\nThe two figures below show the range of point-center distances for the original grouping and for the `kmeans()` clustering.  In the latter case, points are closer to the mean of the cluster, as can be seen by comparing the scale of the two figures.  The total within-cluster sum of squares is `r as.integer(round(km_wcss))` for `kmeans()` compared to `r as.integer(round(grp_z_ss_all))` for the original grouping.\n\n```{r group_norms_lst}\ngroup_norms_lst <- college_km_ctrs |> \n  dplyr::select(- cluster) |> \n  pt_ctr_dist(grp_name = \"grp_lbl\")\n```\n\n```{r cluster_norms_lst}\ncluster_norms_lst <- college_km_ctrs |> \n  dplyr::select(- grp_lbl) |> \n  pt_ctr_dist(grp_name = \"cluster\")\n```\n\n```{r g_grp_norms}\ngroup_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(grp_lbl)) + \n  labs(\n    title = \"Initial groups: point-center distances\"\n  )\n```\n\n```{r g_cluster_norms}\ncluster_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(cluster)) + \n  labs(\n    title = \"K-means clusters: point-center distances\", \n    subtitle = \"(initial centers from initial grouping)\"\n  )\n```\n\n### Results for random inital cluster centers\n\nLet's run `kmeans()` again, this time specifying only the desired number $K$ of clusters.  The function will then select $K$ rows of the data at random as the initial set of cluster centers.  In any case, no matter how the initial cluster centers may be selected, that selection influences the subsequent iterative formation of clusters.\n\nTo make the algorithm more robust we specify parameter `nstart`, the number times to run `kmeans()`, each using a distinct random selection of data rows as initial cluster centers.  For each run the algorithm computes WCSS (the total within-cluster sum of squares).  Once `nstart` runs have been completed, the run having the smallest value of WCSS is reported to the user.  A rule of thumb is to set `nstart` to a value between 20 and 50.\n\nWe now run `kmeans()` again, setting $K = 4$ and `nstart = 25`.  For purposes of comparison with previous results, here is a projection of the resulting 4 clusters onto a (Top10perc, Expend) scatter diagram.\n\n```{r m_kmeans_nstart}\n# to ensure reproducibility, set random seed\nset.seed(42)\n\nm_kmeans_nstart <- college_z_scores |>\n  # remove grp_lbl\n  dplyr::select(- grp_lbl) |>\n  kmeans(\n    centers = 4, \n    nstart = 25\n  )\n```\n\n```{r g_top_10_Expend_nstart}\ng_top_10_Expend_nstart <- college_z_scores |>\n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ) |>\n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend,\n    colour = cluster, shape = cluster\n  )) +\n  scale_shape_manual(values = 0:3) +\n  geom_point() +\n  labs(\n    title = \"(Top10perc, Expend)\",\n    subtitle = \"identified by kmeans cluster (nstart = 25)\"\n  )\ng_top_10_Expend_nstart\n```\n\nInterestingly, we again see strong separation of clusters associated with the values of `Top10perc` and `Expend`.\n\nThe table below shows the number of observations in each combination of the original grouping and the new `kmeans()` clusters.\n\n```{r xtabs_nstart_grp}\nxtabs_nstart_grp <- xtabs(\n  data    = college_z_scores |> \n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ), \n  formula = ~ cluster + grp_lbl\n)\nxtabs_nstart_grp\n```\nAs with the previous `kmeans()` clusters, we again see two of the clusters well aligned with groups $g_{00}$ and $g_{11}$, respectively.\n\nThe similarity of the current clusters with the previous clusters prompts us to examine the number of observations in each pair (old, new) clusters, distinguishing the two as (cl_group, cl_nstart).\n\n```{r xtabs_cl_group_nstart}\nxtabs_cl_group_nstart  <- xtabs(\n  data    = college_z_scores |> \n  mutate(\n    cl_group  = m_kmeans_ctrs$ cluster |> as_factor(), \n    cl_nstart = m_kmeans_nstart$ cluster |> as_factor()\n  ), \n  formula = ~ cl_group + cl_nstart\n)\nxtabs_cl_group_nstart\n```\n\nWe see that the new clusters are essentially a relabeling of the previous clusters.  The figure below of new point-center distances is essentially the same as the previous figure with the first two clusters swapped.  In fact the total within-cluster sum of squares is unchanged from the previous clustering.\n\n```{r nstart_norms_lst}\nnstart_norms_lst <- college_z_scores |> \n  dplyr::select(- grp_lbl) |> \n  mutate(\n    cluster = m_kmeans_nstart$ cluster |> as_factor()\n  ) |> \n  pt_ctr_dist(grp_name = \"cluster\")\n```\n\n```{r g_nstart_norms}\ng_nstart_norms <- nstart_norms_lst$ grp_norms |> \n  ggplot(mapping = aes(y = norm)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(cluster)) + \n  labs(\n    title    = \"K-means clusters: point-center distances\", \n    subtitle = \"(nstart = 25)\"\n  )\ng_nstart_norms\n```\n\n### Which value of $K$?\n\nSo far in our application of `kmeans()` to the college data we have set $K = 4$.  How would the results change with different choices for the value of $K$?  Breaking the observations into smaller but more numerous clusters should in general reduce the total within-cluster sum of squares (WCSS), but we can also expect to reach a point of diminishing returns.\n\nFor the college data, the figure below shows how WCSS decreases as $K$ increases.\n\n```{r mk_tbl}\n# run kmeans on college data for successive values of k\n\n# to ensure reproducibility, set random seed\nset.seed(42)\n\nk_max <- 12L\nnstart_per_k <- 20L\nmk_tbl <- tibble::tibble()\n\ncoll_z <- college_z_scores |> \n  # use only z-scores of original numeric variables\n  dplyr::select(- grp_lbl)\n\nfor (k in 2:k_max) {\n  m_tmp <- coll_z |> \n    kmeans(centers = k, nstart = nstart_per_k)\n  mk_tbl <- mk_tbl |> \n    dplyr::bind_rows(\n      tibble::tibble(\n        k    = k, \n        wcss = m_tmp$ tot.withinss)\n    )\n}\n```\n\n```{r g_mk_tbl}\ng_mk_tbl <- mk_tbl |> \n  ggplot(mapping = aes(\n    x = k, y = wcss\n  )) + \n  scale_x_continuous(\n    name = NULL, \n    breaks = seq(2, 12, 2)\n  ) + \n  geom_line() + \n  labs(\n    title = \"WCSS versus K for college z-scores\", \n    subtitle = \"(nstart = 20)\"\n  )\ng_mk_tbl\n```\n\nIn general, smaller values of $K$ are more interpretable.  Based on the figure above we might set $K$ to be 4 or 5.  This is a judgement call that depends on the circumstances and goals of one's project.\n\n## Other clustering algorithms\n\nClustering methods are an active area of research.  The K-means algorithm, first introduced in the 1930's, has the longest history.  More recent methods can be categorized along the following lines.\n\n  - Soft versus Hard: \"Hard\" clustering assigns each data point to a single cluster.  \"Soft\" clustering assigns to each point a finite probability distribution of cluster membership.\n  \n  - Agglomerative versus Divisive methods define the initial partition of the data into clusters in opposite ways.  Agglomerative clustering initially defines each data point to be a cluster, and then merges clusters.  Divisive clustering initially defines the entire data set to be a single cluster, and then breaks up this and subsequent clusters into smaller clusters.\n  \n  - Hierarchical or not: Instead of a partition, hierarchical methods create a hierarchy of parent (superset) and child (subset) clusters.\n  \n  - Centroid-based versus Density-based: K-means and other centroid-based methods identify a cluster of points with the mean, median, or other central measure of the points in the cluster.  Density-based methods find areas having a high density of points.\n\nSome methods of note are [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) (Density Based Spatial Clustering of Applications with Noise) and [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) (a hierarchical version of DBSCAN).\n\n## Team Exercises\n\n  1.  Iris data: Submit R command `help(\"iris\")` to learn about the measurements of iris flowers available in R: `datasets::iris`.  Summarise these measurements in a graph or table, for example by applying the `GGally::ggpairs()` function.\n\n  1.  Iris K-means: Apply the K-means algorithm to the iris measurements.  How do the computed clusters compare to the grouping by variable `Species`?  Count the number of observations corresponding to each combination of cluster and `Species`, for example by using R function `stats::xtabs()`.\n\n## Resources\n\n[An Introduction to Statistical Learning](https://www.statlearning.com/) by James, Witten, Hastie, & Tibshirani\n\n[K-means clustering with tidy data principles – tidymodels](https://www.tidymodels.org/learn/statistics/k-means/)\n\n[k-means • tidyclust](https://tidyclust.tidymodels.org/articles/k_means.html)\n\n[Cluster analysis - Wikipedia](https://en.wikipedia.org/wiki/Cluster_analysis)\n\n[HDBSCAN with the dbscan package](https://cran.r-project.org/web/packages/dbscan/vignettes/hdbscan.html)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_2a_4350_2025-01-04_2030.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Clustering: EDA in Higher Dimensions","subtitle":"Part 1, session 2a of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Introduce statistical clustering methods."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}