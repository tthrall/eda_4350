{"title":"Conditional Distributions","markdown":{"yaml":{"title":"Conditional Distributions","subtitle":"Part 1, session 1b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Review concepts and techniques of exploratory data analysis."},"headingText":"Heights of Fathers and Sons","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(here)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"handedness_data.R\"))\nsource(here(\"code\", \"rng_tbl.R\"))\n```\n\n------------------------------------------------------------------------\n\n\n```{r father_son_ht}\nfather_son_ht <- UsingR::father.son |> \n  as_tibble() |> \n  rename(father = fheight, son = sheight)\n```\n\n```{r fs_moments}\n# vector of first and second central moments\nfs_moments <- father_son_ht |> \n  summarise(\n    f_avg = mean(father, na.rm = TRUE), \n    s_avg = mean(son, na.rm = TRUE), \n    f_sd  = sd(father, na.rm = TRUE), \n    s_sd  = sd(son, na.rm = TRUE), \n    r     = cor(father, son)\n  ) |> \n  as.vector() |> list_simplify()\n```\n\n```{r f_mpt}\n# construct additional variables\n# f_ivl: successive 2-inch intervals of father heights\n# f_mpt: mid-point of each interval\nfather_son_ht <- father_son_ht |> \n  mutate(\n    f_ivl = cut(father, seq(58, 76, 2)), \n    f_mpt = (2 * ceiling(father/2)) - 1\n  )\n```\n\nThe box-plot below can be regarded as the (sample) *conditional distribution* of sons' heights grouped by the height (rounded to the nearest odd inch) of each son's father.\n\n```{r g_f_mpt}\ng_f_mpt <- father_son_ht |> \n  filter(! is.na(f_mpt)) |> \n  ggplot(mapping = aes(\n    x = f_mpt |> as_factor(), \n    y = son\n  )) + \n  geom_boxplot()\ng_f_mpt\n```\n\n```{r s_stats_per_f_mpt}\n# son statistics per f_ivl\ns_stats_per_f_mpt <- father_son_ht |>    \n  group_by(f_ivl, f_mpt) |>    \n  summarise(     \n    s_count = n(),      \n    s_min   = min(son, na.rm = TRUE),      \n    s_mid   = median(son, na.rm = TRUE),      \n    s_max   = max(son, na.rm = TRUE),      \n    s_avg   = mean(son, na.rm = TRUE)\n  ) |> \n  ungroup()\ns_stats_per_f_mpt |> print(digits = 1)\n```\n\nThe last column in the table above is the sample average of the son's height given the father's height, which we take as an estimate of the population average of the son's height given the father's height, that is, the *conditional expectation* of son's height given father's height.\n\nThe figure below represents these sample conditional averages per father's height as diamonds, whose area is roughly proportional to the number of sons in each group. The figure includes a reference line showing the father's (midpoint) height plus 1 inch, corresponding to our previous calculation of an average son-minus father difference.\n\n```{r g_s_avg_per_f_mpt}\ng_s_avg_per_f_mpt <- s_stats_per_f_mpt |> ggplot(mapping = aes(\n  x = f_mpt, \n  y = s_avg, \n)) + \n  geom_line() + \n  geom_count(\n    shape = 23, \n    size = s_stats_per_f_mpt$s_count |> sqrt()) + \n  geom_abline(\n    intercept = 1, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\") + \n  labs(\n    title = \"Average of sons' heights per father's height\", \n    subtitle = \"(dotted red line shows father's height + 1 inch)\"\n  )\ng_s_avg_per_f_mpt\n```\n\nThe above graph of average son-height per father's height forms an approximate straight line, although the slope of the line is less than 1 (which is the slope of the reference line).\n\n## Z-Scores: transforming data values to standard units\n\nImagine choosing a father-son pair at random from the entire population and measuring their respective heights. This would be an example of a pair of random variables $(X, Y)$. If we happened to know the average and standard deviation of (father, son) heights, respectively, from the entire population, we could convert the given heights to so-called standard units, or z-scores, as follows.\n\n$$\n\\begin{align}\n  Z_x(X) = \\frac{X - \\mu_x}{\\sigma_x} \\\\\n  Z_y(Y) = \\frac{Y - \\mu_y}{\\sigma_y} \\\\\n\\end{align}\n$$\n\nHere $\\mu$ signifies the average (arithmetic mean) height across the entire population, and $\\sigma$ denotes the population standard deviation. Thus $Z_x(X)$ gives the number of standard deviations above or below the population average (expected value).\n\nOf course we seldom have precise values for these population parameters. In practice we then use sample estimates of the parameters, say $\\hat{\\mu}$ for the sample average and $\\hat{\\sigma}$ for the sample standard deviation.\n\n$$ \n\\begin{align}\n  \\hat{\\mu}_x &= \\frac{1}{n} \\sum_{k = 1}^{n} x_k \\\\ \n  \\hat{\\sigma}_{x}^2 &= \\frac{1}{n-1} \\sum_{k = 1}^{n} (x_k - \\hat{\\mu}_x)^2 \\\\ \n\\end{align} \n$$\n\nSo the term \"z-score\" or \"standard unit\" is usually understood with respect to the sample distribution.\n\n$$\n\\begin{align}\n  \\hat{Z}_x(x_k) = \\frac{x_k - \\hat{\\mu}_x}{\\hat{\\sigma}_x} \\\\ \n  \\hat{Z}_y(y_k) = \\frac{y_k - \\hat{\\mu}_y}{\\hat{\\sigma}_y} \\\\ \n\\end{align} \n$$\n\n### SD line\n\nThe line given by the equation $\\hat{Z}_y(y) = \\hat{Z}_x(x)$ is called the \"SD line\". Here's an equivalent equation of this line.\n\n$$\n\\begin{align}\n  \\text{SD line: } \\\\\n  y & = \\mathcal{l}_{SD}(x) \\\\\n  &= \\hat{\\mu}_y + \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\\n\\end{align}\n$$\n\nOf all lines $y = \\mathcal{l}(x)$ we might draw through the $(x_k,y_k)$ data points, the SD line $y = \\mathcal{l}_{SD}(x)$ minimizes the sum of squared distances from each $(x_k,y_k)$ data point to its orthogonal projection to the line.\n\n### Regression line\n\nConsider the father's height as the predictor variable $(x)$ and the son's height as the response variable $(y)$. We now seek a line that minimizes a different metric, namely the distance between the son's height and its linear prediction based on the father's height. In statistical parlance we are regressing the son's height on the father's height. (Because we have just *one* predictor variable this is called *simple* linear regression.) The minimizing line is called the regression line, and has the following equation.\n\n$$\\begin{align}   \n  \\text{Regression line: } \\\\   \ny & = \\mathcal{l}_{R}(x) \\\\   \n&= \\hat{\\mu}_y + \\hat{r}  \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\ \\end{align} $$\n\nAn equivalent equation is $\\hat{Z}_y(y) = \\hat{r} \\hat{Z}_x(x)$, where $\\hat{r}$ denotes the sample correlation coefficient.\n\n$$\n  \\hat{r} = \\frac{1}{n-1} \\sum_{k = 1}^{n} \\hat{Z}_x(x_k) \\hat{Z}_y(y_k)\n$$\n\nNote that $\\hat{r}$ is restricted to the closed interval $[-1, 1]$.\n\nThe figure below shows the SD line and the Regression line for the father-son data.\n\n```{r g_fs_points}\ng_fs_points <- father_son_ht |> \n  ggplot(mapping = aes(x = father, y = son)) + \n  geom_point()\n```\n\n```{r g_fs_lines}\nSD_slope = fs_moments[[\"s_sd\"]] / fs_moments[[\"f_sd\"]]\nR_slope  = fs_moments[[\"r\"]] * SD_slope\n\ng_fs_lines <- g_fs_points + \n  # SD line\n  geom_abline(\n    slope = SD_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - SD_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\"\n  ) + \n  # Regression line\n  geom_abline(\n    slope = R_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - R_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"solid\", \n    linewidth = 2, \n    color = \"blue\"\n  ) + \n  labs(\n    title = \"Father-son heights\", \n    subtitle = \"SD line (dotted red), Regression line (solid blue)\"\n  )\ng_fs_lines\n```\n\nThe two lines intersect at the \"point of averages\", that is, at $(\\hat{\\mu}_x, \\hat{\\mu}_y)$, which need not coincide with any data point.\n\nThe following figure and table summarize the regression \"residuals\", that is the son's height $(y)$ minus the height $(\\hat{y})$ predicted by the linear model.\n\n```{r lm_fs}\n# call functon stats::lm() to fit a linear model\nlm_fs <- lm(\n  data = father_son_ht, \n  formula = son ~ father\n)\nlm_fs\n```\n\n```{r fs_residuals}\n# join fitted values and regression residuals to original data\nfs_residuals <- father_son_ht |> \n  mutate(\n    fitted_value = lm_fs$fitted.values, \n    residual     = lm_fs$residuals\n  )\n```\n\n```{r g_fs_residuals}\n# histogram of residuals\ng_fs_residuals <- fs_residuals |> \n  ggplot(mapping = aes(x = residual)) + \n  geom_histogram() + \n  labs(\n    title = \"Histogram of regression residuals\", \n    subtitle = \"son's height minus predicted height\"\n  )\ng_fs_residuals\n```\n\n```{r fs_residuals__smy}\n# summarize distribution of residuals\nfs_residuals$residual |> summary() |> print(digits = 1)\n```\n\nThere are many ways to examine how well a model represents the data. Here's a scatter diagram of the value predicted (fitted) by the model versus the son's actual height.\n\n```{r g_resid_son}\n# scatter diagram with (x, y) = (son, fitted)\ng_resid_son <- fs_residuals |> \n  ggplot(mapping = aes(\n    x = son, y = fitted_value\n  )) + \n  geom_point() + \n  geom_abline(\n    intercept = 0, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    colour = \"red\"\n  ) + \n  labs(\n    title = \"Model-fitted value versus son's height\", \n    subtitle = \"(reference line: fitted = son)\"\n  )\ng_resid_son\n```\n\nWe see that the sons who are extremely short or extremely tall are not represented well by the model, which is heavily influenced by mid-range father-son heights containing most of the data. The father's height alone is a helpful but imperfect predictor of the son's height.\n\n### The Normal distribution\n\nThe father-son data set is well approximated by a bivariate normal distribution. The parameter estimates are as follows.\n\n```{r fs_moments__show}\nfs_moments |> print(digits = 1)\n```\n\nSons are on average about an inch taller than fathers. Fathers and sons share similar standard deviations (2.7 versus 2.8). The sample correlation coefficient is about 0.5.\n\nAmong the mathematical properties of normal distributions is the fact that if the pair of random variables $(X, Y)$ has a bivariate normal distribution, then the conditional expectation $E(Y | X)$ is indeed the linear regression function $\\mathcal{l}_R(X)$ whose equation is that of the population regression line, $Z_y(Y) = r \\; Z_x(X)$. The conditional distribution $\\mathcal{D}(Y | X)$ of $Y$ given $X$ is normal with a mean of $\\mathcal{l}_R(X)$ and a standard deviation of $\\sqrt{1 - r^2} \\; \\sigma_y$. Conditioning on $X$ thus shrinks the standard deviation of $Y$ by a factor of $\\sqrt{1 - r^2}$. For the father-son data, with $r$ approximately equal to 0.5, this shrinkage factor is approximately 0.87, a 13% reduction in the standard deviation of $Y$.\n\n### Simulating random variables\n\nThe R package `stats` contains functions that generate pseudo-random numbers following normal and other well-known statistical distributions. Here are some functions for simulating independent instances of a continuous or discrete random variable. In the table below, the \"value\" column distinguishes the function output as either continuous (`dbl`) or discrete (`int`).\n\n```{r rng_tbl}\nrng_tbl <- gen_rng_tbl()\n```\n\n```{r rng_tbl__kable}\nrng_tbl |> knitr::kable(\n  caption = \"Some random number generators in the R stats package\"\n)\n```\n\nThe functions listed above are designed to generate a user-prescibed number $n$ of independent instances of a single random variable $X$. For some purposes we may want to simulate a pair of random variables $(X, Y)$ or more generally a vector of random variables $X_{\\bullet} = (X_1, X_2, \\ldots, X_K)$ such that the components of the vector are statistically dependent. Such vectors are said to follow a *multivariate* distribution. For example the `stats` package contains function `rmultinom`, a multivariate extension of `rbinom`. In general multivariate distributions are addressed by special-purpose R packages created by members of the R community.\n\n## Cautionary Remarks\n\n### Robust statistics\n\nThe sample average (arithmetic mean) is notoriously sensitive to outliers (data points far removed from most of the other data points). For this reason, the median is often used in place of the mean to describe central or typical values. For example, medians are commonly used to typify home prices in a neighborhood, and for other financial data.\n\nSimilarly, the interquartile range (IQR, the third minus the first quartile of the data) may be preferred to the standard deviation to measure how widely data points are spread around a central value (e.g., median).\n\nIn the present context this means that both the SD line and the Regression line are highly sensitive to outlying data points.\n\n### Anscombe Quartet\n\nProfessor [Frank Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) constructed the \"Anscombe Quartet\": 4 data sets, each consisting of 11 observations of $(x, y)$ pairs of numeric values. Here are the statistics per group.\n\n```{r anscombe_tbl}\nanscombe_tbl <- datasets::anscombe |> \n  as_tibble()\n\n# construct 4 groups of x-values\nx_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, x1:x4) |> \n  pivot_longer(\n    cols = x1:x4, \n    names_to = \"grp\", \n    names_prefix = \"x\", \n    values_to = \"x\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# construct 4 groups of y-values\ny_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, y1:y4) |> \n  pivot_longer(\n    cols = y1:y4, \n    names_to = \"grp\", \n    names_prefix = \"y\", \n    values_to = \"y\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# join x and y values\nxy_long <- x_tbl |> left_join(\n  y  = y_tbl, \n  by = c(\"idx\", \"grp\")\n) |> \n  dplyr::select(grp, idx, x, y) |> \n  arrange(grp, idx)\n```\n\n```{r xy_stats}\nxy_stats <- xy_long |> \n  summarise(\n    .by   = grp, \n    x_avg = mean(x, na.rm = TRUE), \n    y_avg = mean(y, na.rm = TRUE), \n    x_sd  = sd(x, na.rm = TRUE), \n    y_sd  = sd(y, na.rm = TRUE), \n    r     = cor(x, y)\n  )\nxy_stats\n```\n\nThe four groups share virtually identical averages, standard deviations, and $(x, y)$ correlation coefficients. Consequently the four data sets generate identical regression lines. Yet, as shown below, the pattern of $(x, y)$ values differs markedly among these data sets.\n\n```{r g_xy}\ng_xy <- xy_long |> \n  ggplot(mapping = aes(\n    x = x, y = y, group = grp\n  )) + \n  geom_point() + \n  facet_grid(cols = vars(grp))\ng_xy\n```\n\nMoral: pay attention to the data! The graphical and tabular summaries we choose to present should be useful and informative. For example, it might be helpful to note that group 2 looks like the partial outline of a parabola. We should note that group 3 consists of 10 points falling on a line, with one outlier. In group 4 we should note that 10 of the 11 data $x$-values are identical. We want to minimize the chance of inadvertently conveying false impressions by merely reporting standard summary statistics.\n\n## Class Exercise: Diamond Data\n\nTeam up with a classmate and load the diamond data provided by R package `ggplot2`. Of the 10 variables (data columns) choose one of them as the response variable $(y)$, and another as a predictor variable $(x)$. Construct a scatter diagram of $(x, y)$ data points. Calculate the equation of the regression line. Is the predictor variable useful, or irrelevant? The R package `stats` includes potentially helpful functions including a linear regression function, `stats::lm()`, and a local polynomial regression function `stats::loess()`. Take 20 minutes to prepare to report out to the class.\n\n```{r diamonds}\n# make your own copy of the diamond data\ndiamonds <- ggplot2::diamonds\n```\n\n## Statistical Independence\n\nThe father-son data is an example of a pair of statistically dependent variables, since the distribution of sons' heights changes when conditioned on the father's height. (The same can be said for fathers' heights conditioned on the height of the son.) Short fathers tend to have short sons; tall fathers tend to have tall sons.\n\nPractical examples of statistically *independent* (unrelated) variables exist but are rare, since studies typically collect data on variables believed to be related. Nevertheless, the concept of statistical independence is very useful, as it gives rise to measures of departure from statistical independence (and thus measures of statistical association).\n\nThe correlation coefficient $r$ is an example of such a measure for two continuous variables. If $(X, Y)$ is a pair of statistically independent variables, then $r = 0$. Note, however, that $(X, Y)$ may be statistically dependent even if uncorrelated, that is, even if $r = 0$.\n\n### Definition\n\nThe pair of random variables $(X, Y)$ is defined to be statistically independent if\n\n$$\n\\begin{align}\n  P(X \\in A, \\; Y \\in B) &= P(X \\in A) \\times P(Y \\in B) \\\\\n  & \\text{for all possible sets } A, B \\\\ \n\\end{align}\n$$\n\nIf $P(A) > 0$ then statistical independence implies:\n\n$$\n\\begin{align}\n  P(Y \\in B \\; | \\; X \\in A) &= \\frac{P(X \\in A, \\; Y \\in B)}{P(X \\in A)} \\\\ \n  &= P(Y \\in B) \\\\ \n  & \\text{whenever } P(A) > 0 \\\\\n\\end{align}\n$$\n\nThat is, the conditional probability of random variable $Y$ belonging to set $B$ given that $X$ belongs to set $A$ is equal to the unconditional probability that $Y$ belongs to set $B$. It follows that the conditional expectation $E(Y | X)$ does not depend on $X$, and thus equals the constant $E(Y)$, the unconditional expected value of $Y$.\n\n### The case when X and Y are categorical variables\n\nAs previously noted, for a pair $(X,Y)$ of continuous variables, the correlation coefficient is a measure (though not a definitive measure) of statistical association or dependence. In the case when $X$ and $Y$ are each restricted to a discrete set of values, the chi-square statistic is a useful measure of statistical association.\n\nTo illustrate, we use data on handedness (right, left, or ambidextrous) of US adults aged 25-34. The data were collected by the US Health and Nutrition Examination Survey (HANES), as cited in [FPP](https://www.goodreads.com/book/show/147358.Statistics). The question we investigate is whether handedness is independent of sex (male, female). For each of the six possible combinations of handedness and sex, the table below counts the number of people in the sample having that combination.\n\n```{r hs_tbl}\nhs_tbl <- gen_handedness()\n```\n\n```{r hs_wide}\nhs_wide <- hs_tbl |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r hs_wide__kable}\nhs_wide |> knitr::kable(\n  caption = \"Handedness counts among sampled males and females\", \n  col.names = c(\"handedness\", \"male\", \"female\")\n)\n```\n\nThe percentages of handedness among males and among females are as follows.\n\n```{r h_smy}\nh_smy <- hs_tbl |> \n  summarise(\n    .by = \"hnd\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 3 × 2\n#   hnd   count\n#   <chr> <int>\n# 1 right  2004\n# 2 left    205\n# 3 ambi     28\n```\n\n```{r s_smy}\ns_smy <- hs_tbl |> \n  summarise(\n    .by = \"sex\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <int>\n# 1 male    1067\n# 2 female  1170\n```\n\n```{r hs_pct_wide}\nm_total <- s_smy[[1, 2]]\nf_total <- s_smy[[2, 2]]\n\nhs_pct_wide <- hs_wide |> \n  mutate(\n    male   = 100 * male   / m_total, \n    female = 100 * female / f_total\n  )\n```\n\n```{r hs_pct_wide__kable}\nhs_pct_wide |> knitr::kable(\n  caption = \"Percentage handedness among males, and among females\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n```\n\nIf handedness and sex were independent, we should see similar percentages of males and females for each type of handedness. The above table indeed shows similar percentages, but are they close enough to conclude independence?\n\nIn the early 1900's Karl Pearson developed the chi-squared test of independence of categorical variables. The reasoning is as follows. Suppose we accept as population estimates the data percentages of handedness (across males and females), and we also accept the somewhat different percentages of males and females in the sample. These so-called *marginal* distributions are not in dispute. What we're investigating concerns the cell percentages, combinations of handedness and sex. Under the assumption of independence we would expect the cell percentages in the data to be close to the product of the handedness percentage and the male or female percentage. This gives us an expected cell percentage (assuming independence), which we convert to an expected cell count. Pearson's test of independence is based on the following chi-squared statistic.\n\n$$\n\\begin{align}\n  \\chi^2 &= \\sum_{j = 1}^J {\\sum_{k = 1}^K {\\frac{(O_{j,k} - E_{j,k})^2}{E_{j,k}}}} \\\\ \n  O_{j,k} &= \\text{observed count for cell } \\{j, k\\} \\\\ \n  E_{j,k} &= \\text{expected count for cell } \\{j, k\\} \\\\ \n\\end{align}\n$$\n\nPearson calculated the distribution of this statistic mathematically based on the notion of \"degrees of freedom\".\n\nThat is, for each index $j$ the expected values summed across $k$ are constrained to match the corresponding sum of the observed values. Similarly, for each index $k$ the expected values summed across $j$ are constrained to match the corresponding sum of the observed values. Given these fixed marginal sums, cell values can vary with $(J-1) \\times (K-1)$ degrees of freedom.\n\nUnder the assumption of independence, the chi-squared statistic follows the distribution of the sum of squared independent standard normal variables, the number of independent normal variables matching the degrees of freedom.\n\n```{r hs_chi_sq}\nhs_chi_sq <- hs_wide |> \n  dplyr::select(male, female) |> \n  as.matrix() |> \n  chisq.test()\n\n# data:  as.matrix(dplyr::select(hs_wide, male, female))\n# X-squared = 11.806, df = 2, p-value = 0.002731\n```\n\nFor the handedness data the degrees of freedom equals 2, and the value of the statistic is `r round(hs_chi_sq$statistic, 1)`, which is beyond the 99% quantile of the corresponding chi-squared distribution (and thus yields a \"p-value\" of less than 1%). This would be regarded as strong evidence against the assumption of independence.\n\nThe chi-squared statistic is the sum of squared terms of the following form.\n\n$$\n\\begin{align}\n  \\frac{O_{j,k} - E_{j,k}}{\\sqrt{E_{j,k}}} \\\\ \n\\end{align}\n$$\n\nThese terms are called \"Pearson residuals\". For the handedness data, the Pearson residuals are as follows.\n\n```{r hs_resid_wide}\n# Pearson residuals\nhs_resid_wide <- hs_wide |> \n  dplyr::select(hnd) |> \n  mutate(\n    male   = hs_chi_sq$residuals[, 1], \n    female = hs_chi_sq$residuals[, 2]\n  )\n```\n\n```{r hs_resid_wide__kable}\nhs_resid_wide |> knitr::kable(\n  caption = \"Pearson residuals for handedness data\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n\n```\n\nRoughly speaking, under independence the magnitude of cell values should align with the scale of standard normal variables. For the handedness data, the large value of the chi-square statistic cannot be attributed to a single cell of the table, but rather to the left-handed and ambidextrous cells (handedness to which males are more prone than females).\n\n### Simpson's Paradox\n\n```{r ucb_admissions}\n# reformat data\nucb_admissions <- datasets::UCBAdmissions |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  rename(sex = gender) |> \n  rename(count = n) |> \n  dplyr::select(dept, sex, admit, count)\n```\n\n```{r ucb_sex_smy}\n# count Male and Female applicants\nucb_sex_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sex_smy\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <dbl>\n# 1 Male    2691\n# 2 Female  1835\n```\n\n```{r ucb_total_applicants}\n# count all applicants\nucb_total_applicants <- \n  ucb_sex_smy[[1, 2]] + \n  ucb_sex_smy[[2, 2]]\n```\n\n```{r ucb_sa_smy}\n# count admissions/rejections per sex\nucb_sa_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sa_smy\n# # A tibble: 4 × 3\n#   sex    admit    count\n#   <chr>  <chr>    <dbl>\n# 1 Male   Admitted  1198\n# 2 Male   Rejected  1493\n# 3 Female Admitted   557\n# 4 Female Rejected  1278\n```\n\n```{r ucb_sa_wide}\n# count Males and Females in separate columns\nucb_sa_wide <- ucb_sa_smy |> \n  pivot_wider(\n    names_from = \"sex\", \n    values_from = \"count\"\n  )\n# ucb_sa_wide\n# # A tibble: 2 × 3\n#   admit     Male Female\n#   <chr>    <dbl>  <dbl>\n# 1 Admitted  1198    557\n# 2 Rejected  1493   1278\n```\n\n```{r ucb_sa_pct}\n# convert counts to percentages\nucb_sa_pct <- ucb_sa_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]]\n  )\n# ucb_sa_pct |> print(digits = 1)\n# # A tibble: 2 × 3\n#   admit     Male Female\n#   <chr>    <dbl>  <dbl>\n# 1 Admitted  44.5   30.4\n# 2 Rejected  55.5   69.6\n```\n\nWe now turn to a different set of categorical data from a study of graduate admissions at UC Berkeley in 1973 available in R as `datasets::UCBAdmissions`. The study was prompted by a concern of bias against females. The table below summarizes admission percentages for males and for females across the six largest departments.\n\n```{r ucb_sa_pct__kable}\nucb_sa_pct |> \n  rename(decision = admit) |> \n  knitr::kable(\n  caption = \"Admission percentages for males and for females\", \n  digits = 1\n)\n```\n\nThese percentages look damning, but the table below, showing admission rates per department, tells a different story.\n\n```{r ucb_dept_smy}\n# applicants per department\nucb_dept_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\"), \n    count = sum(count, na.rm = TRUE)\n  ) |> \n  mutate(\n    # percent of all applicants applying to each dept\n    pct = 100 * count / ucb_total_applicants\n  )\n# ucb_dept_smy |> print(digits = 1)\n# # A tibble: 6 × 3\n#   dept  count   pct\n#   <chr> <dbl> <dbl>\n# 1 A       933  20.6\n# 2 B       585  12.9\n# 3 C       918  20.3\n# 4 D       792  17.5\n# 5 E       584  12.9\n# 6 F       714  15.8\n```\n\n```{r ucb_da_smy}\n# count addmissions/rejections per department\nucb_da_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_da_wide}\n# move (Admitted, Rejected) into separate columns\nucb_da_wide <- ucb_da_smy |> \n  pivot_wider(\n    names_from = admit, \n    values_from = count\n  )\n# ucb_da_wide\n# # A tibble: 6 × 3\n#   dept  Admitted Rejected\n#   <chr>    <dbl>    <dbl>\n# 1 A          601      332\n# 2 B          370      215\n# 3 C          322      596\n# 4 D          269      523\n# 5 E          147      437\n# 6 F           46      668\n```\n\n```{r ucb_da_pct}\n# overall admission rate per department\nucb_da_pct <- ucb_da_wide |> \n  mutate(\n    # percent admitted per department (Male + Female)\n    MF_pct = 100 * Admitted / ucb_dept_smy$count\n  )\n# ucb_da_pct |> print(digits = 1)\n# # A tibble: 6 × 4\n#   dept  Admitted Rejected MF_pct\n#   <chr>    <dbl>    <dbl>  <dbl>\n# 1 A          601      332  64.4 \n# 2 B          370      215  63.2 \n# 3 C          322      596  35.1 \n# 4 D          269      523  34.0 \n# 5 E          147      437  25.2 \n# 6 F           46      668   6.44\n```\n\n```{r ucb_m_da_pct}\n# filter on males\n# count per-department applications and admissions\n\n# count male applicants per department\nucb_m_d_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n\n# count male admissions/rejections per department\nucb_m_da_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n\n# move (Admitted, Rejected) counts into separate columns\nucb_m_da_wide <- ucb_m_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n\n# convert counts to percents\nucb_m_da_pct <- ucb_m_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_m_d_smy$count, \n    Rejected = 100 * Rejected / ucb_m_d_smy$count\n  )\n```\n\n```{r ucb_f_da_pct}\n# filter on females\n# count per-department applications and admissions\n\n# count female applicants per department\nucb_f_d_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n\n# count female admissions/rejections per department\nucb_f_da_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n\n# move (Admitted, Rejected) counts into separate columns\nucb_f_da_wide <- ucb_f_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n\n# convert counts to percents\nucb_f_da_pct <- ucb_f_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_f_d_smy$count, \n    Rejected = 100 * Rejected / ucb_f_d_smy$count\n  )\n```\n\n```{r ucb_das_pct}\n# admission rate of each department:  \n# among males, among females, and overall\nucb_das_pct <- ucb_da_pct |> \n  dplyr::select(dept, MF_pct) |> \n  left_join(\n    y  = ucb_m_da_pct |> \n      dplyr::select(dept, Admitted) |> \n      rename(M_pct = Admitted), \n    by = \"dept\"\n  ) |> \n  left_join(\n    y  = ucb_f_da_pct |> \n      dplyr::select(dept, Admitted) |> \n      rename(F_pct = Admitted), \n    by = \"dept\"\n  ) |> \n  dplyr::select(dept, M_pct, F_pct, MF_pct)\n```\n\n```{r ucb_das_pct__kable}\nucb_das_pct |> knitr::kable(\n  caption = \"Admission percentages per department\", \n  col.names = c(\"dept\", \"among_males\", \"among_females\", \"overall\"), \n  digits = 1\n)\n```\n\nThe table shows that four of the six departments admitted a greater percentage of female applicants than male applicants. In the remaining two departments females did somewhat worse than males. Yet, summing over all six departments, women applicants fared decidedly worse than male applicants. How can this be?\n\nThe answer can be found by: (1) noting that the table above lists departments, labeled A through F, in descending order of overall admission rates; and (2) examining the following table that shows each department's share of applicants: male, female, and overall.\n\n```{r ucb_ds_smy}\n# count number of applicantions per (dept, sex)\nucb_ds_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_ds_wide}\n# list (Male, Female) as separate columns\nucb_ds_wide <- ucb_ds_smy |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  ) |> \n  mutate(\n    total = Male + Female\n  )\n```\n\n```{r ucb_ds_wide__kable}\nucb_ds_wide |> knitr::kable(\n  caption = \"Number of applications per department\", \n  col.names = c(\"dept\", \"from_males\", \"from_females\", \"overall\")\n)\n```\n\nHere are the same counts but now converted into per-department percentage of applications from males, females, and overall, respectively.\n\n```{r ucb_ds_pct}\n# convert counts to percents\nucb_ds_pct <- ucb_ds_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]], \n    total  = 100 * total  / ucb_total_applicants\n  )\n```\n\n```{r ucb_ds_pct__kable}\nucb_ds_pct |> knitr::kable(\n  caption = \"Percent of applications per department\", \n  col.names = c(\"dept\", \"from_males\", \"from_females\", \"overall\"), \n  digits = 1\n)\n```\n\nWe see that relatively few females applied to departments A and B, which had the highest overall admission rates. Females tended more than males to apply to departments having overall low rates of admission. That is, departmental admission rate is an explanatory variable missing from the initial summary of male and female admission rates across all six departments.\n\nThis phenomenon, a pattern per group that is masked when summarized across groups, is known as [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). More generally, we must be on guard for possible missing variables (sometimes called \"confounding\" variables) that might alter our conclusions.\n\n## Team Exercises\n\n1.  Simulate bivariate normal variables: Using `stats::rnorm()` or otherwise, generate independent instances of a standard normal variable $X$ (that is, having mean zero and standard deviation 1). Next choose a value of $r$ such that $-1 < r < 1$. Now for each instance of $X$ construct an instance of random variable $Y$ so that the distribution $\\mathcal{D}(Y \\; | \\; X)$ of $Y$ given $X$ is normal with expected value $r \\times X$ and standard deviation $\\sqrt{1 - r^2}$. (Hint: consider constructing $Y$ by using $X$ along with a new, independent standard normal variable $Z$.) What are the *unconditional* mean and standard deviation of $Y$? What is the correlation coefficient of the pair $(X, Y)$? How might you generalize your construction to accommodate other prescribed means $(\\mu_x, \\mu_y)$ and standard deviations $(\\sigma_x, \\sigma_y)$ of $(X, Y)$?\n\n2.  Simpson's paradox: In the discussion above we illustrated Simpson's paradox using the UCB Admissions data. Find or construct a different example.\n\n## Resources\n\n[R Graphics Cookbook (2e)](https://r-graphics.org/) by Winston Chang\n\n[Statistics (4e)](https://www.goodreads.com/book/show/147358.Statistics) by Freedman, Pisani, Purves \\| Goodreads\n\n[Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) (probability theory) - Wikipedia\n\n[Sex bias in graduate admissions](https://pubmed.ncbi.nlm.nih.gov/17835295/): data from Berkeley, by Bickel, Hammel, and O'connell\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(here)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"handedness_data.R\"))\nsource(here(\"code\", \"rng_tbl.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Heights of Fathers and Sons\n\n```{r father_son_ht}\nfather_son_ht <- UsingR::father.son |> \n  as_tibble() |> \n  rename(father = fheight, son = sheight)\n```\n\n```{r fs_moments}\n# vector of first and second central moments\nfs_moments <- father_son_ht |> \n  summarise(\n    f_avg = mean(father, na.rm = TRUE), \n    s_avg = mean(son, na.rm = TRUE), \n    f_sd  = sd(father, na.rm = TRUE), \n    s_sd  = sd(son, na.rm = TRUE), \n    r     = cor(father, son)\n  ) |> \n  as.vector() |> list_simplify()\n```\n\n```{r f_mpt}\n# construct additional variables\n# f_ivl: successive 2-inch intervals of father heights\n# f_mpt: mid-point of each interval\nfather_son_ht <- father_son_ht |> \n  mutate(\n    f_ivl = cut(father, seq(58, 76, 2)), \n    f_mpt = (2 * ceiling(father/2)) - 1\n  )\n```\n\nThe box-plot below can be regarded as the (sample) *conditional distribution* of sons' heights grouped by the height (rounded to the nearest odd inch) of each son's father.\n\n```{r g_f_mpt}\ng_f_mpt <- father_son_ht |> \n  filter(! is.na(f_mpt)) |> \n  ggplot(mapping = aes(\n    x = f_mpt |> as_factor(), \n    y = son\n  )) + \n  geom_boxplot()\ng_f_mpt\n```\n\n```{r s_stats_per_f_mpt}\n# son statistics per f_ivl\ns_stats_per_f_mpt <- father_son_ht |>    \n  group_by(f_ivl, f_mpt) |>    \n  summarise(     \n    s_count = n(),      \n    s_min   = min(son, na.rm = TRUE),      \n    s_mid   = median(son, na.rm = TRUE),      \n    s_max   = max(son, na.rm = TRUE),      \n    s_avg   = mean(son, na.rm = TRUE)\n  ) |> \n  ungroup()\ns_stats_per_f_mpt |> print(digits = 1)\n```\n\nThe last column in the table above is the sample average of the son's height given the father's height, which we take as an estimate of the population average of the son's height given the father's height, that is, the *conditional expectation* of son's height given father's height.\n\nThe figure below represents these sample conditional averages per father's height as diamonds, whose area is roughly proportional to the number of sons in each group. The figure includes a reference line showing the father's (midpoint) height plus 1 inch, corresponding to our previous calculation of an average son-minus father difference.\n\n```{r g_s_avg_per_f_mpt}\ng_s_avg_per_f_mpt <- s_stats_per_f_mpt |> ggplot(mapping = aes(\n  x = f_mpt, \n  y = s_avg, \n)) + \n  geom_line() + \n  geom_count(\n    shape = 23, \n    size = s_stats_per_f_mpt$s_count |> sqrt()) + \n  geom_abline(\n    intercept = 1, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\") + \n  labs(\n    title = \"Average of sons' heights per father's height\", \n    subtitle = \"(dotted red line shows father's height + 1 inch)\"\n  )\ng_s_avg_per_f_mpt\n```\n\nThe above graph of average son-height per father's height forms an approximate straight line, although the slope of the line is less than 1 (which is the slope of the reference line).\n\n## Z-Scores: transforming data values to standard units\n\nImagine choosing a father-son pair at random from the entire population and measuring their respective heights. This would be an example of a pair of random variables $(X, Y)$. If we happened to know the average and standard deviation of (father, son) heights, respectively, from the entire population, we could convert the given heights to so-called standard units, or z-scores, as follows.\n\n$$\n\\begin{align}\n  Z_x(X) = \\frac{X - \\mu_x}{\\sigma_x} \\\\\n  Z_y(Y) = \\frac{Y - \\mu_y}{\\sigma_y} \\\\\n\\end{align}\n$$\n\nHere $\\mu$ signifies the average (arithmetic mean) height across the entire population, and $\\sigma$ denotes the population standard deviation. Thus $Z_x(X)$ gives the number of standard deviations above or below the population average (expected value).\n\nOf course we seldom have precise values for these population parameters. In practice we then use sample estimates of the parameters, say $\\hat{\\mu}$ for the sample average and $\\hat{\\sigma}$ for the sample standard deviation.\n\n$$ \n\\begin{align}\n  \\hat{\\mu}_x &= \\frac{1}{n} \\sum_{k = 1}^{n} x_k \\\\ \n  \\hat{\\sigma}_{x}^2 &= \\frac{1}{n-1} \\sum_{k = 1}^{n} (x_k - \\hat{\\mu}_x)^2 \\\\ \n\\end{align} \n$$\n\nSo the term \"z-score\" or \"standard unit\" is usually understood with respect to the sample distribution.\n\n$$\n\\begin{align}\n  \\hat{Z}_x(x_k) = \\frac{x_k - \\hat{\\mu}_x}{\\hat{\\sigma}_x} \\\\ \n  \\hat{Z}_y(y_k) = \\frac{y_k - \\hat{\\mu}_y}{\\hat{\\sigma}_y} \\\\ \n\\end{align} \n$$\n\n### SD line\n\nThe line given by the equation $\\hat{Z}_y(y) = \\hat{Z}_x(x)$ is called the \"SD line\". Here's an equivalent equation of this line.\n\n$$\n\\begin{align}\n  \\text{SD line: } \\\\\n  y & = \\mathcal{l}_{SD}(x) \\\\\n  &= \\hat{\\mu}_y + \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\\n\\end{align}\n$$\n\nOf all lines $y = \\mathcal{l}(x)$ we might draw through the $(x_k,y_k)$ data points, the SD line $y = \\mathcal{l}_{SD}(x)$ minimizes the sum of squared distances from each $(x_k,y_k)$ data point to its orthogonal projection to the line.\n\n### Regression line\n\nConsider the father's height as the predictor variable $(x)$ and the son's height as the response variable $(y)$. We now seek a line that minimizes a different metric, namely the distance between the son's height and its linear prediction based on the father's height. In statistical parlance we are regressing the son's height on the father's height. (Because we have just *one* predictor variable this is called *simple* linear regression.) The minimizing line is called the regression line, and has the following equation.\n\n$$\\begin{align}   \n  \\text{Regression line: } \\\\   \ny & = \\mathcal{l}_{R}(x) \\\\   \n&= \\hat{\\mu}_y + \\hat{r}  \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\ \\end{align} $$\n\nAn equivalent equation is $\\hat{Z}_y(y) = \\hat{r} \\hat{Z}_x(x)$, where $\\hat{r}$ denotes the sample correlation coefficient.\n\n$$\n  \\hat{r} = \\frac{1}{n-1} \\sum_{k = 1}^{n} \\hat{Z}_x(x_k) \\hat{Z}_y(y_k)\n$$\n\nNote that $\\hat{r}$ is restricted to the closed interval $[-1, 1]$.\n\nThe figure below shows the SD line and the Regression line for the father-son data.\n\n```{r g_fs_points}\ng_fs_points <- father_son_ht |> \n  ggplot(mapping = aes(x = father, y = son)) + \n  geom_point()\n```\n\n```{r g_fs_lines}\nSD_slope = fs_moments[[\"s_sd\"]] / fs_moments[[\"f_sd\"]]\nR_slope  = fs_moments[[\"r\"]] * SD_slope\n\ng_fs_lines <- g_fs_points + \n  # SD line\n  geom_abline(\n    slope = SD_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - SD_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\"\n  ) + \n  # Regression line\n  geom_abline(\n    slope = R_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - R_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"solid\", \n    linewidth = 2, \n    color = \"blue\"\n  ) + \n  labs(\n    title = \"Father-son heights\", \n    subtitle = \"SD line (dotted red), Regression line (solid blue)\"\n  )\ng_fs_lines\n```\n\nThe two lines intersect at the \"point of averages\", that is, at $(\\hat{\\mu}_x, \\hat{\\mu}_y)$, which need not coincide with any data point.\n\nThe following figure and table summarize the regression \"residuals\", that is the son's height $(y)$ minus the height $(\\hat{y})$ predicted by the linear model.\n\n```{r lm_fs}\n# call functon stats::lm() to fit a linear model\nlm_fs <- lm(\n  data = father_son_ht, \n  formula = son ~ father\n)\nlm_fs\n```\n\n```{r fs_residuals}\n# join fitted values and regression residuals to original data\nfs_residuals <- father_son_ht |> \n  mutate(\n    fitted_value = lm_fs$fitted.values, \n    residual     = lm_fs$residuals\n  )\n```\n\n```{r g_fs_residuals}\n# histogram of residuals\ng_fs_residuals <- fs_residuals |> \n  ggplot(mapping = aes(x = residual)) + \n  geom_histogram() + \n  labs(\n    title = \"Histogram of regression residuals\", \n    subtitle = \"son's height minus predicted height\"\n  )\ng_fs_residuals\n```\n\n```{r fs_residuals__smy}\n# summarize distribution of residuals\nfs_residuals$residual |> summary() |> print(digits = 1)\n```\n\nThere are many ways to examine how well a model represents the data. Here's a scatter diagram of the value predicted (fitted) by the model versus the son's actual height.\n\n```{r g_resid_son}\n# scatter diagram with (x, y) = (son, fitted)\ng_resid_son <- fs_residuals |> \n  ggplot(mapping = aes(\n    x = son, y = fitted_value\n  )) + \n  geom_point() + \n  geom_abline(\n    intercept = 0, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    colour = \"red\"\n  ) + \n  labs(\n    title = \"Model-fitted value versus son's height\", \n    subtitle = \"(reference line: fitted = son)\"\n  )\ng_resid_son\n```\n\nWe see that the sons who are extremely short or extremely tall are not represented well by the model, which is heavily influenced by mid-range father-son heights containing most of the data. The father's height alone is a helpful but imperfect predictor of the son's height.\n\n### The Normal distribution\n\nThe father-son data set is well approximated by a bivariate normal distribution. The parameter estimates are as follows.\n\n```{r fs_moments__show}\nfs_moments |> print(digits = 1)\n```\n\nSons are on average about an inch taller than fathers. Fathers and sons share similar standard deviations (2.7 versus 2.8). The sample correlation coefficient is about 0.5.\n\nAmong the mathematical properties of normal distributions is the fact that if the pair of random variables $(X, Y)$ has a bivariate normal distribution, then the conditional expectation $E(Y | X)$ is indeed the linear regression function $\\mathcal{l}_R(X)$ whose equation is that of the population regression line, $Z_y(Y) = r \\; Z_x(X)$. The conditional distribution $\\mathcal{D}(Y | X)$ of $Y$ given $X$ is normal with a mean of $\\mathcal{l}_R(X)$ and a standard deviation of $\\sqrt{1 - r^2} \\; \\sigma_y$. Conditioning on $X$ thus shrinks the standard deviation of $Y$ by a factor of $\\sqrt{1 - r^2}$. For the father-son data, with $r$ approximately equal to 0.5, this shrinkage factor is approximately 0.87, a 13% reduction in the standard deviation of $Y$.\n\n### Simulating random variables\n\nThe R package `stats` contains functions that generate pseudo-random numbers following normal and other well-known statistical distributions. Here are some functions for simulating independent instances of a continuous or discrete random variable. In the table below, the \"value\" column distinguishes the function output as either continuous (`dbl`) or discrete (`int`).\n\n```{r rng_tbl}\nrng_tbl <- gen_rng_tbl()\n```\n\n```{r rng_tbl__kable}\nrng_tbl |> knitr::kable(\n  caption = \"Some random number generators in the R stats package\"\n)\n```\n\nThe functions listed above are designed to generate a user-prescibed number $n$ of independent instances of a single random variable $X$. For some purposes we may want to simulate a pair of random variables $(X, Y)$ or more generally a vector of random variables $X_{\\bullet} = (X_1, X_2, \\ldots, X_K)$ such that the components of the vector are statistically dependent. Such vectors are said to follow a *multivariate* distribution. For example the `stats` package contains function `rmultinom`, a multivariate extension of `rbinom`. In general multivariate distributions are addressed by special-purpose R packages created by members of the R community.\n\n## Cautionary Remarks\n\n### Robust statistics\n\nThe sample average (arithmetic mean) is notoriously sensitive to outliers (data points far removed from most of the other data points). For this reason, the median is often used in place of the mean to describe central or typical values. For example, medians are commonly used to typify home prices in a neighborhood, and for other financial data.\n\nSimilarly, the interquartile range (IQR, the third minus the first quartile of the data) may be preferred to the standard deviation to measure how widely data points are spread around a central value (e.g., median).\n\nIn the present context this means that both the SD line and the Regression line are highly sensitive to outlying data points.\n\n### Anscombe Quartet\n\nProfessor [Frank Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) constructed the \"Anscombe Quartet\": 4 data sets, each consisting of 11 observations of $(x, y)$ pairs of numeric values. Here are the statistics per group.\n\n```{r anscombe_tbl}\nanscombe_tbl <- datasets::anscombe |> \n  as_tibble()\n\n# construct 4 groups of x-values\nx_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, x1:x4) |> \n  pivot_longer(\n    cols = x1:x4, \n    names_to = \"grp\", \n    names_prefix = \"x\", \n    values_to = \"x\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# construct 4 groups of y-values\ny_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, y1:y4) |> \n  pivot_longer(\n    cols = y1:y4, \n    names_to = \"grp\", \n    names_prefix = \"y\", \n    values_to = \"y\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# join x and y values\nxy_long <- x_tbl |> left_join(\n  y  = y_tbl, \n  by = c(\"idx\", \"grp\")\n) |> \n  dplyr::select(grp, idx, x, y) |> \n  arrange(grp, idx)\n```\n\n```{r xy_stats}\nxy_stats <- xy_long |> \n  summarise(\n    .by   = grp, \n    x_avg = mean(x, na.rm = TRUE), \n    y_avg = mean(y, na.rm = TRUE), \n    x_sd  = sd(x, na.rm = TRUE), \n    y_sd  = sd(y, na.rm = TRUE), \n    r     = cor(x, y)\n  )\nxy_stats\n```\n\nThe four groups share virtually identical averages, standard deviations, and $(x, y)$ correlation coefficients. Consequently the four data sets generate identical regression lines. Yet, as shown below, the pattern of $(x, y)$ values differs markedly among these data sets.\n\n```{r g_xy}\ng_xy <- xy_long |> \n  ggplot(mapping = aes(\n    x = x, y = y, group = grp\n  )) + \n  geom_point() + \n  facet_grid(cols = vars(grp))\ng_xy\n```\n\nMoral: pay attention to the data! The graphical and tabular summaries we choose to present should be useful and informative. For example, it might be helpful to note that group 2 looks like the partial outline of a parabola. We should note that group 3 consists of 10 points falling on a line, with one outlier. In group 4 we should note that 10 of the 11 data $x$-values are identical. We want to minimize the chance of inadvertently conveying false impressions by merely reporting standard summary statistics.\n\n## Class Exercise: Diamond Data\n\nTeam up with a classmate and load the diamond data provided by R package `ggplot2`. Of the 10 variables (data columns) choose one of them as the response variable $(y)$, and another as a predictor variable $(x)$. Construct a scatter diagram of $(x, y)$ data points. Calculate the equation of the regression line. Is the predictor variable useful, or irrelevant? The R package `stats` includes potentially helpful functions including a linear regression function, `stats::lm()`, and a local polynomial regression function `stats::loess()`. Take 20 minutes to prepare to report out to the class.\n\n```{r diamonds}\n# make your own copy of the diamond data\ndiamonds <- ggplot2::diamonds\n```\n\n## Statistical Independence\n\nThe father-son data is an example of a pair of statistically dependent variables, since the distribution of sons' heights changes when conditioned on the father's height. (The same can be said for fathers' heights conditioned on the height of the son.) Short fathers tend to have short sons; tall fathers tend to have tall sons.\n\nPractical examples of statistically *independent* (unrelated) variables exist but are rare, since studies typically collect data on variables believed to be related. Nevertheless, the concept of statistical independence is very useful, as it gives rise to measures of departure from statistical independence (and thus measures of statistical association).\n\nThe correlation coefficient $r$ is an example of such a measure for two continuous variables. If $(X, Y)$ is a pair of statistically independent variables, then $r = 0$. Note, however, that $(X, Y)$ may be statistically dependent even if uncorrelated, that is, even if $r = 0$.\n\n### Definition\n\nThe pair of random variables $(X, Y)$ is defined to be statistically independent if\n\n$$\n\\begin{align}\n  P(X \\in A, \\; Y \\in B) &= P(X \\in A) \\times P(Y \\in B) \\\\\n  & \\text{for all possible sets } A, B \\\\ \n\\end{align}\n$$\n\nIf $P(A) > 0$ then statistical independence implies:\n\n$$\n\\begin{align}\n  P(Y \\in B \\; | \\; X \\in A) &= \\frac{P(X \\in A, \\; Y \\in B)}{P(X \\in A)} \\\\ \n  &= P(Y \\in B) \\\\ \n  & \\text{whenever } P(A) > 0 \\\\\n\\end{align}\n$$\n\nThat is, the conditional probability of random variable $Y$ belonging to set $B$ given that $X$ belongs to set $A$ is equal to the unconditional probability that $Y$ belongs to set $B$. It follows that the conditional expectation $E(Y | X)$ does not depend on $X$, and thus equals the constant $E(Y)$, the unconditional expected value of $Y$.\n\n### The case when X and Y are categorical variables\n\nAs previously noted, for a pair $(X,Y)$ of continuous variables, the correlation coefficient is a measure (though not a definitive measure) of statistical association or dependence. In the case when $X$ and $Y$ are each restricted to a discrete set of values, the chi-square statistic is a useful measure of statistical association.\n\nTo illustrate, we use data on handedness (right, left, or ambidextrous) of US adults aged 25-34. The data were collected by the US Health and Nutrition Examination Survey (HANES), as cited in [FPP](https://www.goodreads.com/book/show/147358.Statistics). The question we investigate is whether handedness is independent of sex (male, female). For each of the six possible combinations of handedness and sex, the table below counts the number of people in the sample having that combination.\n\n```{r hs_tbl}\nhs_tbl <- gen_handedness()\n```\n\n```{r hs_wide}\nhs_wide <- hs_tbl |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r hs_wide__kable}\nhs_wide |> knitr::kable(\n  caption = \"Handedness counts among sampled males and females\", \n  col.names = c(\"handedness\", \"male\", \"female\")\n)\n```\n\nThe percentages of handedness among males and among females are as follows.\n\n```{r h_smy}\nh_smy <- hs_tbl |> \n  summarise(\n    .by = \"hnd\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 3 × 2\n#   hnd   count\n#   <chr> <int>\n# 1 right  2004\n# 2 left    205\n# 3 ambi     28\n```\n\n```{r s_smy}\ns_smy <- hs_tbl |> \n  summarise(\n    .by = \"sex\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <int>\n# 1 male    1067\n# 2 female  1170\n```\n\n```{r hs_pct_wide}\nm_total <- s_smy[[1, 2]]\nf_total <- s_smy[[2, 2]]\n\nhs_pct_wide <- hs_wide |> \n  mutate(\n    male   = 100 * male   / m_total, \n    female = 100 * female / f_total\n  )\n```\n\n```{r hs_pct_wide__kable}\nhs_pct_wide |> knitr::kable(\n  caption = \"Percentage handedness among males, and among females\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n```\n\nIf handedness and sex were independent, we should see similar percentages of males and females for each type of handedness. The above table indeed shows similar percentages, but are they close enough to conclude independence?\n\nIn the early 1900's Karl Pearson developed the chi-squared test of independence of categorical variables. The reasoning is as follows. Suppose we accept as population estimates the data percentages of handedness (across males and females), and we also accept the somewhat different percentages of males and females in the sample. These so-called *marginal* distributions are not in dispute. What we're investigating concerns the cell percentages, combinations of handedness and sex. Under the assumption of independence we would expect the cell percentages in the data to be close to the product of the handedness percentage and the male or female percentage. This gives us an expected cell percentage (assuming independence), which we convert to an expected cell count. Pearson's test of independence is based on the following chi-squared statistic.\n\n$$\n\\begin{align}\n  \\chi^2 &= \\sum_{j = 1}^J {\\sum_{k = 1}^K {\\frac{(O_{j,k} - E_{j,k})^2}{E_{j,k}}}} \\\\ \n  O_{j,k} &= \\text{observed count for cell } \\{j, k\\} \\\\ \n  E_{j,k} &= \\text{expected count for cell } \\{j, k\\} \\\\ \n\\end{align}\n$$\n\nPearson calculated the distribution of this statistic mathematically based on the notion of \"degrees of freedom\".\n\nThat is, for each index $j$ the expected values summed across $k$ are constrained to match the corresponding sum of the observed values. Similarly, for each index $k$ the expected values summed across $j$ are constrained to match the corresponding sum of the observed values. Given these fixed marginal sums, cell values can vary with $(J-1) \\times (K-1)$ degrees of freedom.\n\nUnder the assumption of independence, the chi-squared statistic follows the distribution of the sum of squared independent standard normal variables, the number of independent normal variables matching the degrees of freedom.\n\n```{r hs_chi_sq}\nhs_chi_sq <- hs_wide |> \n  dplyr::select(male, female) |> \n  as.matrix() |> \n  chisq.test()\n\n# data:  as.matrix(dplyr::select(hs_wide, male, female))\n# X-squared = 11.806, df = 2, p-value = 0.002731\n```\n\nFor the handedness data the degrees of freedom equals 2, and the value of the statistic is `r round(hs_chi_sq$statistic, 1)`, which is beyond the 99% quantile of the corresponding chi-squared distribution (and thus yields a \"p-value\" of less than 1%). This would be regarded as strong evidence against the assumption of independence.\n\nThe chi-squared statistic is the sum of squared terms of the following form.\n\n$$\n\\begin{align}\n  \\frac{O_{j,k} - E_{j,k}}{\\sqrt{E_{j,k}}} \\\\ \n\\end{align}\n$$\n\nThese terms are called \"Pearson residuals\". For the handedness data, the Pearson residuals are as follows.\n\n```{r hs_resid_wide}\n# Pearson residuals\nhs_resid_wide <- hs_wide |> \n  dplyr::select(hnd) |> \n  mutate(\n    male   = hs_chi_sq$residuals[, 1], \n    female = hs_chi_sq$residuals[, 2]\n  )\n```\n\n```{r hs_resid_wide__kable}\nhs_resid_wide |> knitr::kable(\n  caption = \"Pearson residuals for handedness data\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n\n```\n\nRoughly speaking, under independence the magnitude of cell values should align with the scale of standard normal variables. For the handedness data, the large value of the chi-square statistic cannot be attributed to a single cell of the table, but rather to the left-handed and ambidextrous cells (handedness to which males are more prone than females).\n\n### Simpson's Paradox\n\n```{r ucb_admissions}\n# reformat data\nucb_admissions <- datasets::UCBAdmissions |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  rename(sex = gender) |> \n  rename(count = n) |> \n  dplyr::select(dept, sex, admit, count)\n```\n\n```{r ucb_sex_smy}\n# count Male and Female applicants\nucb_sex_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sex_smy\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <dbl>\n# 1 Male    2691\n# 2 Female  1835\n```\n\n```{r ucb_total_applicants}\n# count all applicants\nucb_total_applicants <- \n  ucb_sex_smy[[1, 2]] + \n  ucb_sex_smy[[2, 2]]\n```\n\n```{r ucb_sa_smy}\n# count admissions/rejections per sex\nucb_sa_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sa_smy\n# # A tibble: 4 × 3\n#   sex    admit    count\n#   <chr>  <chr>    <dbl>\n# 1 Male   Admitted  1198\n# 2 Male   Rejected  1493\n# 3 Female Admitted   557\n# 4 Female Rejected  1278\n```\n\n```{r ucb_sa_wide}\n# count Males and Females in separate columns\nucb_sa_wide <- ucb_sa_smy |> \n  pivot_wider(\n    names_from = \"sex\", \n    values_from = \"count\"\n  )\n# ucb_sa_wide\n# # A tibble: 2 × 3\n#   admit     Male Female\n#   <chr>    <dbl>  <dbl>\n# 1 Admitted  1198    557\n# 2 Rejected  1493   1278\n```\n\n```{r ucb_sa_pct}\n# convert counts to percentages\nucb_sa_pct <- ucb_sa_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]]\n  )\n# ucb_sa_pct |> print(digits = 1)\n# # A tibble: 2 × 3\n#   admit     Male Female\n#   <chr>    <dbl>  <dbl>\n# 1 Admitted  44.5   30.4\n# 2 Rejected  55.5   69.6\n```\n\nWe now turn to a different set of categorical data from a study of graduate admissions at UC Berkeley in 1973 available in R as `datasets::UCBAdmissions`. The study was prompted by a concern of bias against females. The table below summarizes admission percentages for males and for females across the six largest departments.\n\n```{r ucb_sa_pct__kable}\nucb_sa_pct |> \n  rename(decision = admit) |> \n  knitr::kable(\n  caption = \"Admission percentages for males and for females\", \n  digits = 1\n)\n```\n\nThese percentages look damning, but the table below, showing admission rates per department, tells a different story.\n\n```{r ucb_dept_smy}\n# applicants per department\nucb_dept_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\"), \n    count = sum(count, na.rm = TRUE)\n  ) |> \n  mutate(\n    # percent of all applicants applying to each dept\n    pct = 100 * count / ucb_total_applicants\n  )\n# ucb_dept_smy |> print(digits = 1)\n# # A tibble: 6 × 3\n#   dept  count   pct\n#   <chr> <dbl> <dbl>\n# 1 A       933  20.6\n# 2 B       585  12.9\n# 3 C       918  20.3\n# 4 D       792  17.5\n# 5 E       584  12.9\n# 6 F       714  15.8\n```\n\n```{r ucb_da_smy}\n# count addmissions/rejections per department\nucb_da_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_da_wide}\n# move (Admitted, Rejected) into separate columns\nucb_da_wide <- ucb_da_smy |> \n  pivot_wider(\n    names_from = admit, \n    values_from = count\n  )\n# ucb_da_wide\n# # A tibble: 6 × 3\n#   dept  Admitted Rejected\n#   <chr>    <dbl>    <dbl>\n# 1 A          601      332\n# 2 B          370      215\n# 3 C          322      596\n# 4 D          269      523\n# 5 E          147      437\n# 6 F           46      668\n```\n\n```{r ucb_da_pct}\n# overall admission rate per department\nucb_da_pct <- ucb_da_wide |> \n  mutate(\n    # percent admitted per department (Male + Female)\n    MF_pct = 100 * Admitted / ucb_dept_smy$count\n  )\n# ucb_da_pct |> print(digits = 1)\n# # A tibble: 6 × 4\n#   dept  Admitted Rejected MF_pct\n#   <chr>    <dbl>    <dbl>  <dbl>\n# 1 A          601      332  64.4 \n# 2 B          370      215  63.2 \n# 3 C          322      596  35.1 \n# 4 D          269      523  34.0 \n# 5 E          147      437  25.2 \n# 6 F           46      668   6.44\n```\n\n```{r ucb_m_da_pct}\n# filter on males\n# count per-department applications and admissions\n\n# count male applicants per department\nucb_m_d_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n\n# count male admissions/rejections per department\nucb_m_da_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n\n# move (Admitted, Rejected) counts into separate columns\nucb_m_da_wide <- ucb_m_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n\n# convert counts to percents\nucb_m_da_pct <- ucb_m_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_m_d_smy$count, \n    Rejected = 100 * Rejected / ucb_m_d_smy$count\n  )\n```\n\n```{r ucb_f_da_pct}\n# filter on females\n# count per-department applications and admissions\n\n# count female applicants per department\nucb_f_d_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n\n# count female admissions/rejections per department\nucb_f_da_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n\n# move (Admitted, Rejected) counts into separate columns\nucb_f_da_wide <- ucb_f_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n\n# convert counts to percents\nucb_f_da_pct <- ucb_f_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_f_d_smy$count, \n    Rejected = 100 * Rejected / ucb_f_d_smy$count\n  )\n```\n\n```{r ucb_das_pct}\n# admission rate of each department:  \n# among males, among females, and overall\nucb_das_pct <- ucb_da_pct |> \n  dplyr::select(dept, MF_pct) |> \n  left_join(\n    y  = ucb_m_da_pct |> \n      dplyr::select(dept, Admitted) |> \n      rename(M_pct = Admitted), \n    by = \"dept\"\n  ) |> \n  left_join(\n    y  = ucb_f_da_pct |> \n      dplyr::select(dept, Admitted) |> \n      rename(F_pct = Admitted), \n    by = \"dept\"\n  ) |> \n  dplyr::select(dept, M_pct, F_pct, MF_pct)\n```\n\n```{r ucb_das_pct__kable}\nucb_das_pct |> knitr::kable(\n  caption = \"Admission percentages per department\", \n  col.names = c(\"dept\", \"among_males\", \"among_females\", \"overall\"), \n  digits = 1\n)\n```\n\nThe table shows that four of the six departments admitted a greater percentage of female applicants than male applicants. In the remaining two departments females did somewhat worse than males. Yet, summing over all six departments, women applicants fared decidedly worse than male applicants. How can this be?\n\nThe answer can be found by: (1) noting that the table above lists departments, labeled A through F, in descending order of overall admission rates; and (2) examining the following table that shows each department's share of applicants: male, female, and overall.\n\n```{r ucb_ds_smy}\n# count number of applicantions per (dept, sex)\nucb_ds_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_ds_wide}\n# list (Male, Female) as separate columns\nucb_ds_wide <- ucb_ds_smy |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  ) |> \n  mutate(\n    total = Male + Female\n  )\n```\n\n```{r ucb_ds_wide__kable}\nucb_ds_wide |> knitr::kable(\n  caption = \"Number of applications per department\", \n  col.names = c(\"dept\", \"from_males\", \"from_females\", \"overall\")\n)\n```\n\nHere are the same counts but now converted into per-department percentage of applications from males, females, and overall, respectively.\n\n```{r ucb_ds_pct}\n# convert counts to percents\nucb_ds_pct <- ucb_ds_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]], \n    total  = 100 * total  / ucb_total_applicants\n  )\n```\n\n```{r ucb_ds_pct__kable}\nucb_ds_pct |> knitr::kable(\n  caption = \"Percent of applications per department\", \n  col.names = c(\"dept\", \"from_males\", \"from_females\", \"overall\"), \n  digits = 1\n)\n```\n\nWe see that relatively few females applied to departments A and B, which had the highest overall admission rates. Females tended more than males to apply to departments having overall low rates of admission. That is, departmental admission rate is an explanatory variable missing from the initial summary of male and female admission rates across all six departments.\n\nThis phenomenon, a pattern per group that is masked when summarized across groups, is known as [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox). More generally, we must be on guard for possible missing variables (sometimes called \"confounding\" variables) that might alter our conclusions.\n\n## Team Exercises\n\n1.  Simulate bivariate normal variables: Using `stats::rnorm()` or otherwise, generate independent instances of a standard normal variable $X$ (that is, having mean zero and standard deviation 1). Next choose a value of $r$ such that $-1 < r < 1$. Now for each instance of $X$ construct an instance of random variable $Y$ so that the distribution $\\mathcal{D}(Y \\; | \\; X)$ of $Y$ given $X$ is normal with expected value $r \\times X$ and standard deviation $\\sqrt{1 - r^2}$. (Hint: consider constructing $Y$ by using $X$ along with a new, independent standard normal variable $Z$.) What are the *unconditional* mean and standard deviation of $Y$? What is the correlation coefficient of the pair $(X, Y)$? How might you generalize your construction to accommodate other prescribed means $(\\mu_x, \\mu_y)$ and standard deviations $(\\sigma_x, \\sigma_y)$ of $(X, Y)$?\n\n2.  Simpson's paradox: In the discussion above we illustrated Simpson's paradox using the UCB Admissions data. Find or construct a different example.\n\n## Resources\n\n[R Graphics Cookbook (2e)](https://r-graphics.org/) by Winston Chang\n\n[Statistics (4e)](https://www.goodreads.com/book/show/147358.Statistics) by Freedman, Pisani, Purves \\| Goodreads\n\n[Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) (probability theory) - Wikipedia\n\n[Sex bias in graduate admissions](https://pubmed.ncbi.nlm.nih.gov/17835295/): data from Berkeley, by Bickel, Hammel, and O'connell\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_1b_4350_2024-12-28_1720.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Conditional Distributions","subtitle":"Part 1, session 1b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Review concepts and techniques of exploratory data analysis."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}