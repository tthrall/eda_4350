{"title":"Conditional Distributions","markdown":{"yaml":{"title":"Conditional Distributions","subtitle":"Part 1, session 1b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Review concepts and techniques of exploratory data analysis."},"headingText":"Heights of Fathers and Sons","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(here)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"handedness_data.R\"))\n```\n\n------------------------------------------------------------------------\n\n\n```{r father_son_ht}\nfather_son_ht <- UsingR::father.son |> \n  as_tibble() |> \n  rename(father = fheight, son = sheight)\n```\n\n```{r fs_moments}\n# vector of first and second central moments\nfs_moments <- father_son_ht |> \n  summarise(\n    f_avg = mean(father, na.rm = TRUE), \n    s_avg = mean(son, na.rm = TRUE), \n    f_sd  = sd(father, na.rm = TRUE), \n    s_sd  = sd(son, na.rm = TRUE), \n    r     = cor(father, son)\n  ) |> \n  as.vector() |> list_simplify()\n```\n\n```{r f_mpt}\n# construct additional variables\n# f_ivl: successive 2-inch intervals of father heights\n# f_mpt: mid-point of each interval\nfather_son_ht <- father_son_ht |> \n  mutate(\n    f_ivl = cut(father, seq(58, 76, 2)), \n    f_mpt = (2 * ceiling(father/2)) - 1\n  )\n```\n\nThe box-plot below can be regarded as the (sample) *conditional distribution* of sons' heights grouped by the height (rounded to the nearest odd inch) of each son's father.\n\n```{r g_f_mpt}\ng_f_mpt <- father_son_ht |> \n  filter(! is.na(f_mpt)) |> \n  ggplot(mapping = aes(\n    x = f_mpt |> as_factor(), \n    y = son\n  )) + \n  geom_boxplot()\ng_f_mpt\n```\n\n```{r s_stats_per_f_mpt}\n# son statistics per f_ivl\ns_stats_per_f_mpt <- father_son_ht |>    \n  group_by(f_ivl, f_mpt) |>    \n  summarise(     \n    s_count = n(),      \n    s_min   = min(son, na.rm = TRUE),      \n    s_mid   = median(son, na.rm = TRUE),      \n    s_max   = max(son, na.rm = TRUE),      \n    s_avg   = mean(son, na.rm = TRUE)\n  ) |> \n  ungroup()\ns_stats_per_f_mpt |> print(digits = 1)\n```\n\nThe last column in the table above is the sample average of the son's height given the father's height, which we take as an estimate of the population average of the son's height given the father's height, that is, the *conditional expectation* of son's height given father's height.\n\nThe figure below represents these sample conditional averages per father's height as diamonds, whose area is roughly proportional to the number of sons in each group. The figure includes a reference line showing the father's (midpoint) height plus 1 inch, corresponding to our previous calculation of an average son-minus father difference.\n\n```{r g_s_avg_per_f_mpt}\ng_s_avg_per_f_mpt <- s_stats_per_f_mpt |> ggplot(mapping = aes(\n  x = f_mpt, \n  y = s_avg, \n)) + \n  geom_line() + \n  geom_count(\n    shape = 23, \n    size = s_stats_per_f_mpt$s_count |> sqrt()) + \n  geom_abline(\n    intercept = 1, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\") + \n  labs(\n    title = \"Average of sons' heights per father's height\", \n    subtitle = \"(dotted red line shows father's height + 1 inch)\"\n  )\ng_s_avg_per_f_mpt\n```\n\nThe above graph of average son-height per father's height forms an approximate straight line, although the slope of the line is less than 1 (which is the slope of the reference line).\n\n## Z-Scores: transforming data values to standard units\n\nImagine choosing a father-son pair at random from the entire population and measuring their respective heights. This would be an example of a pair of random variables $(X, Y)$. If we happened to know the average and standard deviation of (father, son) heights, respectively, from the entire population, we could convert the given heights to so-called standard units, or z-scores, as follows.\n\n$$\n\\begin{align}\n  Z_x(X) = \\frac{X - \\mu_x}{\\sigma_x} \\\\\n  Z_y(Y) = \\frac{Y - \\mu_y}{\\sigma_y} \\\\\n\\end{align}\n$$\n\nHere $\\mu$ signifies the average (arithmetic mean) height across the entire population, and $\\sigma$ denotes the population standard deviation. Thus $Z_x(X)$ gives the number of standard deviations above or below the population average (expected value).\n\nOf course we seldom have precise values for these population parameters. In practice we then use sample estimates of the parameters, say $\\hat{\\mu}$ for the sample average and $\\hat{\\sigma}$ for the sample standard deviation.\n\n$$ \n\\begin{align}\n  \\hat{\\mu}_x &= \\frac{1}{n} \\sum_{k = 1}^{n} x_k \\\\ \n  \\hat{\\sigma}_{x}^2 &= \\frac{1}{n-1} \\sum_{k = 1}^{n} (x_k - \\hat{\\mu}_x)^2 \\\\ \n\\end{align} \n$$\n\nSo the term \"z-score\" or \"standard unit\" is usually understood with respect to the sample distribution.\n\n$$\n\\begin{align}\n  \\hat{Z}_x(x_k) = \\frac{x_k - \\hat{\\mu}_x}{\\hat{\\sigma}_x} \\\\ \n  \\hat{Z}_y(y_k) = \\frac{y_k - \\hat{\\mu}_y}{\\hat{\\sigma}_y} \\\\ \n\\end{align} \n$$\n\n### SD line\n\nThe line given by the equation $\\hat{Z}_y(y) = \\hat{Z}_x(x)$ is called the \"SD line\". Here's an equivalent equation of this line.\n\n$$\n\\begin{align}\n  \\text{SD line: } \\\\\n  y & = \\mathcal{l}_{SD}(x) \\\\\n  &= \\hat{\\mu}_y + \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\\n\\end{align}\n$$\n\nOf all lines $y = \\mathcal{l}(x)$ we might draw through the $(x_k,y_k)$ data points, the SD line $y = \\mathcal{l}_{SD}(x)$ minimizes the sum of squared distances from each $(x_k,y_k)$ data point to its orthogonal projection to the line.\n\n### Regression line\n\nConsider the father's height as the predictor variable $(x)$ and the son's height as the response variable $(y)$. We now seek a line that minimizes a different metric, namely the distance between the son's height and its linear prediction based on the father's height. In statistical parlance we are regressing the son's height on the father's height. (Because we have just *one* predictor variable this is called *simple* linear regression.) The minimizing line is called the regression line, and has the following equation.\n\n$$\\begin{align}   \n  \\text{Regression line: } \\\\   \ny & = \\mathcal{l}_{R}(x) \\\\   \n&= \\hat{\\mu}_y + \\hat{r}  \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\ \\end{align} $$\n\nAn equivalent equation is $\\hat{Z}_y(y) = \\hat{r} \\hat{Z}_x(x)$, where $\\hat{r}$ denotes the sample correlation coefficient.\n\n$$\n  \\hat{r} = \\frac{1}{n-1} \\sum_{k = 1}^{n} \\hat{Z}_x(x_k) \\hat{Z}_y(y_k)\n$$\n\nNote that $\\hat{r}$ is restricted to the closed interval $[-1, 1]$.\n\nThe figure below shows the SD line and the Regression line for the father-son data.\n\n```{r g_fs_points}\ng_fs_points <- father_son_ht |> \n  ggplot(mapping = aes(x = father, y = son)) + \n  geom_point()\n```\n\n```{r g_fs_lines}\nSD_slope = fs_moments[[\"s_sd\"]] / fs_moments[[\"f_sd\"]]\nR_slope  = fs_moments[[\"r\"]] * SD_slope\n\ng_fs_lines <- g_fs_points + \n  # SD line\n  geom_abline(\n    slope = SD_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - SD_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\"\n  ) + \n  # Regression line\n  geom_abline(\n    slope = R_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - R_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"solid\", \n    linewidth = 2, \n    color = \"blue\"\n  ) + \n  labs(\n    title = \"Father-son heights\", \n    subtitle = \"SD line (dotted red), Regression line (solid blue)\"\n  )\ng_fs_lines\n```\n\nThe two lines intersect at the \"point of averages\", that is, at $(\\hat{\\mu}_x, \\hat{\\mu}_y)$, which need not coincide with any data point.\n\nThe following figure and table summarize the regression \"residuals\", that is the son's height $(y)$ minus the height $(\\hat{y})$ predicted by the linear model.\n\n```{r lm_fs}\nlm_fs <- lm(\n  data = father_son_ht, \n  formula = son ~ father\n)\nlm_fs\n```\n\n```{r fs_residuals}\n# construct a data table including regression residuals\nfs_residuals <- father_son_ht |> \n  mutate(\n    fitted_value = lm_fs$fitted.values, \n    residual     = lm_fs$residuals\n  )\n```\n\n```{r g_fs_residuals}\ng_fs_residuals <- fs_residuals |> \n  ggplot(mapping = aes(x = residual)) + \n  geom_histogram() + \n  labs(\n    title = \"Histogram of regression residuals\", \n    subtitle = \"son's height minus predicted height\"\n  )\ng_fs_residuals\n```\n\n```{r fs_residuals__smy}\nfs_residuals$residual |> summary() |> print(digits = 1)\n```\n\nThere are many ways to examine how well a model represents the data. Here's a scatter diagram of the value predicted (fitted) by the model versus the son's actual height.\n\n```{r g_resid_son}\ng_resid_son <- fs_residuals |> \n  ggplot(mapping = aes(\n    x = son, y = fitted_value\n  )) + \n  geom_point() + \n  geom_abline(\n    intercept = 0, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    colour = \"red\"\n  ) + \n  labs(\n    title = \"Model-fitted value versus son's height\", \n    subtitle = \"(reference line: fitted = son)\"\n  )\ng_resid_son\n```\n\nWe see that the sons who are extremely short or extremely tall are not represented well by the model, which is heavily influenced by mid-range father-son heights containing most of the data. The father's height alone is a helpful but imperfect predictor of the son's height.\n\n### The Normal distribution\n\nThe father-son data set is well approximated by a bivariate normal distribution. The parameter estimates are as follows.\n\n```{r fs_moments__show}\nfs_moments |> print(digits = 1)\n```\n\nSons are on average about an inch taller than fathers. Fathers and sons share similar standard deviations (2.7 versus 2.8). The sample correlation coefficient is about 0.5.\n\nAmong the mathematical properties of normal distributions is the fact that if the pair of random variables $(X, Y)$ has a bivariate normal distribution, then the conditional expectation $E(Y | X)$ is indeed the linear regression function $\\mathcal{l}_R(X)$ whose equation is that of the population regression line, $Z_y(Y) = r Z_x(X)$. The conditional distribution $\\mathcal{D}(Y | X)$ of $Y$ given $X$ is normal with a mean of $\\mathcal{l}_R(X)$ and a standard deviation of $\\sqrt{1 - r^2} \\; \\sigma_y$. Conditioning on $X$ thus shrinks the standard deviation of $Y$ by a factor of $\\sqrt{1 - r^2}$. For the father-son data, with $r$ approximately equal to 0.5, this shrinkage factor is approximately 0.87, a 13% reduction in the standard deviation of $Y$.\n\n## Cautionary Remarks\n\n### Robust statistics\n\nThe sample average (arithmetic mean) is notoriously sensitive to outliers (data points far removed from most of the other data points). For this reason, the median is often used in place of the mean to describe central or typical values. For example, medians are commonly used to typify home prices in a neighborhood, and for other financial data.\n\nSimilarly, the interquartile range (IQR, the third minus the first quartile of the data) may be preferred to the standard deviation to measure how widely data points are spread around a central value (e.g., median).\n\nIn the present context this means that both the SD line and the Regression line are highly sensitive to outlying data points.\n\n### Anscombe Quartet\n\nProfessor [Frank Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) constructed the \"Anscombe Quartet\": 4 data sets, each consisting of 11 observations of $(x, y)$ pairs of numeric values. Here are the statistics per group.\n\n```{r anscombe_tbl}\nanscombe_tbl <- datasets::anscombe |> \n  as_tibble()\n\n# construct 4 groups of x-values\nx_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, x1:x4) |> \n  pivot_longer(\n    cols = x1:x4, \n    names_to = \"grp\", \n    names_prefix = \"x\", \n    values_to = \"x\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# construct 4 groups of y-values\ny_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, y1:y4) |> \n  pivot_longer(\n    cols = y1:y4, \n    names_to = \"grp\", \n    names_prefix = \"y\", \n    values_to = \"y\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# join x and y values\nxy_long <- x_tbl |> left_join(\n  y  = y_tbl, \n  by = c(\"idx\", \"grp\")\n) |> \n  dplyr::select(grp, idx, x, y) |> \n  arrange(grp, idx)\n```\n\n```{r xy_stats}\nxy_stats <- xy_long |> \n  summarise(\n    .by   = grp, \n    x_avg = mean(x, na.rm = TRUE), \n    y_avg = mean(y, na.rm = TRUE), \n    x_sd  = sd(x, na.rm = TRUE), \n    y_sd  = sd(y, na.rm = TRUE), \n    r     = cor(x, y)\n  )\nxy_stats\n```\n\nThe four groups share identical averages, standard deviations, and $(x, y)$ correlation coefficients. Consequently the four data sets generate identical regression lines. Yet, as shown below, the pattern of $(x, y)$ values differs markedly among these data sets.\n\n```{r g_xy}\ng_xy <- xy_long |> \n  ggplot(mapping = aes(\n    x = x, y = y, group = grp\n  )) + \n  geom_point() + \n  facet_grid(cols = vars(grp))\ng_xy\n```\n\nMoral: pay attention to the data! The graphical and tabular summaries we choose to present should be useful and informative. For example, it might be helpful to note that group 2 looks like the partial outline of a parabola. We should note that group 3 consists of 10 points falling on a line, with one outlier. In group 4 we should note that 10 of the 11 data values are identical. We want to minimize the chance of inadvertently conveying false impressions by merely reporting standard summary statistics.\n\n## Class Exercise: Diamond Data\n\nTeam up with a classmate and load the diamond data provided by R package `ggplot2`. Of the 10 variables (data columns) choose one of them as the response variable $(y)$, and another as a predictor variable $(x)$. Construct a scatter diagram of $(x, y)$ data points. Calculate the equation of the regression line. Is the predictor variable useful, or irrelevant? The R package `stats` includes potentially helpful functions including a linear regression function, `stats::lm()`, and a local polynomial regression function `stats::loess()`. Take 20 minutes to prepare to report out to the class.\n\n```{r diamonds}\n# make your own copy of the diamond data\ndiamonds <- ggplot2::diamonds\n```\n\n## Statistical Independence\n\nThe father-son data is an example of a pair of statistically dependent variables, since the distribution of sons' heights changes when conditioned on the father's height. (The same can be said for fathers' heights conditioned on the height of the son.) Short fathers tend to have short sons; tall fathers tend to have tall sons.\n\nPractical examples of independent variables exist but are rare, since studies typically collect data on variables believed to be related. Nevertheless, the concept of statistical independence is very useful, as it gives rise to measures of departure from statistical independence (and thus measures of statistical association).\n\nThe correlation coefficient $r$ is an example of such a measure for two continuous variables. If $(X, Y)$ is a pair of statistically independent variables, then $r = 0$. Note, however, that $(X, Y)$ may be statistically dependent even if they are uncorrelated, that is, even if $r = 0$.\n\n### Definition\n\nThe pair of random variables $(X, Y)$ is defined to be statistically independent if\n\n$$\n\\begin{align}\n  P(X \\in A, Y \\in B) &= P(X \\in A) \\times P(Y \\in B) \\\\\n  & \\text{for all possible sets } A, B \\\\ \n\\end{align}\n$$\n\nIn this case we have\n\n$$\n\\begin{align}\n  P(Y \\in B | X \\in A) &= \\frac{P(X \\in A, Y \\in B)}{P(X \\in A)} \\\\ \n  &= P(Y \\in B) \\\\ \n  & \\text{for all possible sets } A, B \\\\\n\\end{align}\n$$\n\nThat is, the conditional probability of random variable $Y$ belonging to set $B$ given that $X$ belongs to set $A$ is equal to the unconditional probability that $Y$ belongs to set $B$. It follows that the conditional expectation $E(Y | X)$ does not depend on $X$, and thus equals the constant $E(Y)$, the unconditional expected value of $Y$.\n\n### The case when X and Y are categorical variables\n\nAs previously noted, for a pair $(X,Y)$ of continuous variables, the correlation coefficient is a measure (though not a definitive measure) of statistical association or dependence. In the case when $X$ and $Y$ are each restricted to a discrete set of values, the chi-square statistic is a useful measure of statistical association.\n\nWe use data on handedness (right, left, or ambidextrous) of US adults aged 25-34. The data were collected by the US Health and Nutrition Examination Survey (HANES), as cited in [FPP](https://www.goodreads.com/book/show/147358.Statistics). The question we investigate is whether handedness is independent of sex (male, female). Here are the counts for the six combinations of handedness and sex.\n\n```{r hs_tbl}\nhs_tbl <- gen_handedness()\n```\n\n```{r hs_wide}\nhs_wide <- hs_tbl |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r hs_wide__kable}\nhs_wide |> knitr::kable(\n  caption = \"Handedness of sampled males and females\", \n  col.names = c(\"handedness\", \"male\", \"female\")\n)\n```\n\nThe percentages of handedness among males and among females are as follows.\n\n```{r h_smy}\nh_smy <- hs_tbl |> \n  summarise(\n    .by = \"hnd\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 3 × 2\n#   hnd   count\n#   <chr> <int>\n# 1 right  2004\n# 2 left    205\n# 3 ambi     28\n```\n\n```{r s_smy}\ns_smy <- hs_tbl |> \n  summarise(\n    .by = \"sex\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <int>\n# 1 male    1067\n# 2 female  1170\n```\n\n```{r hs_pct_wide}\nm_total <- s_smy[[1, 2]]\nf_total <- s_smy[[2, 2]]\n\nhs_pct_wide <- hs_wide |> \n  mutate(\n    male   = 100 * male   / m_total, \n    female = 100 * female / f_total\n  )\n```\n\n```{r hs_pct_wide__kable}\nhs_pct_wide |> knitr::kable(\n  caption = \"Percentage handedness among males, and among females\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n```\n\nIf handedness and sex were independent, we should see similar percentages of males and females for each type of handedness. The above table indeed shows similar percentages, but are they close enough to conclude independence?\n\nIn the early 1900's Karl Pearson developed the chi-squared test of independence of categorical variables. The reasoning is as follows. Suppose we accept the data values for the overall distribution of handedness (across males and females), and we also accept the distribution between males and females (across handedness). These distributions are not in dispute. What we're investigating concerns the cell percentages, combinations of handedness and sex. Under the assumption of independence we would expect the cell percentages in the data to be close to the product of the handedness percentage and the male/female percentage. This gives us an expected percentage (assuming independence), which we convert to an expected count. Pearson's test of independence is based on the following chi-squared statistic.\n\n$$\n\\begin{align}\n  \\chi^2 &= \\sum_{j = 1}^J {\\sum_{k = 1}^K {\\frac{(O_{j,k} - E_{j,k})^2}{E_{j,k}}}} \\\\ \n  O_{j,k} &= \\text{observed count for cell } \\{j, k\\} \\\\ \n  E_{j,k} &= \\text{expected count for cell } \\{j, k\\} \\\\ \n\\end{align}\n$$\n\nPearson calculated the distribution of this statistic mathematically based on the notion of \"degrees of freedom\".\n\nThat is, for each index $j$ the expected values summed across $k$ are constrained to match the corresponding sum of the observed values. Similarly, for each index $k$ the expected values summed across $j$ are constrained to match the corresponding sum of the observed values. With these fixed marginal sums cell values can vary with $(J-1) \\times (K-1)$ degrees of freedom.\n\nUnder the assumption of independence, the chi-squared statistic follows the distribution of the sum of squared independent standard normal variables, the number of independent normal variables matching the degrees of freedom.\n\n```{r hs_chi_sq}\nhs_chi_sq <- hs_wide |> \n  dplyr::select(male, female) |> \n  as.matrix() |> \n  chisq.test()\n\n# data:  as.matrix(dplyr::select(hs_wide, male, female))\n# X-squared = 11.806, df = 2, p-value = 0.002731\n```\n\nFor the handedness data the degrees of freedom equals 2, and the value of the statistic is `r round(hs_chi_sq$statistic, 1)`, which is beyond the 99% quantile of the distribution (and thus yields a \"p-value\" of less than 1%). This would be regarded as strong evidence against the assumption of independence.\n\nThe chi-square statistic is the sum of squared terms of the following form.\n\n$$\n\\begin{align}\n  \\frac{O_{j,k} - E_{j,k}}{\\sqrt{E_{j,k}}} \\\\ \n\\end{align}\n$$\n\nThese terms are called \"Pearson residuals\". For the handedness data, the Pearson residuals are as follows.\n\n```{r hs_resid_wide}\n# Pearson residuals\nhs_resid_wide <- hs_wide |> \n  dplyr::select(hnd) |> \n  mutate(\n    male   = hs_chi_sq$residuals[, 1], \n    female = hs_chi_sq$residuals[, 2]\n  )\n```\n\n```{r hs_resid_wide__kable}\nhs_resid_wide |> knitr::kable(\n  caption = \"Pearson residuals for handedness data\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n\n```\n\nRoughly speaking, under independence the magnitude of cell values should align with the scale of standard normal variables. In this case the large value of the chi-square statistic cannot be attributed to a single cell of the table, but rather to the left-handed and ambidextrous cells, handedness to which males are more prone than females.\n\n### Simpson's Paradox\n\nWe'll use data from a study of graduate admissions at UC Berkeley in 1973 available in R as `datasets::UCBAdmissions`. The study was prompted by a concern of bias against females. The table below summarizes admission percentages for males and for females across the six largest departments.\n\n```{r ucb_admissions}\n# reformat data\nucb_admissions <- datasets::UCBAdmissions |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  rename(sex = gender) |> \n  rename(count = n) |> \n  dplyr::select(dept, sex, admit, count)\n```\n\n```{r ucb_sex_smy}\nucb_sex_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sex_smy\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <dbl>\n# 1 Male    2691\n# 2 Female  1835\n```\n\n```{r ucb_sa_smy}\nucb_sa_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sa_smy\n# # A tibble: 4 × 3\n#   sex    admit    count\n#   <chr>  <chr>    <dbl>\n# 1 Male   Admitted  1198\n# 2 Male   Rejected  1493\n# 3 Female Admitted   557\n# 4 Female Rejected  1278\n```\n\n```{r ucb_sa_wide}\nucb_sa_wide <- ucb_sa_smy |> \n  pivot_wider(\n    names_from = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r ucb_sa_pct}\nucb_sa_pct <- ucb_sa_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]]\n  )\n```\n\n```{r ucb_sa_pct__kable}\nucb_sa_pct |> knitr::kable(\n  caption = \"Admission percentages for males and for females\", \n  digits = 1\n)\n```\n\nThese percentages look damning, but the table below showing admission rates per department tells a different story. (The departments are denoted A through F.)\n\n```{r ucb_dept_smy}\nucb_dept_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_dept_smy\n# # A tibble: 6 × 2\n#   dept  count\n#   <chr> <dbl>\n# 1 A       933\n# 2 B       585\n# 3 C       918\n# 4 D       792\n# 5 E       584\n# 6 F       714\n```\n\n```{r ucb_da_smy}\nucb_da_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_da_wide}\nucb_da_wide <- ucb_da_smy |> \n  pivot_wider(\n    names_from = admit, \n    values_from = count\n  )\n# ucb_da_wide\n# # A tibble: 6 × 3\n#   dept  Admitted Rejected\n#   <chr>    <dbl>    <dbl>\n# 1 A          601      332\n# 2 B          370      215\n# 3 C          322      596\n# 4 D          269      523\n# 5 E          147      437\n# 6 F           46      668\n```\n\n```{r ucb_m_d_smy}\nucb_m_d_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_f_d_smy}\nucb_f_d_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_m_da_smy}\nucb_m_da_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_m_da_wide}\nucb_m_da_wide <- ucb_m_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n```\n\n```{r ucb_m_da_pct}\nucb_m_da_pct <- ucb_m_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_m_d_smy$count, \n    Rejected = 100 * Rejected / ucb_m_d_smy$count\n  )\n```\n\n```{r ucb_da_pct}\nucb_da_pct <- ucb_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_dept_smy$count, \n    Rejected = 100 * Rejected / ucb_dept_smy$count\n  )\n```\n\n```{r ucb_das_pct}\nucb_das_pct <- ucb_da_pct |> \n  dplyr::select(dept, Admitted) |> \n  left_join(\n    y  = XXX, \n    by = \"dept\"\n  ) |> \n  left_join(\n    y  = YYY, \n    by = \"dept\"\n  )\n```\n\n## Class Exercise: General Social Survey Data\n\nThe [General Social Survey](https://gss.norc.org/) is a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey consists of thousands of questions designed to monitor changes in social characteristics and attitudes in the US. In R package `forcats`, Hadley Wickham formed the data set `gss_cat` consisting of just a few columns to illustrate the challenges one encounters when working with categorical (non-numeric) variables (\"factors\").\n\nTeam up with a classmate and load the `gss_cat` data-set provided by R package `forcats`. How many rows of data are there? How many columns? What questions occur to you about the data? How might you address those questions? Take 15 minutes to prepare to report out to the class.\n\n```{r gss_cat}\n# make your own copy of the survey data \ngss_cat <- forcats::gss_cat\n```\n\n## Discussion: what should EDA mean?\n\nExploratory Data Analysis (EDA) is an approach to data analysis advocated by [John Tukey](https://en.wikipedia.org/wiki/John_Tukey), a leading American statistician of the 20th century. The approach contrasts with what Tukey called \"confirmatory analysis\", that is, a focus on probability models of data-generation along with the estimation or testing of model parameters. The difference is one of emphasis: EDA includes models suggested by data, but with an emphasis on understanding current and potential data sets.\n\nThe exploration is led by one's questions about the data. Relevant questions may or may not be obvious (or given). Variables may or may not be readily categorized as \"response variables\" versus \"predictor variables\". The ability to develop and recognize relevant questions is an important skill largely gained through experience.\n\nImportant EDA outcomes include\n\n-   the discovery of unanticipated data patterns, and\n\n-   proposals to examine tentative answers suggested by the current data, perhaps using a new data set designed for this purpose.\n\nEDA methods are used within the context of confirmatory analysis to examine the data for errors not encompassed by the models under study (e.g., errors in data transcription or transmission), or to search for other departures from model assumptions.\n\nEDA methods can be broadly understood as the methods of descriptive statistics: data summaries (graphical or tabular) intended to enhance our understanding of the data. EDA differs from descriptive statistics in a reliance on the questions of the data analyst and a readiness to examine various transformations of the data.\n\nAs an example, here are some ways we might address the question of how, if at all, the heights of fathers and sons differ in the data presented above.\n\n```{r height_per_fs}\n# render \"father\" and \"son\" as levels of factor \"fs\"\nheight_per_fs <- father_son_ht |> \n  # continue to identify father-son pairs\n  mutate(idx = 1:nrow(father_son_ht)) |>\n  dplyr::select(idx, father, son) |> \n  pivot_longer(\n    cols = c(father, son), \n    names_to = \"fs\", \n    values_to = \"height\"\n  )\n```\n\n```{r g_ht_per_fs}\ng_ht_per_fs <- height_per_fs |> \n  ggplot(mapping = aes(x = fs |> as_factor(), y = height)) + \n  geom_boxplot()\ng_ht_per_fs\n```\n\n```{r ht_per_fs__summarise}\nfs_smy <- height_per_fs |> dplyr::select(fs, height) |>\n  group_by(fs) |> \n  summarise(\n    min = min(height), \n    mid = median(height), \n    avg = mean(height), \n    max = max(height)\n  )\nfs_smy |> print(digits = 1)\n```\n\nThe box-plot and table above show that, on average, sons are about an inch taller than fathers. Here's a histogram of the difference in heights (son minus father) across father-son pairs.\n\n```{r s_minus_f}\nfather_son_ht <- father_son_ht |> \n  mutate(s_minus_f = son - father)\n```\n\n```{r s_minus_f__hist}\ng_s_minus_f <- father_son_ht |> \n  ggplot(mapping = aes(x = s_minus_f)) + \n  geom_histogram()\ng_s_minus_f\n```\n\n```{r s_minus_f__smy}\nfather_son_ht$s_minus_f |> \n  summary() |> \n  print(digits = 1)\n```\n\nThe summary of individual differences in height (son minus father) strengthens the previous aggregate summaries: the distribution of son's height minus father's height is fairly symmetric around a difference of about one inch.\n\n## Team Exercises\n\n1.  Response versus predictor variables: for each of the data sets presented above, propose one or more variables as response variables. How, if at all, might someone else argue for a different choice? What should we mean by \"response\" and \"predictor\" variables? Describe a situation in which this distinction would not be suitable.\n\n2.  Regression to the mean: the discussion of father-son heights asserts that: \"extremely tall or short fathers \\[correspond\\] to not quite so extremely tall or short sons, respectively\". How would you formulate the meaning of that phrase? Do the data demonstrate this phenomenon?\n\n3.  Diamond data: propose a question for the diamond data, and then try to address that question. What (if anything) did you learn from this task?\n\n4.  Survey data: propose a question for the gss_cat data, and then try to address that question. What (if anything) did you learn from this task?\n\n## Resources\n\n[R Graphics Cookbook (2e)](https://r-graphics.org/) by Winston Chang\n\n[Statistics (4e)](https://www.goodreads.com/book/show/147358.Statistics) by Freedman, Pisani, Purves \\| Goodreads\n\n[Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) (probability theory) - Wikipedia\n\n[Sex bias in graduate admissions](https://pubmed.ncbi.nlm.nih.gov/17835295/): data from Berkeley, by Bickel, Hammel, and O'connell\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(here)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\nsource(here(\"code\", \"handedness_data.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Heights of Fathers and Sons\n\n```{r father_son_ht}\nfather_son_ht <- UsingR::father.son |> \n  as_tibble() |> \n  rename(father = fheight, son = sheight)\n```\n\n```{r fs_moments}\n# vector of first and second central moments\nfs_moments <- father_son_ht |> \n  summarise(\n    f_avg = mean(father, na.rm = TRUE), \n    s_avg = mean(son, na.rm = TRUE), \n    f_sd  = sd(father, na.rm = TRUE), \n    s_sd  = sd(son, na.rm = TRUE), \n    r     = cor(father, son)\n  ) |> \n  as.vector() |> list_simplify()\n```\n\n```{r f_mpt}\n# construct additional variables\n# f_ivl: successive 2-inch intervals of father heights\n# f_mpt: mid-point of each interval\nfather_son_ht <- father_son_ht |> \n  mutate(\n    f_ivl = cut(father, seq(58, 76, 2)), \n    f_mpt = (2 * ceiling(father/2)) - 1\n  )\n```\n\nThe box-plot below can be regarded as the (sample) *conditional distribution* of sons' heights grouped by the height (rounded to the nearest odd inch) of each son's father.\n\n```{r g_f_mpt}\ng_f_mpt <- father_son_ht |> \n  filter(! is.na(f_mpt)) |> \n  ggplot(mapping = aes(\n    x = f_mpt |> as_factor(), \n    y = son\n  )) + \n  geom_boxplot()\ng_f_mpt\n```\n\n```{r s_stats_per_f_mpt}\n# son statistics per f_ivl\ns_stats_per_f_mpt <- father_son_ht |>    \n  group_by(f_ivl, f_mpt) |>    \n  summarise(     \n    s_count = n(),      \n    s_min   = min(son, na.rm = TRUE),      \n    s_mid   = median(son, na.rm = TRUE),      \n    s_max   = max(son, na.rm = TRUE),      \n    s_avg   = mean(son, na.rm = TRUE)\n  ) |> \n  ungroup()\ns_stats_per_f_mpt |> print(digits = 1)\n```\n\nThe last column in the table above is the sample average of the son's height given the father's height, which we take as an estimate of the population average of the son's height given the father's height, that is, the *conditional expectation* of son's height given father's height.\n\nThe figure below represents these sample conditional averages per father's height as diamonds, whose area is roughly proportional to the number of sons in each group. The figure includes a reference line showing the father's (midpoint) height plus 1 inch, corresponding to our previous calculation of an average son-minus father difference.\n\n```{r g_s_avg_per_f_mpt}\ng_s_avg_per_f_mpt <- s_stats_per_f_mpt |> ggplot(mapping = aes(\n  x = f_mpt, \n  y = s_avg, \n)) + \n  geom_line() + \n  geom_count(\n    shape = 23, \n    size = s_stats_per_f_mpt$s_count |> sqrt()) + \n  geom_abline(\n    intercept = 1, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\") + \n  labs(\n    title = \"Average of sons' heights per father's height\", \n    subtitle = \"(dotted red line shows father's height + 1 inch)\"\n  )\ng_s_avg_per_f_mpt\n```\n\nThe above graph of average son-height per father's height forms an approximate straight line, although the slope of the line is less than 1 (which is the slope of the reference line).\n\n## Z-Scores: transforming data values to standard units\n\nImagine choosing a father-son pair at random from the entire population and measuring their respective heights. This would be an example of a pair of random variables $(X, Y)$. If we happened to know the average and standard deviation of (father, son) heights, respectively, from the entire population, we could convert the given heights to so-called standard units, or z-scores, as follows.\n\n$$\n\\begin{align}\n  Z_x(X) = \\frac{X - \\mu_x}{\\sigma_x} \\\\\n  Z_y(Y) = \\frac{Y - \\mu_y}{\\sigma_y} \\\\\n\\end{align}\n$$\n\nHere $\\mu$ signifies the average (arithmetic mean) height across the entire population, and $\\sigma$ denotes the population standard deviation. Thus $Z_x(X)$ gives the number of standard deviations above or below the population average (expected value).\n\nOf course we seldom have precise values for these population parameters. In practice we then use sample estimates of the parameters, say $\\hat{\\mu}$ for the sample average and $\\hat{\\sigma}$ for the sample standard deviation.\n\n$$ \n\\begin{align}\n  \\hat{\\mu}_x &= \\frac{1}{n} \\sum_{k = 1}^{n} x_k \\\\ \n  \\hat{\\sigma}_{x}^2 &= \\frac{1}{n-1} \\sum_{k = 1}^{n} (x_k - \\hat{\\mu}_x)^2 \\\\ \n\\end{align} \n$$\n\nSo the term \"z-score\" or \"standard unit\" is usually understood with respect to the sample distribution.\n\n$$\n\\begin{align}\n  \\hat{Z}_x(x_k) = \\frac{x_k - \\hat{\\mu}_x}{\\hat{\\sigma}_x} \\\\ \n  \\hat{Z}_y(y_k) = \\frac{y_k - \\hat{\\mu}_y}{\\hat{\\sigma}_y} \\\\ \n\\end{align} \n$$\n\n### SD line\n\nThe line given by the equation $\\hat{Z}_y(y) = \\hat{Z}_x(x)$ is called the \"SD line\". Here's an equivalent equation of this line.\n\n$$\n\\begin{align}\n  \\text{SD line: } \\\\\n  y & = \\mathcal{l}_{SD}(x) \\\\\n  &= \\hat{\\mu}_y + \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\\n\\end{align}\n$$\n\nOf all lines $y = \\mathcal{l}(x)$ we might draw through the $(x_k,y_k)$ data points, the SD line $y = \\mathcal{l}_{SD}(x)$ minimizes the sum of squared distances from each $(x_k,y_k)$ data point to its orthogonal projection to the line.\n\n### Regression line\n\nConsider the father's height as the predictor variable $(x)$ and the son's height as the response variable $(y)$. We now seek a line that minimizes a different metric, namely the distance between the son's height and its linear prediction based on the father's height. In statistical parlance we are regressing the son's height on the father's height. (Because we have just *one* predictor variable this is called *simple* linear regression.) The minimizing line is called the regression line, and has the following equation.\n\n$$\\begin{align}   \n  \\text{Regression line: } \\\\   \ny & = \\mathcal{l}_{R}(x) \\\\   \n&= \\hat{\\mu}_y + \\hat{r}  \\frac{\\hat{\\sigma}_y}{\\hat{\\sigma}_x} (x - \\hat{\\mu}_x) \\\\ \\end{align} $$\n\nAn equivalent equation is $\\hat{Z}_y(y) = \\hat{r} \\hat{Z}_x(x)$, where $\\hat{r}$ denotes the sample correlation coefficient.\n\n$$\n  \\hat{r} = \\frac{1}{n-1} \\sum_{k = 1}^{n} \\hat{Z}_x(x_k) \\hat{Z}_y(y_k)\n$$\n\nNote that $\\hat{r}$ is restricted to the closed interval $[-1, 1]$.\n\nThe figure below shows the SD line and the Regression line for the father-son data.\n\n```{r g_fs_points}\ng_fs_points <- father_son_ht |> \n  ggplot(mapping = aes(x = father, y = son)) + \n  geom_point()\n```\n\n```{r g_fs_lines}\nSD_slope = fs_moments[[\"s_sd\"]] / fs_moments[[\"f_sd\"]]\nR_slope  = fs_moments[[\"r\"]] * SD_slope\n\ng_fs_lines <- g_fs_points + \n  # SD line\n  geom_abline(\n    slope = SD_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - SD_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"dotted\", \n    linewidth = 2, \n    color = \"red\"\n  ) + \n  # Regression line\n  geom_abline(\n    slope = R_slope, \n    intercept = \n      fs_moments[[\"s_avg\"]] - R_slope * fs_moments[[\"f_avg\"]], \n    linetype = \"solid\", \n    linewidth = 2, \n    color = \"blue\"\n  ) + \n  labs(\n    title = \"Father-son heights\", \n    subtitle = \"SD line (dotted red), Regression line (solid blue)\"\n  )\ng_fs_lines\n```\n\nThe two lines intersect at the \"point of averages\", that is, at $(\\hat{\\mu}_x, \\hat{\\mu}_y)$, which need not coincide with any data point.\n\nThe following figure and table summarize the regression \"residuals\", that is the son's height $(y)$ minus the height $(\\hat{y})$ predicted by the linear model.\n\n```{r lm_fs}\nlm_fs <- lm(\n  data = father_son_ht, \n  formula = son ~ father\n)\nlm_fs\n```\n\n```{r fs_residuals}\n# construct a data table including regression residuals\nfs_residuals <- father_son_ht |> \n  mutate(\n    fitted_value = lm_fs$fitted.values, \n    residual     = lm_fs$residuals\n  )\n```\n\n```{r g_fs_residuals}\ng_fs_residuals <- fs_residuals |> \n  ggplot(mapping = aes(x = residual)) + \n  geom_histogram() + \n  labs(\n    title = \"Histogram of regression residuals\", \n    subtitle = \"son's height minus predicted height\"\n  )\ng_fs_residuals\n```\n\n```{r fs_residuals__smy}\nfs_residuals$residual |> summary() |> print(digits = 1)\n```\n\nThere are many ways to examine how well a model represents the data. Here's a scatter diagram of the value predicted (fitted) by the model versus the son's actual height.\n\n```{r g_resid_son}\ng_resid_son <- fs_residuals |> \n  ggplot(mapping = aes(\n    x = son, y = fitted_value\n  )) + \n  geom_point() + \n  geom_abline(\n    intercept = 0, \n    slope = 1, \n    linetype = \"dotted\", \n    linewidth = 2, \n    colour = \"red\"\n  ) + \n  labs(\n    title = \"Model-fitted value versus son's height\", \n    subtitle = \"(reference line: fitted = son)\"\n  )\ng_resid_son\n```\n\nWe see that the sons who are extremely short or extremely tall are not represented well by the model, which is heavily influenced by mid-range father-son heights containing most of the data. The father's height alone is a helpful but imperfect predictor of the son's height.\n\n### The Normal distribution\n\nThe father-son data set is well approximated by a bivariate normal distribution. The parameter estimates are as follows.\n\n```{r fs_moments__show}\nfs_moments |> print(digits = 1)\n```\n\nSons are on average about an inch taller than fathers. Fathers and sons share similar standard deviations (2.7 versus 2.8). The sample correlation coefficient is about 0.5.\n\nAmong the mathematical properties of normal distributions is the fact that if the pair of random variables $(X, Y)$ has a bivariate normal distribution, then the conditional expectation $E(Y | X)$ is indeed the linear regression function $\\mathcal{l}_R(X)$ whose equation is that of the population regression line, $Z_y(Y) = r Z_x(X)$. The conditional distribution $\\mathcal{D}(Y | X)$ of $Y$ given $X$ is normal with a mean of $\\mathcal{l}_R(X)$ and a standard deviation of $\\sqrt{1 - r^2} \\; \\sigma_y$. Conditioning on $X$ thus shrinks the standard deviation of $Y$ by a factor of $\\sqrt{1 - r^2}$. For the father-son data, with $r$ approximately equal to 0.5, this shrinkage factor is approximately 0.87, a 13% reduction in the standard deviation of $Y$.\n\n## Cautionary Remarks\n\n### Robust statistics\n\nThe sample average (arithmetic mean) is notoriously sensitive to outliers (data points far removed from most of the other data points). For this reason, the median is often used in place of the mean to describe central or typical values. For example, medians are commonly used to typify home prices in a neighborhood, and for other financial data.\n\nSimilarly, the interquartile range (IQR, the third minus the first quartile of the data) may be preferred to the standard deviation to measure how widely data points are spread around a central value (e.g., median).\n\nIn the present context this means that both the SD line and the Regression line are highly sensitive to outlying data points.\n\n### Anscombe Quartet\n\nProfessor [Frank Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) constructed the \"Anscombe Quartet\": 4 data sets, each consisting of 11 observations of $(x, y)$ pairs of numeric values. Here are the statistics per group.\n\n```{r anscombe_tbl}\nanscombe_tbl <- datasets::anscombe |> \n  as_tibble()\n\n# construct 4 groups of x-values\nx_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, x1:x4) |> \n  pivot_longer(\n    cols = x1:x4, \n    names_to = \"grp\", \n    names_prefix = \"x\", \n    values_to = \"x\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# construct 4 groups of y-values\ny_tbl <- anscombe_tbl |> \n  # original row index\n  mutate(idx = 1:nrow(anscombe)) |> \n  dplyr::select(idx, y1:y4) |> \n  pivot_longer(\n    cols = y1:y4, \n    names_to = \"grp\", \n    names_prefix = \"y\", \n    values_to = \"y\"\n  ) |> \n  mutate(grp = as.integer(grp))\n\n# join x and y values\nxy_long <- x_tbl |> left_join(\n  y  = y_tbl, \n  by = c(\"idx\", \"grp\")\n) |> \n  dplyr::select(grp, idx, x, y) |> \n  arrange(grp, idx)\n```\n\n```{r xy_stats}\nxy_stats <- xy_long |> \n  summarise(\n    .by   = grp, \n    x_avg = mean(x, na.rm = TRUE), \n    y_avg = mean(y, na.rm = TRUE), \n    x_sd  = sd(x, na.rm = TRUE), \n    y_sd  = sd(y, na.rm = TRUE), \n    r     = cor(x, y)\n  )\nxy_stats\n```\n\nThe four groups share identical averages, standard deviations, and $(x, y)$ correlation coefficients. Consequently the four data sets generate identical regression lines. Yet, as shown below, the pattern of $(x, y)$ values differs markedly among these data sets.\n\n```{r g_xy}\ng_xy <- xy_long |> \n  ggplot(mapping = aes(\n    x = x, y = y, group = grp\n  )) + \n  geom_point() + \n  facet_grid(cols = vars(grp))\ng_xy\n```\n\nMoral: pay attention to the data! The graphical and tabular summaries we choose to present should be useful and informative. For example, it might be helpful to note that group 2 looks like the partial outline of a parabola. We should note that group 3 consists of 10 points falling on a line, with one outlier. In group 4 we should note that 10 of the 11 data values are identical. We want to minimize the chance of inadvertently conveying false impressions by merely reporting standard summary statistics.\n\n## Class Exercise: Diamond Data\n\nTeam up with a classmate and load the diamond data provided by R package `ggplot2`. Of the 10 variables (data columns) choose one of them as the response variable $(y)$, and another as a predictor variable $(x)$. Construct a scatter diagram of $(x, y)$ data points. Calculate the equation of the regression line. Is the predictor variable useful, or irrelevant? The R package `stats` includes potentially helpful functions including a linear regression function, `stats::lm()`, and a local polynomial regression function `stats::loess()`. Take 20 minutes to prepare to report out to the class.\n\n```{r diamonds}\n# make your own copy of the diamond data\ndiamonds <- ggplot2::diamonds\n```\n\n## Statistical Independence\n\nThe father-son data is an example of a pair of statistically dependent variables, since the distribution of sons' heights changes when conditioned on the father's height. (The same can be said for fathers' heights conditioned on the height of the son.) Short fathers tend to have short sons; tall fathers tend to have tall sons.\n\nPractical examples of independent variables exist but are rare, since studies typically collect data on variables believed to be related. Nevertheless, the concept of statistical independence is very useful, as it gives rise to measures of departure from statistical independence (and thus measures of statistical association).\n\nThe correlation coefficient $r$ is an example of such a measure for two continuous variables. If $(X, Y)$ is a pair of statistically independent variables, then $r = 0$. Note, however, that $(X, Y)$ may be statistically dependent even if they are uncorrelated, that is, even if $r = 0$.\n\n### Definition\n\nThe pair of random variables $(X, Y)$ is defined to be statistically independent if\n\n$$\n\\begin{align}\n  P(X \\in A, Y \\in B) &= P(X \\in A) \\times P(Y \\in B) \\\\\n  & \\text{for all possible sets } A, B \\\\ \n\\end{align}\n$$\n\nIn this case we have\n\n$$\n\\begin{align}\n  P(Y \\in B | X \\in A) &= \\frac{P(X \\in A, Y \\in B)}{P(X \\in A)} \\\\ \n  &= P(Y \\in B) \\\\ \n  & \\text{for all possible sets } A, B \\\\\n\\end{align}\n$$\n\nThat is, the conditional probability of random variable $Y$ belonging to set $B$ given that $X$ belongs to set $A$ is equal to the unconditional probability that $Y$ belongs to set $B$. It follows that the conditional expectation $E(Y | X)$ does not depend on $X$, and thus equals the constant $E(Y)$, the unconditional expected value of $Y$.\n\n### The case when X and Y are categorical variables\n\nAs previously noted, for a pair $(X,Y)$ of continuous variables, the correlation coefficient is a measure (though not a definitive measure) of statistical association or dependence. In the case when $X$ and $Y$ are each restricted to a discrete set of values, the chi-square statistic is a useful measure of statistical association.\n\nWe use data on handedness (right, left, or ambidextrous) of US adults aged 25-34. The data were collected by the US Health and Nutrition Examination Survey (HANES), as cited in [FPP](https://www.goodreads.com/book/show/147358.Statistics). The question we investigate is whether handedness is independent of sex (male, female). Here are the counts for the six combinations of handedness and sex.\n\n```{r hs_tbl}\nhs_tbl <- gen_handedness()\n```\n\n```{r hs_wide}\nhs_wide <- hs_tbl |> \n  pivot_wider(\n    names_from  = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r hs_wide__kable}\nhs_wide |> knitr::kable(\n  caption = \"Handedness of sampled males and females\", \n  col.names = c(\"handedness\", \"male\", \"female\")\n)\n```\n\nThe percentages of handedness among males and among females are as follows.\n\n```{r h_smy}\nh_smy <- hs_tbl |> \n  summarise(\n    .by = \"hnd\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 3 × 2\n#   hnd   count\n#   <chr> <int>\n# 1 right  2004\n# 2 left    205\n# 3 ambi     28\n```\n\n```{r s_smy}\ns_smy <- hs_tbl |> \n  summarise(\n    .by = \"sex\", \n    count = sum(count, na.rm = TRUE)\n  )\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <int>\n# 1 male    1067\n# 2 female  1170\n```\n\n```{r hs_pct_wide}\nm_total <- s_smy[[1, 2]]\nf_total <- s_smy[[2, 2]]\n\nhs_pct_wide <- hs_wide |> \n  mutate(\n    male   = 100 * male   / m_total, \n    female = 100 * female / f_total\n  )\n```\n\n```{r hs_pct_wide__kable}\nhs_pct_wide |> knitr::kable(\n  caption = \"Percentage handedness among males, and among females\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n```\n\nIf handedness and sex were independent, we should see similar percentages of males and females for each type of handedness. The above table indeed shows similar percentages, but are they close enough to conclude independence?\n\nIn the early 1900's Karl Pearson developed the chi-squared test of independence of categorical variables. The reasoning is as follows. Suppose we accept the data values for the overall distribution of handedness (across males and females), and we also accept the distribution between males and females (across handedness). These distributions are not in dispute. What we're investigating concerns the cell percentages, combinations of handedness and sex. Under the assumption of independence we would expect the cell percentages in the data to be close to the product of the handedness percentage and the male/female percentage. This gives us an expected percentage (assuming independence), which we convert to an expected count. Pearson's test of independence is based on the following chi-squared statistic.\n\n$$\n\\begin{align}\n  \\chi^2 &= \\sum_{j = 1}^J {\\sum_{k = 1}^K {\\frac{(O_{j,k} - E_{j,k})^2}{E_{j,k}}}} \\\\ \n  O_{j,k} &= \\text{observed count for cell } \\{j, k\\} \\\\ \n  E_{j,k} &= \\text{expected count for cell } \\{j, k\\} \\\\ \n\\end{align}\n$$\n\nPearson calculated the distribution of this statistic mathematically based on the notion of \"degrees of freedom\".\n\nThat is, for each index $j$ the expected values summed across $k$ are constrained to match the corresponding sum of the observed values. Similarly, for each index $k$ the expected values summed across $j$ are constrained to match the corresponding sum of the observed values. With these fixed marginal sums cell values can vary with $(J-1) \\times (K-1)$ degrees of freedom.\n\nUnder the assumption of independence, the chi-squared statistic follows the distribution of the sum of squared independent standard normal variables, the number of independent normal variables matching the degrees of freedom.\n\n```{r hs_chi_sq}\nhs_chi_sq <- hs_wide |> \n  dplyr::select(male, female) |> \n  as.matrix() |> \n  chisq.test()\n\n# data:  as.matrix(dplyr::select(hs_wide, male, female))\n# X-squared = 11.806, df = 2, p-value = 0.002731\n```\n\nFor the handedness data the degrees of freedom equals 2, and the value of the statistic is `r round(hs_chi_sq$statistic, 1)`, which is beyond the 99% quantile of the distribution (and thus yields a \"p-value\" of less than 1%). This would be regarded as strong evidence against the assumption of independence.\n\nThe chi-square statistic is the sum of squared terms of the following form.\n\n$$\n\\begin{align}\n  \\frac{O_{j,k} - E_{j,k}}{\\sqrt{E_{j,k}}} \\\\ \n\\end{align}\n$$\n\nThese terms are called \"Pearson residuals\". For the handedness data, the Pearson residuals are as follows.\n\n```{r hs_resid_wide}\n# Pearson residuals\nhs_resid_wide <- hs_wide |> \n  dplyr::select(hnd) |> \n  mutate(\n    male   = hs_chi_sq$residuals[, 1], \n    female = hs_chi_sq$residuals[, 2]\n  )\n```\n\n```{r hs_resid_wide__kable}\nhs_resid_wide |> knitr::kable(\n  caption = \"Pearson residuals for handedness data\", \n  col.names = c(\"handedness\", \"male\", \"female\"), \n  digits = 1\n)\n\n```\n\nRoughly speaking, under independence the magnitude of cell values should align with the scale of standard normal variables. In this case the large value of the chi-square statistic cannot be attributed to a single cell of the table, but rather to the left-handed and ambidextrous cells, handedness to which males are more prone than females.\n\n### Simpson's Paradox\n\nWe'll use data from a study of graduate admissions at UC Berkeley in 1973 available in R as `datasets::UCBAdmissions`. The study was prompted by a concern of bias against females. The table below summarizes admission percentages for males and for females across the six largest departments.\n\n```{r ucb_admissions}\n# reformat data\nucb_admissions <- datasets::UCBAdmissions |> \n  as_tibble() |> \n  rename_with(tolower) |> \n  rename(sex = gender) |> \n  rename(count = n) |> \n  dplyr::select(dept, sex, admit, count)\n```\n\n```{r ucb_sex_smy}\nucb_sex_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sex_smy\n# # A tibble: 2 × 2\n#   sex    count\n#   <chr>  <dbl>\n# 1 Male    2691\n# 2 Female  1835\n```\n\n```{r ucb_sa_smy}\nucb_sa_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"sex\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_sa_smy\n# # A tibble: 4 × 3\n#   sex    admit    count\n#   <chr>  <chr>    <dbl>\n# 1 Male   Admitted  1198\n# 2 Male   Rejected  1493\n# 3 Female Admitted   557\n# 4 Female Rejected  1278\n```\n\n```{r ucb_sa_wide}\nucb_sa_wide <- ucb_sa_smy |> \n  pivot_wider(\n    names_from = \"sex\", \n    values_from = \"count\"\n  )\n```\n\n```{r ucb_sa_pct}\nucb_sa_pct <- ucb_sa_wide |> \n  mutate(\n    Male   = 100 * Male   / ucb_sex_smy[[1, 2]], \n    Female = 100 * Female / ucb_sex_smy[[2, 2]]\n  )\n```\n\n```{r ucb_sa_pct__kable}\nucb_sa_pct |> knitr::kable(\n  caption = \"Admission percentages for males and for females\", \n  digits = 1\n)\n```\n\nThese percentages look damning, but the table below showing admission rates per department tells a different story. (The departments are denoted A through F.)\n\n```{r ucb_dept_smy}\nucb_dept_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\"), \n    count = sum(count, na.rm = TRUE)\n  )\n# ucb_dept_smy\n# # A tibble: 6 × 2\n#   dept  count\n#   <chr> <dbl>\n# 1 A       933\n# 2 B       585\n# 3 C       918\n# 4 D       792\n# 5 E       584\n# 6 F       714\n```\n\n```{r ucb_da_smy}\nucb_da_smy <- ucb_admissions |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_da_wide}\nucb_da_wide <- ucb_da_smy |> \n  pivot_wider(\n    names_from = admit, \n    values_from = count\n  )\n# ucb_da_wide\n# # A tibble: 6 × 3\n#   dept  Admitted Rejected\n#   <chr>    <dbl>    <dbl>\n# 1 A          601      332\n# 2 B          370      215\n# 3 C          322      596\n# 4 D          269      523\n# 5 E          147      437\n# 6 F           46      668\n```\n\n```{r ucb_m_d_smy}\nucb_m_d_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_f_d_smy}\nucb_f_d_smy <- ucb_admissions |> \n  filter(sex == \"Female\") |> \n  summarise(\n    .by = \"dept\", \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_m_da_smy}\nucb_m_da_smy <- ucb_admissions |> \n  filter(sex == \"Male\") |> \n  summarise(\n    .by = c(\"dept\", \"admit\"), \n    count = sum(count, na.rm = TRUE)\n  )\n```\n\n```{r ucb_m_da_wide}\nucb_m_da_wide <- ucb_m_da_smy |> \n  pivot_wider(\n    names_from  = \"admit\", \n    values_from = \"count\"\n  )\n```\n\n```{r ucb_m_da_pct}\nucb_m_da_pct <- ucb_m_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_m_d_smy$count, \n    Rejected = 100 * Rejected / ucb_m_d_smy$count\n  )\n```\n\n```{r ucb_da_pct}\nucb_da_pct <- ucb_da_wide |> \n  mutate(\n    Admitted = 100 * Admitted / ucb_dept_smy$count, \n    Rejected = 100 * Rejected / ucb_dept_smy$count\n  )\n```\n\n```{r ucb_das_pct}\nucb_das_pct <- ucb_da_pct |> \n  dplyr::select(dept, Admitted) |> \n  left_join(\n    y  = XXX, \n    by = \"dept\"\n  ) |> \n  left_join(\n    y  = YYY, \n    by = \"dept\"\n  )\n```\n\n## Class Exercise: General Social Survey Data\n\nThe [General Social Survey](https://gss.norc.org/) is a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey consists of thousands of questions designed to monitor changes in social characteristics and attitudes in the US. In R package `forcats`, Hadley Wickham formed the data set `gss_cat` consisting of just a few columns to illustrate the challenges one encounters when working with categorical (non-numeric) variables (\"factors\").\n\nTeam up with a classmate and load the `gss_cat` data-set provided by R package `forcats`. How many rows of data are there? How many columns? What questions occur to you about the data? How might you address those questions? Take 15 minutes to prepare to report out to the class.\n\n```{r gss_cat}\n# make your own copy of the survey data \ngss_cat <- forcats::gss_cat\n```\n\n## Discussion: what should EDA mean?\n\nExploratory Data Analysis (EDA) is an approach to data analysis advocated by [John Tukey](https://en.wikipedia.org/wiki/John_Tukey), a leading American statistician of the 20th century. The approach contrasts with what Tukey called \"confirmatory analysis\", that is, a focus on probability models of data-generation along with the estimation or testing of model parameters. The difference is one of emphasis: EDA includes models suggested by data, but with an emphasis on understanding current and potential data sets.\n\nThe exploration is led by one's questions about the data. Relevant questions may or may not be obvious (or given). Variables may or may not be readily categorized as \"response variables\" versus \"predictor variables\". The ability to develop and recognize relevant questions is an important skill largely gained through experience.\n\nImportant EDA outcomes include\n\n-   the discovery of unanticipated data patterns, and\n\n-   proposals to examine tentative answers suggested by the current data, perhaps using a new data set designed for this purpose.\n\nEDA methods are used within the context of confirmatory analysis to examine the data for errors not encompassed by the models under study (e.g., errors in data transcription or transmission), or to search for other departures from model assumptions.\n\nEDA methods can be broadly understood as the methods of descriptive statistics: data summaries (graphical or tabular) intended to enhance our understanding of the data. EDA differs from descriptive statistics in a reliance on the questions of the data analyst and a readiness to examine various transformations of the data.\n\nAs an example, here are some ways we might address the question of how, if at all, the heights of fathers and sons differ in the data presented above.\n\n```{r height_per_fs}\n# render \"father\" and \"son\" as levels of factor \"fs\"\nheight_per_fs <- father_son_ht |> \n  # continue to identify father-son pairs\n  mutate(idx = 1:nrow(father_son_ht)) |>\n  dplyr::select(idx, father, son) |> \n  pivot_longer(\n    cols = c(father, son), \n    names_to = \"fs\", \n    values_to = \"height\"\n  )\n```\n\n```{r g_ht_per_fs}\ng_ht_per_fs <- height_per_fs |> \n  ggplot(mapping = aes(x = fs |> as_factor(), y = height)) + \n  geom_boxplot()\ng_ht_per_fs\n```\n\n```{r ht_per_fs__summarise}\nfs_smy <- height_per_fs |> dplyr::select(fs, height) |>\n  group_by(fs) |> \n  summarise(\n    min = min(height), \n    mid = median(height), \n    avg = mean(height), \n    max = max(height)\n  )\nfs_smy |> print(digits = 1)\n```\n\nThe box-plot and table above show that, on average, sons are about an inch taller than fathers. Here's a histogram of the difference in heights (son minus father) across father-son pairs.\n\n```{r s_minus_f}\nfather_son_ht <- father_son_ht |> \n  mutate(s_minus_f = son - father)\n```\n\n```{r s_minus_f__hist}\ng_s_minus_f <- father_son_ht |> \n  ggplot(mapping = aes(x = s_minus_f)) + \n  geom_histogram()\ng_s_minus_f\n```\n\n```{r s_minus_f__smy}\nfather_son_ht$s_minus_f |> \n  summary() |> \n  print(digits = 1)\n```\n\nThe summary of individual differences in height (son minus father) strengthens the previous aggregate summaries: the distribution of son's height minus father's height is fairly symmetric around a difference of about one inch.\n\n## Team Exercises\n\n1.  Response versus predictor variables: for each of the data sets presented above, propose one or more variables as response variables. How, if at all, might someone else argue for a different choice? What should we mean by \"response\" and \"predictor\" variables? Describe a situation in which this distinction would not be suitable.\n\n2.  Regression to the mean: the discussion of father-son heights asserts that: \"extremely tall or short fathers \\[correspond\\] to not quite so extremely tall or short sons, respectively\". How would you formulate the meaning of that phrase? Do the data demonstrate this phenomenon?\n\n3.  Diamond data: propose a question for the diamond data, and then try to address that question. What (if anything) did you learn from this task?\n\n4.  Survey data: propose a question for the gss_cat data, and then try to address that question. What (if anything) did you learn from this task?\n\n## Resources\n\n[R Graphics Cookbook (2e)](https://r-graphics.org/) by Winston Chang\n\n[Statistics (4e)](https://www.goodreads.com/book/show/147358.Statistics) by Freedman, Pisani, Purves \\| Goodreads\n\n[Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) (probability theory) - Wikipedia\n\n[Sex bias in graduate admissions](https://pubmed.ncbi.nlm.nih.gov/17835295/): data from Berkeley, by Bickel, Hammel, and O'connell\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_1b_4350_2024-12-27_0500.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Conditional Distributions","subtitle":"Part 1, session 1b of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Review concepts and techniques of exploratory data analysis."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}