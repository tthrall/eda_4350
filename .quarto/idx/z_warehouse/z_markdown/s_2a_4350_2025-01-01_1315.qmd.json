{"title":"Clustering: EDA in Higher Dimensions","markdown":{"yaml":{"title":"Clustering: EDA in Higher Dimensions","subtitle":"Part 1, session 2a of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"abstract":"Introduce statistical clustering methods."},"headingText":"if needed, load local code as follows","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\n\n# source(here(\"code\", \"handedness_data.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Initial Remarks\n\nIn the context of machine learning, \"unsupervised learning\" encompasses algorithms designed to seek patterns in data, with minimal required guidance from the user, and without the benefit of a response variable (or a set of observation \"labels\") to be approximated by functions of predictor variables.  Most but not all of these algorithms are designed for high-dimensional data, that is, data with many variables (columns).\n\nThis approach is in the spirit of exploratory data analysis (EDA), which is practically forced upon us when we come across a new data set (and can remain essential throughout the life-cycle of a project). On the one hand, EDA has no sharp measures of success that we are driven to optimize, so one might say that we're strolling through the data, taking in the scenery. On the other hand, we are looking for unanticipated patterns in the data, and therefore it is useful to record just what patterns we do anticipate, conjecture, or wonder about.\n\nIn our previous sessions we have reviewed some ways of looking at one or two variables at a time. When we are presented with a new data set having many variables, it is useful to simplify the data in some way to help us form a first impression.\n\nOne way is to look for a few functions of the many variables that somehow carry much of the original information. The prime method for doing so is principal component analysis (PCA), where the functions are linear and capture much of the variation in the data. We'll come back to PCA in subsequent sessions.\n\nIn this session we'll discuss another way to simplify the data, namely, to group observations having similar profiles (patterns in the values of the variables). Such clustering methods are currently of great interest in both science and industry, giving rise to a number of new clustering methods.\n\n## US Colleges\n\n```{r college}\ncollege <- ISLR2::College |> \n  as_tibble(rownames = \"college_name\")\n```\n\n```{r college_vars_tbl}\ncollege_vars_tbl <- tibble::tibble(\n  var_name = names(college),\n) |> \n  mutate(\n    dscr = case_when(\n      var_name == \"college_name\" ~ \n        \"Name of the college or university\", \n      var_name == \"Private\" ~ \n        \"No or Yes indicating private or public\", \n      var_name == \"Apps\" ~ \n        \"Number of applications received\", \n      var_name == \"Accept\" ~ \n        \"Number of applications accepted\", \n      var_name == \"Enroll\" ~ \n        \"Number of new students enrolled\", \n      var_name == \"Top10perc\" ~ \n        \"Pct. new students from top 10% of H.S. class\", \n      var_name == \"Top25perc\" ~ \n        \"Pct. new students from top 25% of H.S. class\", \n      var_name == \"F.Undergrad\" ~ \n        \"Number of fulltime undergraduates\", \n      var_name == \"P.Undergrad\" ~ \n        \"Number of parttime undergraduates\", \n      var_name == \"Outstate\" ~ \n        \"Out-of-state tuition\", \n      var_name == \"Room.Board\" ~ \n        \"Room and board costs\", \n      var_name == \"Books\" ~ \n        \"Estimated book costs\", \n      var_name == \"Personal\" ~ \n        \"Estimated personal spending\", \n      var_name == \"PhD\" ~ \n        \"Pct. of faculty with PhD's\", \n      var_name == \"Terminal\" ~ \n        \"Pct. of faculty with terminal degree\", \n      var_name == \"S.F.Ratio\" ~ \n        \"Student/faculty ratio\", \n      var_name == \"perc.alumni\" ~ \n        \"Pct. alumni who donate\", \n      var_name == \"Expend\" ~ \n        \"Instructional expenditure per student\", \n      var_name == \"Grad.Rate\" ~ \n        \"Graduation rate\"\n    )\n  )\n```\n\nWe begin with data about US universities and colleges from 1995, described in the book, *An Introduction to Statistical Learning with applications in R* ([ISLR](https://www.statlearning.com)). The data are recorded within the R package `ISLR2` as `ISLR2::College`.  From the R command `help(\"College\")` we see that the data consist of 777 observations (data rows), and obtain the following description of the variables (data columns).\n\n```{r college_vars_tbl__kable}\ncollege_vars_tbl |> knitr::kable(\n  caption = \"US college variables (1995 issue of USNWR)\", \n  col.names = c(\"variable\", \"description\")\n)\n```\n\n### Class Exercise\n\nTeam up with a classmate and make your own copy of the `ISLR2::College` data.  Record your questions and conjectures about the data.  Which of these could be addressed by the set of data variables?  Take 15 minutes to prepare to report out to the class.\n\n### K-means Clustering\n\nWe'll start our discussion of clustering using a function in the R `stats` package, namely `stats::kmeans()`.  The function requires us to specify either the desired number $K$ of clusters (groups) of observations to be formed, or else to provide an initial set of $K$ cluster centers. If we merely provide the desired number $K$ of clusters, `kmeans()` randomly selects $K$ data rows (observations) as the initial set of cluster centers. \n\nGiven a set of cluster centers the algorithm iteratively searches for a better set, meaning a set having a smaller \"within-cluster sum of squares\".  In broad terms, this is done in two steps.\n\n  1.  Assign: the algorithm assigns each data point to the nearest center (in Euclidean distance) thereby partitioning the data points into $K$ clusters.\n  1.  Update: For each cluster of data points, the algorihtm calculates a new cluster center, namely, the mean vector of the data points within the cluster.  This yields a new set of cluster centers.\n  \nConvergence criteria: For each cluster the algorithm adds up the (point, center) squared distances.  Those $K$ sums of squares are summed to form the \"within-cluster sum of squares (WCSS)\".  In addition the algorithm records the grand mean vector, the average across the entire data set and sums the squared (cluster-center, grand-mean) Euclidean distances, which is called the \"between-cluster sum of squares (BCSS)\".  The algorithm concludes the search when cluster-membership no longer changes, or when the decrease in WCSS is sufficiently small.\n\n#### Grouping variables\n\nLet's try specifying $K$ cluster centers based on some initial ideas about how the observations might be grouped.  To specify cluster centers let's try grouping the data based on binary factors (splitting on the median value) derived from the following variables:\n\n  - enrollment percentage (100 * Enroll / Apps)\n  - Top10perc\n  - Expend\n\nBefore proceeding, let's examine the distribution of these variables.  Here's a set of scatter plots showing each pair of the above three variables.\n\n```{r college_factors}\n# split selected variables at their median value\ncollege_factors <- college |> \n  mutate(\n    enroll_pct = 100 * Enroll / Apps,\n    enroll_fct = enroll_pct < median(enroll_pct),\n    top_10_fct = Top10perc  < median(Top10perc),\n    expend_fct = Expend     < median(Expend)\n  ) |> \n  # for binary factors above, re-code (T, F) to (\"lwr\", \"upr\")\n  mutate(across(\n    .cols = c(enroll_fct, top_10_fct, expend_fct), \n    .fns  = ~ if_else(.x, \"lwr\", \"upr\")\n  )) |> \n  # label each group with 3 bits\n  mutate(\n    enroll_lbl = if_else(enroll_fct == \"lwr\", 0L, 1L), \n    top_10_lbl = if_else(top_10_fct == \"lwr\", 0L, 1L), \n    expend_lbl = if_else(expend_fct == \"lwr\", 0L, 1L), \n    grp_lbl    = paste0(\"g_\", enroll_lbl, top_10_lbl, expend_lbl)\n  ) |> \n  # remove redundant labeling vars\n  dplyr::select(- enroll_lbl, - top_10_lbl, - expend_lbl)\n\n# record the median values\nenr_pct_mid <- median(college_factors$ enroll_pct)\ntop_10_mid  <- median(college_factors$ Top10perc)\nexpend_mid  <- median(college_factors$ Expend)\n```\n\n```{r g_grp_var_matrix}\n# Form a matrix of figures, showing each \n# distinct pair of variables in a scatter diagram.\ng_grp_var_matrix <- college_factors |> \n  GGally::ggpairs(\n    columns = c(\"enroll_pct\", \"Top10perc\", \"Expend\")\n  )\ng_grp_var_matrix\n```\n\nWe see that `enroll_pct` is negatively correlated with the other two variables, `Top10perc` and `Expend`, which are positively correlated.  Also note that `Expend` is measured (in US dollars) on a very different scale from the other two variables (to be further discussed).\n\nNow let's look at the three variables more closely, one at a time.\n\n```{r g_enroll_pct}\ng_enroll_pct <- college_factors |> \n  ggplot(mapping = aes(x = enroll_pct)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = enr_pct_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% enrolled per application\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_enroll_pct\n```\n\n```{r g_top_10}\ng_top_10 <- college_factors |> \n  ggplot(mapping = aes(x = Top10perc)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = top_10_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% of new students from top 10% in their HS\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_top_10\n```\n\n```{r g_Expend}\ng_Expend <- college_factors |> \n  ggplot(mapping = aes(x = Expend)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = expend_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"Instructional expenditure per student\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_Expend\n```\n\nThe figures above show decent variation on either side of the median value.  Therefore cutting each of the 3 variables at their respective medians is not unduly distorting, as long as we bear in mind that each distribution has a right tail (with varying degrees of positive skewness).\n\nFor each of the variables shown we create a binary variable having two levels, \"lwr\" and \"upr\", meaning below the median value, or else no less than the median value.  We then form 8 groups, each a combination of levels of the 3 binary variables, which we label $g_{000}, g_{001}, \\ldots, g_{111}$ to denote the binary factors based on `enroll_pct`, `Top10perc`, and `Expend` (in that order), with {\"lwr\", \"upr\"} coded as {0, 1}.\n\nWe will provide function `kmeans()` the averages of the numeric variables within each group, thus representing a sample of 777 observations by just 8 groups.  But we are still in a 18-dimensional space corresponding to the 18 numeric data columns (the original 17 plus `enroll_pct`).\n\nTo examine our initial grouping of the data let's first select the variables from which we defined the 8 groups and examine the scatter diagrams of each pair using color to distinguish points belonging to different groups.  This selection of variables should maximize the separation between groups, and just serves as a check.\n\nHere are scatter diagrams of each pair  of the variables `enroll_pct`, `Top10perc`, and `Expend`, with points colored by group membership.\n\n```{r g_enroll_top_10}\ng_enroll_top_10 <- college_factors |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Top10perc, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Top10perc)\", \n    subtitle = \"grouping variables (1, 2)\"\n  )\ng_enroll_top_10\n```\n\n```{r g_enroll_Expend}\ng_enroll_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Expend)\", \n    subtitle = \"grouping variables (1, 3)\"\n  )\ng_enroll_Expend\n```\n\n```{r g_top_10_Expend}\ng_top_10_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouping variables (2, 3)\"\n  )\ng_top_10_Expend\n```\n\nThe figures show good separation between groups, as we expected.\n\nAgain as a reality-check, the table below shows the mean per group of the underlying grouping variables.\n\n```{r means_per_grp}\nmeans_per_grp <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # reduce grouping variables to just grp_lbl\n  dplyr::select(- (enroll_fct:expend_fct)) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  ), \n  # grouping variable\n  .by = \"grp_lbl\", \n  # group size: number of original data rows per group\n  count = n()\n  ) |> \n  arrange(grp_lbl)\n```\n\n```{r means_per_grp__kable}\nmeans_per_grp |> \n  dplyr::select(\n    grp_lbl, enroll_pct, Top10perc, Expend\n  ) |> \n  knitr::kable(\n    caption = \"Mean values per group of underlying grouping variables\", \n    col.names = c(\"group\", \"enroll_pct\", \"Top10perc\", \"Expend\"), \n    digits = 1\n  )\n  \n```\n\n#### Results for specified inital cluster centers\n\nThe figures and table immediately above serve to check that we've not made some error in the formation of the 8 groups.  Calculated, but not shown, are the mean values per group of the remaining numeric variables.\n\nNow let's see the clusters that the `kmeans()` function devises when we provide it with initial cluster centers.  For each of our 8 groups we calculated a vector of mean values across 18 numeric variables.  We now provide these 8 vectors of mean values as 8 initial cluster centers to the `kmeans()` function.[^cc_matrix]\n\n[^cc_matrix]: `kmeans()` expects the cluster centers to be expressed as a matrix, in which each _row_ represents a single cluster center and each column  represents a \"coordinate\" of the cluster centers that matches and is labeled as one of the numeric variables (columns) of the data matrix.\n\nThe resulting clusters are shown in the scatter diagrams below, where for purposes of comparison we have projected the clusters onto the grouping variables we previously adopted.\n\n```{r m_kmeans_ctrs}\n# to ensure reproducibility, set random seed\nset.seed(42) # nod to Hitchhiker's Guide to the Galaxy\n\n# use means_per_grp as initial cluster centers\nm_kmeans_ctrs <- college_factors |> \n  # retain only original numeric variables and enroll_pct\n  dplyr::select(\n    - college_name, - Private, - (enroll_fct:grp_lbl)\n  ) |> \n  kmeans(\n    centers = means_per_grp |> \n      # retain only original numeric variables and enroll_pct\n      dplyr::select(\n        - grp_lbl, - count\n      )\n  )\n```\n\n```{r college_km_ctrs}\n# assign observations to km clusters\ncollege_km_ctrs <- college_factors |> \n  mutate(cluster = m_kmeans_ctrs$ cluster)\n```\n\n```{r g_enroll_top_10_kmc}\ng_enroll_top_10_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Top10perc, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Top10perc)\", \n    subtitle = \"grouping variables (1, 2)\"\n  )\ng_enroll_top_10_kmc\n```\n\n```{r g_enroll_Expend_kmc}\ng_enroll_Expend_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Expend, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) + \n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Expend)\", \n    subtitle = \"grouping variables (1, 3)\"\n  )\ng_enroll_Expend_kmc\n```\n\n```{r g_top_10_Expend_kmc}\ng_top_10_Expend_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) + \n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouping variables (2, 3)\"\n  )\ng_top_10_Expend_kmc\n```\n\n```{r km_ctr_tbl}\n# reformat cluster centers as a tibble\nkm_ctr_tbl <- m_kmeans_ctrs$ centers |> \n  as_tibble(rownames = \"cluster\") |> \n  mutate(cluster = as.integer(cluster)) |> \n  dplyr::select(cluster, everything())\n```\n\n```{r km_ctr_tbl__kable}\nkm_ctr_tbl |> \n  dplyr::select(cluster, enroll_pct, Top10perc, Expend) |> \n  knitr::kable(\n  caption = \"K-means: grouping variable means per cluster\", \n  digits = 1\n)\n```\n\nThe clusters formed by `kmeans()` seem rather similar to our original groups.  If we scrutinize the `kmeans()` clustering shown above, we can see that the clusters closely follow the `Expend` variable, which may be due to the large scale of that variable.\n\n#### Results for random inital cluster centers\n\n\n\n#### Which value of $K$?\n\n\n\n#### Other clustering algorithms\n\n\n\n## Team Exercises\n\n\n\n## Resources\n\n[An Introduction to Statistical Learning](https://www.statlearning.com/) by James, Witten, Hastie, & Tibshirani\n\n[K-means clustering with tidy data principles – tidymodels](https://www.tidymodels.org/learn/statistics/k-means/)\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  echo    = FALSE, \n  error   = FALSE, \n  message = FALSE, \n  warning = FALSE\n)\n```\n\n```{r libraries}\nlibrary(assertthat)\nlibrary(GGally)\nlibrary(here)\nlibrary(ISLR2)\nlibrary(latex2exp)\nlibrary(tidyverse)\nlibrary(tinytex)\nlibrary(UsingR)\n\n```\n\n```{r local_source}\n# if needed, load local code as follows\n\n# source(here(\"code\", \"handedness_data.R\"))\n```\n\n------------------------------------------------------------------------\n\n## Initial Remarks\n\nIn the context of machine learning, \"unsupervised learning\" encompasses algorithms designed to seek patterns in data, with minimal required guidance from the user, and without the benefit of a response variable (or a set of observation \"labels\") to be approximated by functions of predictor variables.  Most but not all of these algorithms are designed for high-dimensional data, that is, data with many variables (columns).\n\nThis approach is in the spirit of exploratory data analysis (EDA), which is practically forced upon us when we come across a new data set (and can remain essential throughout the life-cycle of a project). On the one hand, EDA has no sharp measures of success that we are driven to optimize, so one might say that we're strolling through the data, taking in the scenery. On the other hand, we are looking for unanticipated patterns in the data, and therefore it is useful to record just what patterns we do anticipate, conjecture, or wonder about.\n\nIn our previous sessions we have reviewed some ways of looking at one or two variables at a time. When we are presented with a new data set having many variables, it is useful to simplify the data in some way to help us form a first impression.\n\nOne way is to look for a few functions of the many variables that somehow carry much of the original information. The prime method for doing so is principal component analysis (PCA), where the functions are linear and capture much of the variation in the data. We'll come back to PCA in subsequent sessions.\n\nIn this session we'll discuss another way to simplify the data, namely, to group observations having similar profiles (patterns in the values of the variables). Such clustering methods are currently of great interest in both science and industry, giving rise to a number of new clustering methods.\n\n## US Colleges\n\n```{r college}\ncollege <- ISLR2::College |> \n  as_tibble(rownames = \"college_name\")\n```\n\n```{r college_vars_tbl}\ncollege_vars_tbl <- tibble::tibble(\n  var_name = names(college),\n) |> \n  mutate(\n    dscr = case_when(\n      var_name == \"college_name\" ~ \n        \"Name of the college or university\", \n      var_name == \"Private\" ~ \n        \"No or Yes indicating private or public\", \n      var_name == \"Apps\" ~ \n        \"Number of applications received\", \n      var_name == \"Accept\" ~ \n        \"Number of applications accepted\", \n      var_name == \"Enroll\" ~ \n        \"Number of new students enrolled\", \n      var_name == \"Top10perc\" ~ \n        \"Pct. new students from top 10% of H.S. class\", \n      var_name == \"Top25perc\" ~ \n        \"Pct. new students from top 25% of H.S. class\", \n      var_name == \"F.Undergrad\" ~ \n        \"Number of fulltime undergraduates\", \n      var_name == \"P.Undergrad\" ~ \n        \"Number of parttime undergraduates\", \n      var_name == \"Outstate\" ~ \n        \"Out-of-state tuition\", \n      var_name == \"Room.Board\" ~ \n        \"Room and board costs\", \n      var_name == \"Books\" ~ \n        \"Estimated book costs\", \n      var_name == \"Personal\" ~ \n        \"Estimated personal spending\", \n      var_name == \"PhD\" ~ \n        \"Pct. of faculty with PhD's\", \n      var_name == \"Terminal\" ~ \n        \"Pct. of faculty with terminal degree\", \n      var_name == \"S.F.Ratio\" ~ \n        \"Student/faculty ratio\", \n      var_name == \"perc.alumni\" ~ \n        \"Pct. alumni who donate\", \n      var_name == \"Expend\" ~ \n        \"Instructional expenditure per student\", \n      var_name == \"Grad.Rate\" ~ \n        \"Graduation rate\"\n    )\n  )\n```\n\nWe begin with data about US universities and colleges from 1995, described in the book, *An Introduction to Statistical Learning with applications in R* ([ISLR](https://www.statlearning.com)). The data are recorded within the R package `ISLR2` as `ISLR2::College`.  From the R command `help(\"College\")` we see that the data consist of 777 observations (data rows), and obtain the following description of the variables (data columns).\n\n```{r college_vars_tbl__kable}\ncollege_vars_tbl |> knitr::kable(\n  caption = \"US college variables (1995 issue of USNWR)\", \n  col.names = c(\"variable\", \"description\")\n)\n```\n\n### Class Exercise\n\nTeam up with a classmate and make your own copy of the `ISLR2::College` data.  Record your questions and conjectures about the data.  Which of these could be addressed by the set of data variables?  Take 15 minutes to prepare to report out to the class.\n\n### K-means Clustering\n\nWe'll start our discussion of clustering using a function in the R `stats` package, namely `stats::kmeans()`.  The function requires us to specify either the desired number $K$ of clusters (groups) of observations to be formed, or else to provide an initial set of $K$ cluster centers. If we merely provide the desired number $K$ of clusters, `kmeans()` randomly selects $K$ data rows (observations) as the initial set of cluster centers. \n\nGiven a set of cluster centers the algorithm iteratively searches for a better set, meaning a set having a smaller \"within-cluster sum of squares\".  In broad terms, this is done in two steps.\n\n  1.  Assign: the algorithm assigns each data point to the nearest center (in Euclidean distance) thereby partitioning the data points into $K$ clusters.\n  1.  Update: For each cluster of data points, the algorihtm calculates a new cluster center, namely, the mean vector of the data points within the cluster.  This yields a new set of cluster centers.\n  \nConvergence criteria: For each cluster the algorithm adds up the (point, center) squared distances.  Those $K$ sums of squares are summed to form the \"within-cluster sum of squares (WCSS)\".  In addition the algorithm records the grand mean vector, the average across the entire data set and sums the squared (cluster-center, grand-mean) Euclidean distances, which is called the \"between-cluster sum of squares (BCSS)\".  The algorithm concludes the search when cluster-membership no longer changes, or when the decrease in WCSS is sufficiently small.\n\n#### Grouping variables\n\nLet's try specifying $K$ cluster centers based on some initial ideas about how the observations might be grouped.  To specify cluster centers let's try grouping the data based on binary factors (splitting on the median value) derived from the following variables:\n\n  - enrollment percentage (100 * Enroll / Apps)\n  - Top10perc\n  - Expend\n\nBefore proceeding, let's examine the distribution of these variables.  Here's a set of scatter plots showing each pair of the above three variables.\n\n```{r college_factors}\n# split selected variables at their median value\ncollege_factors <- college |> \n  mutate(\n    enroll_pct = 100 * Enroll / Apps,\n    enroll_fct = enroll_pct < median(enroll_pct),\n    top_10_fct = Top10perc  < median(Top10perc),\n    expend_fct = Expend     < median(Expend)\n  ) |> \n  # for binary factors above, re-code (T, F) to (\"lwr\", \"upr\")\n  mutate(across(\n    .cols = c(enroll_fct, top_10_fct, expend_fct), \n    .fns  = ~ if_else(.x, \"lwr\", \"upr\")\n  )) |> \n  # label each group with 3 bits\n  mutate(\n    enroll_lbl = if_else(enroll_fct == \"lwr\", 0L, 1L), \n    top_10_lbl = if_else(top_10_fct == \"lwr\", 0L, 1L), \n    expend_lbl = if_else(expend_fct == \"lwr\", 0L, 1L), \n    grp_lbl    = paste0(\"g_\", enroll_lbl, top_10_lbl, expend_lbl)\n  ) |> \n  # remove redundant labeling vars\n  dplyr::select(- enroll_lbl, - top_10_lbl, - expend_lbl)\n\n# record the median values\nenr_pct_mid <- median(college_factors$ enroll_pct)\ntop_10_mid  <- median(college_factors$ Top10perc)\nexpend_mid  <- median(college_factors$ Expend)\n```\n\n```{r g_grp_var_matrix}\n# Form a matrix of figures, showing each \n# distinct pair of variables in a scatter diagram.\ng_grp_var_matrix <- college_factors |> \n  GGally::ggpairs(\n    columns = c(\"enroll_pct\", \"Top10perc\", \"Expend\")\n  )\ng_grp_var_matrix\n```\n\nWe see that `enroll_pct` is negatively correlated with the other two variables, `Top10perc` and `Expend`, which are positively correlated.  Also note that `Expend` is measured (in US dollars) on a very different scale from the other two variables (to be further discussed).\n\nNow let's look at the three variables more closely, one at a time.\n\n```{r g_enroll_pct}\ng_enroll_pct <- college_factors |> \n  ggplot(mapping = aes(x = enroll_pct)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = enr_pct_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% enrolled per application\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_enroll_pct\n```\n\n```{r g_top_10}\ng_top_10 <- college_factors |> \n  ggplot(mapping = aes(x = Top10perc)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = top_10_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"% of new students from top 10% in their HS\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_top_10\n```\n\n```{r g_Expend}\ng_Expend <- college_factors |> \n  ggplot(mapping = aes(x = Expend)) + \n  geom_histogram() + \n  geom_vline(\n    xintercept = expend_mid, linewidth = 2, colour = \"red\"\n  ) + \n  labs(\n    title = \"Instructional expenditure per student\", \n    subtitle = \"(vertical line at median)\"\n  )\ng_Expend\n```\n\nThe figures above show decent variation on either side of the median value.  Therefore cutting each of the 3 variables at their respective medians is not unduly distorting, as long as we bear in mind that each distribution has a right tail (with varying degrees of positive skewness).\n\nFor each of the variables shown we create a binary variable having two levels, \"lwr\" and \"upr\", meaning below the median value, or else no less than the median value.  We then form 8 groups, each a combination of levels of the 3 binary variables, which we label $g_{000}, g_{001}, \\ldots, g_{111}$ to denote the binary factors based on `enroll_pct`, `Top10perc`, and `Expend` (in that order), with {\"lwr\", \"upr\"} coded as {0, 1}.\n\nWe will provide function `kmeans()` the averages of the numeric variables within each group, thus representing a sample of 777 observations by just 8 groups.  But we are still in a 18-dimensional space corresponding to the 18 numeric data columns (the original 17 plus `enroll_pct`).\n\nTo examine our initial grouping of the data let's first select the variables from which we defined the 8 groups and examine the scatter diagrams of each pair using color to distinguish points belonging to different groups.  This selection of variables should maximize the separation between groups, and just serves as a check.\n\nHere are scatter diagrams of each pair  of the variables `enroll_pct`, `Top10perc`, and `Expend`, with points colored by group membership.\n\n```{r g_enroll_top_10}\ng_enroll_top_10 <- college_factors |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Top10perc, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Top10perc)\", \n    subtitle = \"grouping variables (1, 2)\"\n  )\ng_enroll_top_10\n```\n\n```{r g_enroll_Expend}\ng_enroll_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Expend)\", \n    subtitle = \"grouping variables (1, 3)\"\n  )\ng_enroll_Expend\n```\n\n```{r g_top_10_Expend}\ng_top_10_Expend <- college_factors |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = grp_lbl, shape = grp_lbl\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouping variables (2, 3)\"\n  )\ng_top_10_Expend\n```\n\nThe figures show good separation between groups, as we expected.\n\nAgain as a reality-check, the table below shows the mean per group of the underlying grouping variables.\n\n```{r means_per_grp}\nmeans_per_grp <- college_factors |> \n  # remove original non-numeric variables\n  dplyr::select(- college_name, - Private) |> \n  # reduce grouping variables to just grp_lbl\n  dplyr::select(- (enroll_fct:expend_fct)) |> \n  # vector of mean values per group\n  summarise(across(\n    .cols = everything(), \n    .fns  = ~ mean(.x, na.rm = TRUE)\n  ), \n  # grouping variable\n  .by = \"grp_lbl\", \n  # group size: number of original data rows per group\n  count = n()\n  ) |> \n  arrange(grp_lbl)\n```\n\n```{r means_per_grp__kable}\nmeans_per_grp |> \n  dplyr::select(\n    grp_lbl, enroll_pct, Top10perc, Expend\n  ) |> \n  knitr::kable(\n    caption = \"Mean values per group of underlying grouping variables\", \n    col.names = c(\"group\", \"enroll_pct\", \"Top10perc\", \"Expend\"), \n    digits = 1\n  )\n  \n```\n\n#### Results for specified inital cluster centers\n\nThe figures and table immediately above serve to check that we've not made some error in the formation of the 8 groups.  Calculated, but not shown, are the mean values per group of the remaining numeric variables.\n\nNow let's see the clusters that the `kmeans()` function devises when we provide it with initial cluster centers.  For each of our 8 groups we calculated a vector of mean values across 18 numeric variables.  We now provide these 8 vectors of mean values as 8 initial cluster centers to the `kmeans()` function.[^cc_matrix]\n\n[^cc_matrix]: `kmeans()` expects the cluster centers to be expressed as a matrix, in which each _row_ represents a single cluster center and each column  represents a \"coordinate\" of the cluster centers that matches and is labeled as one of the numeric variables (columns) of the data matrix.\n\nThe resulting clusters are shown in the scatter diagrams below, where for purposes of comparison we have projected the clusters onto the grouping variables we previously adopted.\n\n```{r m_kmeans_ctrs}\n# to ensure reproducibility, set random seed\nset.seed(42) # nod to Hitchhiker's Guide to the Galaxy\n\n# use means_per_grp as initial cluster centers\nm_kmeans_ctrs <- college_factors |> \n  # retain only original numeric variables and enroll_pct\n  dplyr::select(\n    - college_name, - Private, - (enroll_fct:grp_lbl)\n  ) |> \n  kmeans(\n    centers = means_per_grp |> \n      # retain only original numeric variables and enroll_pct\n      dplyr::select(\n        - grp_lbl, - count\n      )\n  )\n```\n\n```{r college_km_ctrs}\n# assign observations to km clusters\ncollege_km_ctrs <- college_factors |> \n  mutate(cluster = m_kmeans_ctrs$ cluster)\n```\n\n```{r g_enroll_top_10_kmc}\ng_enroll_top_10_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Top10perc, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) +\n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Top10perc)\", \n    subtitle = \"grouping variables (1, 2)\"\n  )\ng_enroll_top_10_kmc\n```\n\n```{r g_enroll_Expend_kmc}\ng_enroll_Expend_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = enroll_pct, y = Expend, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) + \n  geom_point() + \n  labs(\n    title = \"(enroll_pct, Expend)\", \n    subtitle = \"grouping variables (1, 3)\"\n  )\ng_enroll_Expend_kmc\n```\n\n```{r g_top_10_Expend_kmc}\ng_top_10_Expend_kmc <- college_km_ctrs |> \n  mutate(cluster = as_factor(cluster)) |> \n  ggplot(mapping = aes(\n    x = Top10perc, y = Expend, \n    colour = cluster, shape = cluster\n  )) + \n  scale_shape_manual(values = 0:7) + \n  geom_point() + \n  labs(\n    title = \"(Top10perc, Expend)\", \n    subtitle = \"grouping variables (2, 3)\"\n  )\ng_top_10_Expend_kmc\n```\n\n```{r km_ctr_tbl}\n# reformat cluster centers as a tibble\nkm_ctr_tbl <- m_kmeans_ctrs$ centers |> \n  as_tibble(rownames = \"cluster\") |> \n  mutate(cluster = as.integer(cluster)) |> \n  dplyr::select(cluster, everything())\n```\n\n```{r km_ctr_tbl__kable}\nkm_ctr_tbl |> \n  dplyr::select(cluster, enroll_pct, Top10perc, Expend) |> \n  knitr::kable(\n  caption = \"K-means: grouping variable means per cluster\", \n  digits = 1\n)\n```\n\nThe clusters formed by `kmeans()` seem rather similar to our original groups.  If we scrutinize the `kmeans()` clustering shown above, we can see that the clusters closely follow the `Expend` variable, which may be due to the large scale of that variable.\n\n#### Results for random inital cluster centers\n\n\n\n#### Which value of $K$?\n\n\n\n#### Other clustering algorithms\n\n\n\n## Team Exercises\n\n\n\n## Resources\n\n[An Introduction to Statistical Learning](https://www.statlearning.com/) by James, Witten, Hastie, & Tibshirani\n\n[K-means clustering with tidy data principles – tidymodels](https://www.tidymodels.org/learn/statistics/k-means/)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":{"html_document":{"toc":true,"df_print":"paged","mathjax":"default"},"word_document":{"toc":true,"df_print":"tibble"},"pdf_document":{"toc":true,"df_print":"tibble"}},"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"s_2a_4350_2025-01-01_1315.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","editor":"visual","title":"Clustering: EDA in Higher Dimensions","subtitle":"Part 1, session 2a of Data Mining Intro","author":[{"name":"Send comments to: Tony T (adthral)"}],"date":"`r format(Sys.time(), '%Y-%m-%d %H:%M', usetz = TRUE)`","abstract":"Introduce statistical clustering methods."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":[]}